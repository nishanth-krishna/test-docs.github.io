{"0": {
    "doc": "About OpenSearch",
    "title": "Introduction to OpenSearch",
    "content": "OpenSearch is a distributed search and analytics engine based on Apache Lucene. After adding your data to OpenSearch, you can perform full-text searches on it with all of the features you might expect: search by field, search multiple indices, boost fields, rank results by score, sort results by field, and aggregate results. Unsurprisingly, people often use search engines like OpenSearch as the backend for a search application—think Wikipedia or an online store. It offers excellent performance and can scale up and down as the needs of the application grow or shrink. An equally popular, but less obvious use case is log analytics, in which you take the logs from an application, feed them into OpenSearch, and use the rich search and visualization functionality to identify issues. For example, a malfunctioning web server might throw a 500 error 0.5% of the time, which can be hard to notice unless you have a real-time graph of all HTTP status codes that the server has thrown in the past four hours. You can use OpenSearch Dashboards to build these sorts of visualizations from data in OpenSearch. ",
    "url": "http://localhost:4000/docs/latest/about/#introduction-to-opensearch",
    "relUrl": "/about/#introduction-to-opensearch"
  },"1": {
    "doc": "About OpenSearch",
    "title": "Clusters and nodes",
    "content": "Its distributed design means that you interact with OpenSearch clusters. Each cluster is a collection of one or more nodes, servers that store your data and process search requests. You can run OpenSearch locally on a laptop—its system requirements are minimal—but you can also scale a single cluster to hundreds of powerful machines in a data center. In a single node cluster, such as a laptop, one machine has to do everything: manage the state of the cluster, index and search data, and perform any preprocessing of data prior to indexing it. As a cluster grows, however, you can subdivide responsibilities. Nodes with fast disks and plenty of RAM might be great at indexing and searching data, whereas a node with plenty of CPU power and a tiny disk could manage cluster state. For more information on setting node types, see Cluster formation. ",
    "url": "http://localhost:4000/docs/latest/about/#clusters-and-nodes",
    "relUrl": "/about/#clusters-and-nodes"
  },"2": {
    "doc": "About OpenSearch",
    "title": "Indices and documents",
    "content": "OpenSearch organizes data into indices. Each index is a collection of JSON documents. If you have a set of raw encyclopedia articles or log lines that you want to add to OpenSearch, you must first convert them to JSON. A simple JSON document for a movie might look like this: . { \"title\": \"The Wind Rises\", \"release_date\": \"2013-07-20\" } . When you add the document to an index, OpenSearch adds some metadata, such as the unique document ID: . { \"_index\": \"&lt;index-name&gt;\", \"_type\": \"_doc\", \"_id\": \"&lt;document-id&gt;\", \"_version\": 1, \"_source\": { \"title\": \"The Wind Rises\", \"release_date\": \"2013-07-20\" } } . Indices also contain mappings and settings: . | A mapping is the collection of fields that documents in the index have. In this case, those fields are title and release_date. | Settings include data like the index name, creation date, and number of shards. | . ",
    "url": "http://localhost:4000/docs/latest/about/#indices-and-documents",
    "relUrl": "/about/#indices-and-documents"
  },"3": {
    "doc": "About OpenSearch",
    "title": "Primary and replica shards",
    "content": "OpenSearch splits indices into shards for even distribution across nodes in a cluster. For example, a 400 GB index might be too large for any single node in your cluster to handle, but split into ten shards, each one 40 GB, OpenSearch can distribute the shards across ten nodes and work with each shard individually. By default, OpenSearch creates a replica shard for each primary shard. If you split your index into ten shards, for example, OpenSearch also creates ten replica shards. These replica shards act as backups in the event of a node failure—OpenSearch distributes replica shards to different nodes than their corresponding primary shards—but they also improve the speed and rate at which the cluster can process search requests. You might specify more than one replica per index for a search-heavy workload. Despite being a piece of an OpenSearch index, each shard is actually a full Lucene index—confusing, we know. This detail is important, though, because each instance of Lucene is a running process that consumes CPU and memory. More shards is not necessarily better. Splitting a 400 GB index into 1,000 shards, for example, would place needless strain on your cluster. A good rule of thumb is to keep shard size between 10–50 GB. ",
    "url": "http://localhost:4000/docs/latest/about/#primary-and-replica-shards",
    "relUrl": "/about/#primary-and-replica-shards"
  },"4": {
    "doc": "About OpenSearch",
    "title": "REST API",
    "content": "You interact with OpenSearch clusters using the REST API, which offers a lot of flexibility. You can use clients like curl or any programming language that can send HTTP requests. To add a JSON document to an OpenSearch index (i.e. index a document), you send an HTTP request: . PUT https://&lt;host&gt;:&lt;port&gt;/&lt;index-name&gt;/_doc/&lt;document-id&gt; { \"title\": \"The Wind Rises\", \"release_date\": \"2013-07-20\" } . To run a search for the document: . GET https://&lt;host&gt;:&lt;port&gt;/&lt;index-name&gt;/_search?q=wind . To delete the document: . DELETE https://&lt;host&gt;:&lt;port&gt;/&lt;index-name&gt;/_doc/&lt;document-id&gt; . You can change most OpenSearch settings using the REST API, modify indices, check the health of the cluster, get statistics—almost everything. ",
    "url": "http://localhost:4000/docs/latest/about/#rest-api",
    "relUrl": "/about/#rest-api"
  },"5": {
    "doc": "About OpenSearch",
    "title": "About OpenSearch",
    "content": " ",
    "url": "http://localhost:4000/docs/latest/about/",
    "relUrl": "/about/"
  },"6": {
    "doc": "Breaking changes",
    "title": "2.0.0",
    "content": "Remove mapping types parameter . The type parameter has been removed from all OpenSearch API endpoints. Instead, indexes can be categorized by document type. For more details, see issue #1940. Deprecate outdated nomenclature . In order for OpenSearch to include more inclusive naming conventions, we’ve replaced the following terms in our code with more inclusive terms: . | “Whitelist” is now “Allow list” | “Blacklist” is now “Deny list” | “Master” is now “Cluster Manager” | . If you are still using the outdated terms in the context of the security APIs or for node management, your calls and automation will continue to work until the terms are removed later in 2022. Add OpenSearch Notifications plugins . In OpenSearch 2.0, the Alerting plugin is now integrated with new plugins for Notifications. If you want to continue to use the notification action in the Alerting plugin, install the new backend plugins notifications-core and notifications. If you want to manage notifications in OpenSearch Dashboards, use the new notificationsDashboards plugin. For more information, see Questions about destinations on the Monitors page. Drop support for JDK 8 . A Lucene upgrade forced OpenSearch to drop support for JDK 8. As a consequence, the Java high-level REST client no longer supports JDK 8. Restoring JDK 8 support is currently an opensearch-java proposal #156 and will require removing OpenSearch core as a dependency from the Java client (issue #262). ",
    "url": "http://localhost:4000/docs/latest/breaking-changes/#200",
    "relUrl": "/breaking-changes/#200"
  },"7": {
    "doc": "Breaking changes",
    "title": "Breaking changes",
    "content": " ",
    "url": "http://localhost:4000/docs/latest/breaking-changes/",
    "relUrl": "/breaking-changes/"
  },"8": {
    "doc": "OpenSearch documentation",
    "title": "OpenSearch documentation",
    "content": "Welcome to the OpenSearch documentation! With this documentation, you’ll learn how to use OpenSearch — the only 100% open-source search, analytics, and visualization suite. We have a dedicated and growing number of technical writers who are building our documentation library. We also welcome and encourage community input. To contribute, see the Contributing file. A good place to start is by browsing issues labeled “good first issue.” . ",
    "url": "http://localhost:4000/docs/latest/",
    "relUrl": "/"
  },"9": {
    "doc": "OpenSearch documentation",
    "title": "test for right nav",
    "content": "hfbvhs vseh visev eisv iwe ve viwe vihw evih sjbsz fskhb vfg weuv wes fcug esvugesuigvf ugsd fjweuov uwe vu suov ouwev suw eov wueosv sjbe vuose v a . ",
    "url": "http://localhost:4000/docs/latest/#test-for-right-nav",
    "relUrl": "/#test-for-right-nav"
  },"10": {
    "doc": "OpenSearch documentation",
    "title": "Getting started",
    "content": ". | About OpenSearch | Quickstart | Install OpenSearch | Install OpenSearch Dashboards | See the FAQ | . ",
    "url": "http://localhost:4000/docs/latest/#getting-started",
    "relUrl": "/#getting-started"
  },"11": {
    "doc": "OpenSearch documentation",
    "title": "Why use OpenSearch?",
    "content": "With OpenSearch, you can perform the following use cases: . | | | | . | Fast, Scalable Full-text Search | Application and Infrastructure Monitoring | Security and Event Information Management | Operational Health Tracking | . | Help users find the right information within your application, website, or data lake catalog. | Easily store and analyze log data, and set automated alerts for underperformance. | Centralize logs to enable real-time security monitoring and forensic analysis. | Use observability logs, metrics, and traces to monitor your applications and business in real time. | . Additional features and plugins: . OpenSearch has several features and plugins to help index, secure, monitor, and analyze your data. Most OpenSearch plugins have corresponding OpenSearch Dashboards plugins that provide a convenient, unified user interface. | Anomaly detection - Identify atypical data and receive automatic notifications | KNN - Find “nearest neighbors” in your vector data | Performance Analyzer - Monitor and optimize your cluster | SQL - Use SQL or a piped processing language to query your data | Index State Management - Automate index operations | ML Commons plugin - Train and execute machine-learning models | Asynchronous search - Run search requests in the background | Cross-cluster replication - Replicate your data across multiple OpenSearch clusters | . ",
    "url": "http://localhost:4000/docs/latest/#why-use-opensearch",
    "relUrl": "/#why-use-opensearch"
  },"12": {
    "doc": "OpenSearch documentation",
    "title": "The secure path forward",
    "content": "OpenSearch includes a demo configuration so that you can get up and running quickly, but before using OpenSearch in a production environment, you must configure the security plugin manually with your own certificates, authentication method, users, and passwords. ",
    "url": "http://localhost:4000/docs/latest/#the-secure-path-forward",
    "relUrl": "/#the-secure-path-forward"
  },"13": {
    "doc": "OpenSearch documentation",
    "title": "Looking for the Javadoc?",
    "content": "See opensearch.org/javadocs/. ",
    "url": "http://localhost:4000/docs/latest/#looking-for-the-javadoc",
    "relUrl": "/#looking-for-the-javadoc"
  },"14": {
    "doc": "OpenSearch documentation",
    "title": "Get involved",
    "content": "OpenSearch is supported by Amazon Web Services. All components are available under the Apache License, Version 2.0 on GitHub. The project welcomes GitHub issues, bug fixes, features, plugins, documentation—anything at all. To get involved, see Contributing on the OpenSearch website. OpenSearch includes certain Apache-licensed Elasticsearch code from Elasticsearch B.V. and other source code. Elasticsearch B.V. is not the source of that other source code. ELASTICSEARCH is a registered trademark of Elasticsearch B.V. ",
    "url": "http://localhost:4000/docs/latest/#get-involved",
    "relUrl": "/#get-involved"
  },"15": {
    "doc": "Quickstart",
    "title": "Quickstart",
    "content": "Get started using OpenSearch and OpenSearch Dashboards by deploying your containers with Docker. Before proceeding, you need to get Docker and Docker Compose installed on your local machine. The Docker Compose commands used in this guide are written with a hyphen (for example, docker-compose). If you installed Docker Desktop on your machine, which automatically installs a bundled version of Docker Compose, then you should remove the hyphen. For example, change docker-compose to docker compose. ",
    "url": "http://localhost:4000/docs/latest/quickstart/",
    "relUrl": "/quickstart/"
  },"16": {
    "doc": "Quickstart",
    "title": "Starting your cluster",
    "content": "You’ll need a special file, called a Compose file, that Docker Compose uses to define and create the containers in your cluster. The OpenSearch Project provides a sample Compose file that you can use to get started. Learn more about working with Compose files by reviewing the official Compose specification. | Before running OpenSearch on your machine, you should disable memory paging and swapping performance on the host to improve performance and increase the number of memory maps available to OpenSearch. See important system settings for more information. # Disable memory paging and swapping. sudo swapoff -a # Edit the sysctl config file that defines the host's max map count. sudo vi /etc/sysctl.conf # Set max map count to the recommended value of 262144. vm.max_map_count=262144 # Reload the kernel parameters. sudo sysctl -p . | Download the sample Compose file to your host. You can download the file with command line utilities like curl and wget, or you can manually copy docker-compose.yml from the OpenSearch Project documentation-website repository using a web browser. # Using cURL: curl -O https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/docker-compose.yml # Using wget: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/docker-compose.yml . | In your terminal application, navigate to the directory containing the docker-compose.yml file you just downloaded, and run the following command to create and start the cluster as a background process. docker-compose up -d . | Confirm that the containers are running with the command docker-compose ps. You should see an output like the following: $ docker-compose ps NAME COMMAND SERVICE STATUS PORTS opensearch-dashboards \"./opensearch-dashbo…\" opensearch-dashboards running 0.0.0.0:5601-&gt;5601/tcp opensearch-node1 \"./opensearch-docker…\" opensearch-node1 running 0.0.0.0:9200-&gt;9200/tcp, 9300/tcp, 0.0.0.0:9600-&gt;9600/tcp, 9650/tcp opensearch-node2 \"./opensearch-docker…\" opensearch-node2 running 9200/tcp, 9300/tcp, 9600/tcp, 9650/tcp . | Query the OpenSearch REST API to verify that the service is running. You should use -k (also written as --insecure) to disable host name checking because the default security configuration uses demo certificates. Use -u to pass the default username and password (admin:admin). curl https://localhost:9200 -ku admin:admin . Sample response: . { \"name\" : \"opensearch-node1\", \"cluster_name\" : \"opensearch-cluster\", \"cluster_uuid\" : \"Cd7SL5ysRSyuau325M3h9w\", \"version\" : { \"distribution\" : \"opensearch\", \"number\" : \"2.3.0\", \"build_type\" : \"tar\", \"build_hash\" : \"6f6e84ebc54af31a976f53af36a5c69d474a5140\", \"build_date\" : \"2022-09-09T00:07:12.137133581Z\", \"build_snapshot\" : false, \"lucene_version\" : \"9.3.0\", \"minimum_wire_compatibility_version\" : \"7.10.0\", \"minimum_index_compatibility_version\" : \"7.0.0\" }, \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\" } . | Explore OpenSearch Dashboards by opening http://localhost:5601/ in a web browser on the same host that is running your OpenSearch cluster. The default username is admin and the default password is admin. | . ",
    "url": "http://localhost:4000/docs/latest/quickstart/#starting-your-cluster",
    "relUrl": "/quickstart/#starting-your-cluster"
  },"17": {
    "doc": "Quickstart",
    "title": "Create an index and field mappings using sample data",
    "content": "Create an index and define field mappings using a dataset provided by the OpenSearch Project. The same fictitious e-commerce data is also used for sample visualizations in OpenSearch Dashboards. To learn more, see Getting started with OpenSearch Dashboards. | Download ecommerce-field_mappings.json. This file defines a mapping for the sample data you will use. # Using cURL: curl -O https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/ecommerce-field_mappings.json # Using wget: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/ecommerce-field_mappings.json . | Download ecommerce.json. This file contains the index data formatted so that it can be ingested by the bulk API. To learn more, see index data and Bulk. # Using cURL: curl -O https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/ecommerce.json # Using wget: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/ecommerce.json . | Define the field mappings with the mapping file. curl -H \"Content-Type: application/x-ndjson\" -X PUT \"https://localhost:9200/ecommerce\" -ku admin:admin --data-binary \"@ecommerce-field_mappings.json\" . | Upload the index to the bulk API. curl -H \"Content-Type: application/x-ndjson\" -X PUT \"https://localhost:9200/ecommerce/_bulk\" -ku admin:admin --data-binary \"@ecommerce.json\" . | Query the data using the search API. The following command submits a query that will return documents where customer_first_name is Sonya. curl -H 'Content-Type: application/json' -X GET \"https://localhost:9200/ecommerce/_search?pretty=true\" -ku admin:admin -d' {\"query\":{\"match\":{\"customer_first_name\":\"Sonya\"}}}' . Queries submitted to the OpenSearch REST API will generally return a flat JSON by default. For a human readable response body, use the query parameter pretty=true. For more information about pretty and other useful query parameters, see Common REST parameters. | Access OpenSearch Dashboards by opening http://localhost:5601/ in a web browser on the same host that is running your OpenSearch cluster. The default username is admin and the default password is admin. | On the top menu bar, go to Management &gt; Dev Tools. | In the left pane of the console, enter the following: GET ecommerce/_search { \"query\": { \"match\": { \"customer_first_name\": \"Sonya\" } } } . | Choose the triangle icon at the top right of the request to submit the query. You can also submit the request by pressing Ctrl+Enter (or Cmd+Enter for Mac users). To learn more about using the OpenSearch Dashboards console for submitting queries, see Running queries in the console. | . ",
    "url": "http://localhost:4000/docs/latest/quickstart/#create-an-index-and-field-mappings-using-sample-data",
    "relUrl": "/quickstart/#create-an-index-and-field-mappings-using-sample-data"
  },"18": {
    "doc": "Quickstart",
    "title": "Next steps",
    "content": "You successfully deployed your own OpenSearch cluster with OpenSearch Dashboards and added some sample data. Now you’re ready to learn about configuration and functionality in more detail. Here are a few recommendations on where to begin: . | About the security plugin | OpenSearch configuration | OpenSearch plugin installation | Getting started with OpenSearch Dashboards | OpenSearch tools | Index APIs | . ",
    "url": "http://localhost:4000/docs/latest/quickstart/#next-steps",
    "relUrl": "/quickstart/#next-steps"
  },"19": {
    "doc": "Quickstart",
    "title": "Common issues",
    "content": "Review these common issues and suggested solutions if your containers fail to start or exit unexpectedly. Docker commands require elevated permissions . Eliminate the need for running your Docker commands with sudo by adding your user to the docker user group. See Docker’s Post-installation steps for Linux for more information. sudo usermod -aG docker $USER . Error message: “-bash: docker-compose: command not found” . If you installed Docker Desktop, then Docker Compose is already installed on your machine. Try docker compose (without the hyphen) instead of docker-compose. See Use Docker Compose. Error message: “docker: ‘compose’ is not a docker command.” . If you installed Docker Engine, then you must install Docker Compose separately, and you will use the command docker-compose (with a hyphen). See Docker Compose. Error message: “max virtual memory areas vm.max_map_count [65530] is too low” . OpenSearch will fail to start if your host’s vm.max_map_count is too low. Review the important system settings if you see the following errors in the service log, and set vm.max_map_count appropriately. opensearch-node1 | ERROR: [1] bootstrap checks failed opensearch-node1 | [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] opensearch-node1 | ERROR: OpenSearch did not exit normally - check the logs at /usr/share/opensearch/logs/opensearch-cluster.log . ",
    "url": "http://localhost:4000/docs/latest/quickstart/#common-issues",
    "relUrl": "/quickstart/#common-issues"
  },"20": {
    "doc": "Version history",
    "title": "Version history",
    "content": "| OpenSearch version | Release highlights | Release date | . | 2.5.0 | Includes index management UI enhancements, multi-layer maps, Jaeger support for observability, Debian distributions, returning cluster health by awareness attribute, cluster manager task throttling, weighted zonal search request routing policy, and query string support in index rollups. Experimental features include request-level durability in remote-backed storage and GPU acceleration for ML nodes. For a full list of release highlights, see the Release Notes. | 24 January 2023 | . | 2.4.1 | Includes maintenance changes and bug fixes for gradle check and indexing pressure tests. Adds support for skipping changelog. | 13 December 2022 | . | 2.4.0 | Includes Windows support, Point-in-time search, custom k-NN filtering, xy_point and xy_shape field types for Cartesian coordinates, GeoHex grid aggregation, and resilience enhancements, including search backpressure. In OpenSearch Dashboards, this release adds snapshot restore functionality, multiple authentication, and aggregate view of saved objects. This release includes the following experimental features: searchable snapshots, Compare Search Results, multiple data sources in OpenSearch Dashboards, a new Model Serving Framework in ML Commons, a new Neural Search plugin that supports semantic search, and a new Security Analytics plugin to analyze security logs. For a full list of release highlights, see the Release Notes. | 15 November 2022 | . | 2.3.0 | This release includes the following experimental features: segment replication, remote-backed storage, and drag and drop for OpenSearch Dashboards. Experimental features allow you to test new functionality in OpenSearch. Because these features are still being developed, your testing and feedback can help shape the development of the feature before it’s official released. We do not recommend use of experimental features in production. Additionally, this release adds maketime and makedate datetime functions for the SQL plugin. Creates a new OpenSearch Playground demo site for OpenSearch Dashboards. For a full list of release highlights, see the Release Notes. | 14 September 2022 | . | 2.2.1 | Includes gradle updates and bug fixes for gradle check. | 01 September 2022 | . | 2.2.0 | Includes support for Logistic Regression and RCFSummarize machine learning algorithms in ML Commons, Lucene or C-based Nmslib and Faiss libraries for approximate k-NN search, search by relevance using SQL and PPL queries, custom region maps for visualizations, and rollup enhancements. For a full list of release highlights, see the Release Notes. | 11 August 2022 | . | 2.1.0 | Includes support for dedicated ML node in the ML Commons plugin, relevance search and other features in SQL, multi-terms aggregation, and Snapshot Management. For a full list of release highlights, see the Release Notes. | 07 July 2022 | . | 2.0.1 | Includes bug fixes and maintenance updates for Alerting and Anomaly Detection. | 16 June 2022 | . | 2.0.0 | Includes document-level monitors for alerting, OpenSearch Notifications plugins, and Geo Map Tiles in OpenSearch Dashboards. Also adds support for Lucene 9 and bug fixes for all OpenSearch plugins. For a full list of release highlights, see the Release Notes. | 26 May 2022 | . | 2.0.0-rc1 | The Release Candidate for 2.0.0. This version allows you to preview the upcoming 2.0.0 release before the GA release. The preview release adds document-level alerting, support for Lucene 9, and the ability to use term lookup queries in document level security. | 03 May 2022 | . | 1.3.7 | Adds Windows support. Includes maintenance updates and bug fixes for error handling. | 13 December 2022 | . | 1.3.6 | Includes maintenance updates and bug fixes for tenancy in the OpenSearch Security Dashboards plugin. | 06 October 2022 | . | 1.3.5 | Includes maintenance updates and bug fixes for gradle check and OpenSearch security. | 01 September 2022 | . | 1.3.4 | Includes maintenance updates and bug fixes for OpenSearch and OpenSearch Dashboards. | 14 July 2022 | . | 1.3.3 | Adds enhancements to Anomaly Detection and ML Commons. Bug fixes for Anomaly Detection, Observability, and k-NN. | 09 June 2022 | . | 1.3.2 | Bug fixes for Anomaly Detection and the Security Dashboards Plugin, adds the option to install OpenSearch using RPM, as well as enhancements to the ML Commons execute task, and the removal of the job-scheduler zip in Anomaly Detection. | 05 May 2022 | . | 1.3.1 | Bug fixes when using document-level security, and adjusted ML Commons to use the latest RCF jar and protostuff to RCF model serialization. | 30 March 2022 | . | 1.3.0 | Adds Model Type Validation to Validate Detector API, continuous transforms, custom actions, applied policy parameter to Explain API, default action retries, and new rollover and transition conditions to Index Management, new ML Commons plugin, parse command to SQL, Application Analytics, Live Tail, Correlation, and Events Flyout to Observability, and auto backport and support for OPENSEARCH_JAVA_HOME to Performance Analyzer. Bug fixes. | 17 March 2022 | . | 1.2.4 | Updates Performance Analyzer, SQL, and Security plugins to Log4j 2.17.1, Alerting and Job Scheduler to cron-utils 9.1.6, and gson in Anomaly Detection and SQL. | 18 January 2022 | . | 1.2.3 | Updates the version of Log4j used in OpenSearch to Log4j 2.17.0 as recommended by the advisory in CVE-2021-45105. | 22 December 2021 | . | 1.2.0 | Adds observability, new validation API for Anomaly Detection, shard-level indexing back-pressure, new “match” query type for SQL and PPL, support for Faiss libraries in k-NN, and custom Dashboards branding. | 23 November 2021 | . | 1.1.0 | Adds cross-cluster replication, security for Index Management, bucket-level alerting, a CLI to help with upgrading from Elasticsearch OSS to OpenSearch, and enhancements to high cardinality data in the anomaly detection plugin. | 05 October 2021 | . | 1.0.1 | Bug fixes. | 01 September 2021 | . | 1.0.0 | General availability release. Adds compatibility setting for clients that require a version check before connecting. | 12 July 2021 | . | 1.0.0-rc1 | First release candidate. | 07 June 2021 | . | 1.0.0-beta1 | Initial beta release. Refactors plugins to work with OpenSearch. | 13 May 2021 | . ",
    "url": "http://localhost:4000/docs/latest/version-history/",
    "relUrl": "/version-history/"
  },"21": {
    "doc": "Index rollups",
    "title": "Index rollups",
    "content": "Time series data increases storage costs, strains cluster health, and slows down aggregations over time. Index rollup lets you periodically reduce data granularity by rolling up old data into summarized indices. You pick the fields that interest you and use index rollup to create a new index with only those fields aggregated into coarser time buckets. You can store months or years of historical data at a fraction of the cost with the same query performance. For example, say you collect CPU consumption data every five seconds and store it on a hot node. Instead of moving older data to a read-only warm node, you can roll up or compress this data with only the average CPU consumption per day or with a 10% decrease in its interval every week. You can use index rollup in three ways: . | Use the index rollup API for an on-demand index rollup job that operates on an index that’s not being actively ingested such as a rolled-over index. For example, you can perform an index rollup operation to reduce data collected at a five minute interval to a weekly average for trend analysis. | Use the OpenSearch Dashboards UI to create an index rollup job that runs on a defined schedule. You can also set it up to roll up your indices as it’s being actively ingested. For example, you can continuously roll up Logstash indices from a five second interval to a one hour interval. | Specify the index rollup job as an ISM action for complete index management. This allows you to roll up an index after a certain event such as a rollover, index age reaching a certain point, index becoming read-only, and so on. You can also have rollover and index rollup jobs running in sequence, where the rollover first moves the current index to a warm node and then the index rollup job creates a new index with the minimized data on the hot node. | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-rollups/index/",
    "relUrl": "/im-plugin/index-rollups/index/"
  },"22": {
    "doc": "Index rollups",
    "title": "Create an Index Rollup Job",
    "content": "To get started, choose Index Management in OpenSearch Dashboards. Select Rollup Jobs and choose Create rollup job. Step 1: Set up indices . | In the Job name and description section, specify a unique name and an optional description for the index rollup job. | In the Indices section, select the source and target index. The source index is the one that you want to roll up. The source index remains as is, the index rollup job creates a new index referred to as a target index. The target index is where the index rollup results are saved. For target index, you can either type in a name for a new index or you select an existing index. | Choose Next | . After you create an index rollup job, you can’t change your index selections. Step 2: Define aggregations and metrics . Select the attributes with the aggregations (terms and histograms) and metrics (avg, sum, max, min, and value count) that you want to roll up. Make sure you don’t add a lot of highly granular attributes, because you won’t save much space. For example, consider a dataset of cities and demographics within those cities. You can aggregate based on cities and specify demographics within a city as metrics. The order in which you select attributes is critical. A city followed by a demographic is different from a demographic followed by a city. | In the Time aggregation section, select a timestamp field. Choose between a Fixed or Calendar interval type and specify the interval and timezone. The index rollup job uses this information to create a date histogram for the timestamp field. | (Optional) Add additional aggregations for each field. You can choose terms aggregation for all field types and histogram aggregation only for numeric fields. | (Optional) Add additional metrics for each field. You can choose between All, Min, Max, Sum, Avg, or Value Count. | Choose Next. | . Step 3: Specify schedule . Specify a schedule to roll up your indices as it’s being ingested. The index rollup job is enabled by default. | Specify if the data is continuous or not. | For roll up execution frequency, select Define by fixed interval and specify the Rollup interval and the time unit or Define by cron expression and add in a cron expression to select the interval. To learn how to define a cron expression, see Alerting. | Specify the number of pages per execution process. A larger number means faster execution and more cost for memory. | (Optional) Add a delay to the roll up executions. This is the amount of time the job waits for data ingestion to accommodate any processing time. For example, if you set this value to 10 minutes, an index rollup that executes at 2 PM to roll up 1 PM to 2 PM of data starts at 2:10 PM. | Choose Next. | . Step 4: Review and create . Review your configuration and select Create. Step 5: Search the target index . You can use the standard _search API to search the target index. Make sure that the query matches the constraints of the target index. For example, if you don’t set up terms aggregations on a field, you don’t receive results for terms aggregations. If you don’t set up the maximum aggregations, you don’t receive results for maximum aggregations. You can’t access the internal structure of the data in the target index because the plugin automatically rewrites the query in the background to suit the target index. This is to make sure you can use the same query for the source and target index. To query the target index, set size to 0: . GET target_index/_search { \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"avg_cpu\": { \"avg\": { \"field\": \"cpu_usage\" } } } } . Consider a scenario where you collect rolled up data from 1 PM to 9 PM in hourly intervals and live data from 7 PM to 11 PM in minutely intervals. If you execute an aggregation over these in the same query, for 7 PM to 9 PM, you see an overlap of both rolled up data and live data because they get counted twice in the aggregations. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-rollups/index/#create-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/index/#create-an-index-rollup-job"
  },"23": {
    "doc": "Index rollups",
    "title": "Sample Walkthrough",
    "content": "This walkthrough uses the OpenSearch Dashboards sample e-commerce data. To add that sample data, log in to OpenSearch Dashboards, choose Home and Try our sample data. For Sample eCommerce orders, choose Add data. Then run a search: . GET opensearch_dashboards_sample_data_ecommerce/_search . Sample response . { \"took\": 23, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4675, \"relation\": \"eq\" }, \"max_score\": 1, \"hits\": [ { \"_index\": \"opensearch_dashboards_sample_data_ecommerce\", \"_type\": \"_doc\", \"_id\": \"jlMlwXcBQVLeQPrkC_kQ\", \"_score\": 1, \"_source\": { \"category\": [ \"Women's Clothing\", \"Women's Accessories\" ], \"currency\": \"EUR\", \"customer_first_name\": \"Selena\", \"customer_full_name\": \"Selena Mullins\", \"customer_gender\": \"FEMALE\", \"customer_id\": 42, \"customer_last_name\": \"Mullins\", \"customer_phone\": \"\", \"day_of_week\": \"Saturday\", \"day_of_week_i\": 5, \"email\": \"selena@mullins-family.zzz\", \"manufacturer\": [ \"Tigress Enterprises\" ], \"order_date\": \"2021-02-27T03:56:10+00:00\", \"order_id\": 581553, \"products\": [ { \"base_price\": 24.99, \"discount_percentage\": 0, \"quantity\": 1, \"manufacturer\": \"Tigress Enterprises\", \"tax_amount\": 0, \"product_id\": 19240, \"category\": \"Women's Clothing\", \"sku\": \"ZO0064500645\", \"taxless_price\": 24.99, \"unit_discount_amount\": 0, \"min_price\": 12.99, \"_id\": \"sold_product_581553_19240\", \"discount_amount\": 0, \"created_on\": \"2016-12-24T03:56:10+00:00\", \"product_name\": \"Blouse - port royal\", \"price\": 24.99, \"taxful_price\": 24.99, \"base_unit_price\": 24.99 }, { \"base_price\": 10.99, \"discount_percentage\": 0, \"quantity\": 1, \"manufacturer\": \"Tigress Enterprises\", \"tax_amount\": 0, \"product_id\": 17221, \"category\": \"Women's Accessories\", \"sku\": \"ZO0085200852\", \"taxless_price\": 10.99, \"unit_discount_amount\": 0, \"min_price\": 5.06, \"_id\": \"sold_product_581553_17221\", \"discount_amount\": 0, \"created_on\": \"2016-12-24T03:56:10+00:00\", \"product_name\": \"Snood - rose\", \"price\": 10.99, \"taxful_price\": 10.99, \"base_unit_price\": 10.99 } ], \"sku\": [ \"ZO0064500645\", \"ZO0085200852\" ], \"taxful_total_price\": 35.98, \"taxless_total_price\": 35.98, \"total_quantity\": 2, \"total_unique_products\": 2, \"type\": \"order\", \"user\": \"selena\", \"geoip\": { \"country_iso_code\": \"MA\", \"location\": { \"lon\": -8, \"lat\": 31.6 }, \"region_name\": \"Marrakech-Tensift-Al Haouz\", \"continent_name\": \"Africa\", \"city_name\": \"Marrakesh\" }, \"event\": { \"dataset\": \"sample_ecommerce\" } } } ] } } ... Create an index rollup job. This example picks the order_date, customer_gender, geoip.city_name, geoip.region_name, and day_of_week fields and rolls them into an example_rollup target index: . PUT _plugins/_rollup/jobs/example { \"rollup\": { \"enabled\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"last_updated_time\": 1602100553, \"description\": \"An example policy that rolls up the sample ecommerce data\", \"source_index\": \"opensearch_dashboards_sample_data_ecommerce\", \"target_index\": \"example_rollup\", \"page_size\": 1000, \"delay\": 0, \"continuous\": false, \"dimensions\": [ { \"date_histogram\": { \"source_field\": \"order_date\", \"fixed_interval\": \"60m\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"customer_gender\" } }, { \"terms\": { \"source_field\": \"geoip.city_name\" } }, { \"terms\": { \"source_field\": \"geoip.region_name\" } }, { \"terms\": { \"source_field\": \"day_of_week\" } } ], \"metrics\": [ { \"source_field\": \"taxless_total_price\", \"metrics\": [ { \"avg\": {} }, { \"sum\": {} }, { \"max\": {} }, { \"min\": {} }, { \"value_count\": {} } ] }, { \"source_field\": \"total_quantity\", \"metrics\": [ { \"avg\": {} }, { \"max\": {} } ] } ] } } . You can query the example_rollup index for the terms aggregations on the fields set up in the rollup job. You get back the same response that you would on the original opensearch_dashboards_sample_data_ecommerce source index. POST example_rollup/_search { \"size\": 0, \"query\": { \"bool\": { \"must\": {\"term\": { \"geoip.region_name\": \"California\" } } } }, \"aggregations\": { \"daily_numbers\": { \"terms\": { \"field\": \"day_of_week\" }, \"aggs\": { \"per_city\": { \"terms\": { \"field\": \"geoip.city_name\" }, \"aggregations\": { \"average quantity\": { \"avg\": { \"field\": \"total_quantity\" } } } }, \"total_revenue\": { \"sum\": { \"field\": \"taxless_total_price\" } } } } } } . Sample Response . { \"took\" : 14, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 281, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"aggregations\" : { \"daily_numbers\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Friday\", \"doc_count\" : 59, \"total_revenue\" : { \"value\" : 4858.84375 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 59, \"average quantity\" : { \"value\" : 2.305084745762712 } } ] } }, { \"key\" : \"Saturday\", \"doc_count\" : 46, \"total_revenue\" : { \"value\" : 3547.203125 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 46, \"average quantity\" : { \"value\" : 2.260869565217391 } } ] } }, { \"key\" : \"Tuesday\", \"doc_count\" : 45, \"total_revenue\" : { \"value\" : 3983.28125 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 45, \"average quantity\" : { \"value\" : 2.2888888888888888 } } ] } }, { \"key\" : \"Sunday\", \"doc_count\" : 44, \"total_revenue\" : { \"value\" : 3308.1640625 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 44, \"average quantity\" : { \"value\" : 2.090909090909091 } } ] } }, { \"key\" : \"Thursday\", \"doc_count\" : 40, \"total_revenue\" : { \"value\" : 2876.125 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 40, \"average quantity\" : { \"value\" : 2.3 } } ] } }, { \"key\" : \"Monday\", \"doc_count\" : 38, \"total_revenue\" : { \"value\" : 2673.453125 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 38, \"average quantity\" : { \"value\" : 2.1578947368421053 } } ] } }, { \"key\" : \"Wednesday\", \"doc_count\" : 38, \"total_revenue\" : { \"value\" : 3202.453125 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 38, \"average quantity\" : { \"value\" : 2.236842105263158 } } ] } } ] } } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-rollups/index/#sample-walkthrough",
    "relUrl": "/im-plugin/index-rollups/index/#sample-walkthrough"
  },"24": {
    "doc": "Index rollups",
    "title": "The doc_count field",
    "content": "The doc_count field in bucket aggregations contains the number of documents collected in each bucket. When calculating the bucket’s doc_count, the number of documents is incremented by the number of the pre-aggregated documents in each summary document. The doc_count returned from rollup searches represents the total number of matching documents from the source index. The document count for each bucket is the same whether you search the source index or the rollup target index. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-rollups/index/#the-doc_count-field",
    "relUrl": "/im-plugin/index-rollups/index/#the-doc_count-field"
  },"25": {
    "doc": "Index rollups",
    "title": "Query string queries",
    "content": "To take advantage of shorter and easier to write strings in Query DSL, you can use query strings to simplify search queries in rollup indexes. To use query strings, add the following fields to your rollup search request. \"query\": { \"query_string\": { \"query\": \"field_name:field_value\" } } . The following example uses a query string with a * wildcard operator to search inside a rollup index called my_server_logs_rollup. GET my_server_logs_rollup/_search { \"size\": 0, \"query\": { \"query_string\": { \"query\": \"email* OR inventory\", \"default_field\": \"service_name\" } }, \"aggs\": { \"service_name\": { \"terms\": { \"field\": \"service_name\" }, \"aggs\": { \"region\": { \"terms\": { \"field\": \"region\" }, \"aggs\": { \"average quantity\": { \"avg\": { \"field\": \"cpu_usage\" } } } } } } } } . For more information on which parameters are supported in query strings, see Advanced filter options. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-rollups/index/#query-string-queries",
    "relUrl": "/im-plugin/index-rollups/index/#query-string-queries"
  },"26": {
    "doc": "Index rollups",
    "title": "Dynamic target index",
    "content": "In ISM rollup, the target_index field may contain a template that is compiled at the time of each rollup indexing. For example, if you specify the target_index field as rollup_ndx-{{ctx.source_index}}, the source index log-000001 will roll up into a target index rollup_ndx-log-000001. This allows you to roll up data into multiple time-based indices, with one rollup job created for each source index. The source_index parameter in {{ctx.source_index}} cannot contain wildcards. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-rollups/index/#dynamic-target-index",
    "relUrl": "/im-plugin/index-rollups/index/#dynamic-target-index"
  },"27": {
    "doc": "Index rollups",
    "title": "Searching multiple rollup indices",
    "content": "When data is rolled up into multiple target indices, you can run one search across all of the rollup indices. To search multiple target indices that have the same rollup, specify the index names as a comma-separated list or a wildcard pattern. For example, with target_index as rollup_ndx-{{ctx.source_index}} and source indices that start with log, specify the rollup_ndx-log* pattern. Or, to search for rolled up log-000001 and log-000002 indices, specify the rollup_ndx-log-000001,rollup_ndx-log-000002 list. You cannot search a mix of rollup and non-rollup indices with the same query. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-rollups/index/#searching-multiple-rollup-indices",
    "relUrl": "/im-plugin/index-rollups/index/#searching-multiple-rollup-indices"
  },"28": {
    "doc": "Index rollups",
    "title": "Example",
    "content": "The following example demonstrates the doc_count field, dynamic index names, and searching multiple rollup indices with the same rollup. Step 1: Add an index template for ISM to manage the rolling over of the indices aliased by log. PUT _index_template/ism_rollover { \"index_patterns\": [\"log*\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"log\" } } } . Step 2: Set up an ISM rollover policy to roll over any index whose name starts with log* after one document is uploaded to it, and then roll up the individual backing index. The target index name is dynamically generated from the source index name by prepending the string rollup_ndx- to the source index name. PUT _plugins/_ism/policies/rollover_policy { \"policy\": { \"description\": \"Example rollover policy.\", \"default_state\": \"rollover\", \"states\": [ { \"name\": \"rollover\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 1 } } ], \"transitions\": [ { \"state_name\": \"rp\" } ] }, { \"name\": \"rp\", \"actions\": [ { \"rollup\": { \"ism_rollup\": { \"target_index\": \"rollup_ndx-{{ctx.source_index}}\", \"description\": \"Example rollup job\", \"page_size\": 200, \"dimensions\": [ { \"date_histogram\": { \"source_field\": \"ts\", \"fixed_interval\": \"60m\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"message.keyword\" } } ], \"metrics\": [ { \"source_field\": \"msg_size\", \"metrics\": [ { \"sum\": {} } ] } ] } } } ], \"transitions\": [] } ], \"ism_template\": { \"index_patterns\": [\"log*\"], \"priority\": 100 } } } . Step 3: Create an index named log-000001 and set up an alias log for it. PUT log-000001 { \"aliases\": { \"log\": { \"is_write_index\": true } } } . Step 4: Index four documents into the index created above. Two of the documents have the message “Success”, and two have the message “Error”. POST log/_doc?refresh=true { \"ts\" : \"2022-08-26T09:28:48-04:00\", \"message\": \"Success\", \"msg_size\": 10 } . POST log/_doc?refresh=true { \"ts\" : \"2022-08-26T10:06:25-04:00\", \"message\": \"Error\", \"msg_size\": 20 } . POST log/_doc?refresh=true { \"ts\" : \"2022-08-26T10:23:54-04:00\", \"message\": \"Error\", \"msg_size\": 30 } . POST log/_doc?refresh=true { \"ts\" : \"2022-08-26T10:53:41-04:00\", \"message\": \"Success\", \"msg_size\": 40 } . Once you index the first document, the rollover action is executed. This action creates the index log-000002 with rollover_policy attached to it. Then the rollup action is executed, which creates the rollup index rollup_ndx-log-000001. To monitor the status of rollover and rollup index creation, you can use the ISM explain API: GET _plugins/_ism/explain . Step 5: Search the rollup index. GET rollup_ndx-log-*/_search { \"size\": 0, \"query\": { \"match_all\": {} }, \"aggregations\": { \"message_numbers\": { \"terms\": { \"field\": \"message.keyword\" }, \"aggs\": { \"per_message\": { \"terms\": { \"field\": \"message.keyword\" }, \"aggregations\": { \"sum_message\": { \"sum\": { \"field\": \"msg_size\" } } } } } } } } . The response contains two buckets, “Error” and “Success”, and the document count for each bucket is 2: . { \"took\" : 30, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 4, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"aggregations\" : { \"message_numbers\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Success\", \"doc_count\" : 2, \"per_message\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Success\", \"doc_count\" : 2, \"sum_message\" : { \"value\" : 50.0 } } ] } }, { \"key\" : \"Error\", \"doc_count\" : 2, \"per_message\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Error\", \"doc_count\" : 2, \"sum_message\" : { \"value\" : 50.0 } } ] } } ] } } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-rollups/index/#example",
    "relUrl": "/im-plugin/index-rollups/index/#example"
  },"29": {
    "doc": "Index rollups API",
    "title": "Index rollups API",
    "content": "Use the index rollup operations to programmatically work with index rollup jobs. . | Create or update an index rollup job | Get an index rollup job | Delete an index rollup job | Start or stop an index rollup job | Explain an index rollup job | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-rollups/rollup-api/",
    "relUrl": "/im-plugin/index-rollups/rollup-api/"
  },"30": {
    "doc": "Index rollups API",
    "title": "Create or update an index rollup job",
    "content": "Introduced 1.0 . Creates or updates an index rollup job. You must provide the seq_no and primary_term parameters. Request . PUT _plugins/_rollup/jobs/&lt;rollup_id&gt; // Create PUT _plugins/_rollup/jobs/&lt;rollup_id&gt;?if_seq_no=1&amp;if_primary_term=1 // Update { \"rollup\": { \"source_index\": \"nyc-taxi-data\", \"target_index\": \"rollup-nyc-taxi-data\", \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Days\" } }, \"description\": \"Example rollup job\", \"enabled\": true, \"page_size\": 200, \"delay\": 0, \"roles\": [ \"rollup_all\", \"nyc_taxi_all\", \"example_rollup_index_all\" ], \"continuous\": false, \"dimensions\": { \"date_histogram\": { \"source_field\": \"tpep_pickup_datetime\", \"fixed_interval\": \"1h\", \"timezone\": \"America/Los_Angeles\" }, \"terms\": { \"source_field\": \"PULocationID\" }, \"metrics\": [ { \"source_field\": \"passenger_count\", \"metrics\": [ { \"avg\": {} }, { \"sum\": {} }, { \"max\": {} }, { \"min\": {} }, { \"value_count\": {} } ] } ] } } } . You can specify the following options. | Options | Description | Type | Required | . | source_index | The name of the detector. | String | Yes | . | target_index | Specify the target index that the rolled up data is ingested into. You can either create a new target index or use an existing index. The target index cannot be a combination of raw and rolled up data. This field supports dynamically generated index names like rollup_{{ctx.source_index}}, where source_index cannot contain wildcards. | String | Yes | . | schedule | Schedule of the index rollup job which can be an interval or a cron expression. | Object | Yes | . | schedule.interval | Specify the frequency of execution of the rollup job. | Object | No | . | schedule.interval.start_time | Start time of the interval. | Timestamp | Yes | . | schedule.interval.period | Define the interval period. | String | Yes | . | schedule.interval.unit | Specify the time unit of the interval. | String | Yes | . | schedule.interval.cron | Optionally, specify a cron expression to define therollup frequency. | List | No | . | schedule.interval.cron.expression | Specify a Unix cron expression. | String | Yes | . | schedule.interval.cron.timezone | Specify timezones as defined by the IANA Time Zone Database. Defaults to UTC. | String | No | . | description | Optionally, describe the rollup job. | String | No | . | enabled | When true, the index rollup job is scheduled. Default is true. | Boolean | Yes | . | continuous | Specify whether or not the index rollup job continuously rolls up data forever or just executes over the current data set once and stops. Default is false. | Boolean | Yes | . | error_notification | Set up a Mustache message template sent for error notifications. For example, if an index rollup job fails, the system sends a message to a Slack channel. | Object | No | . | page_size | Specify the number of buckets to paginate through at a time while rolling up. | Number | Yes | . | delay | The number of milliseconds to delay execution of the index rollup job. | Long | No | . | dimensions | Specify aggregations to create dimensions for the roll up time window. | Object | Yes | . | dimensions.date_histogram | Specify either fixed_interval or calendar_interval, but not both. Either one limits what you can query in the target index. | Object | No | . | dimensions.date_histogram.fixed_interval | Specify the fixed interval for aggregations in milliseconds, seconds, minutes, hours, or days. | String | No | . | dimensions.date_histogram.calendar_interval | Specify the calendar interval for aggregations in minutes, hours, days, weeks, months, quarters, or years. | String | No | . | dimensions.date_histogram.field | Specify the date field used in date histogram aggregation. | String | No | . | dimensions.date_histogram.timezone | Specify the timezones as defined by the IANA Time Zone Database. The default is UTC. | String | No | . | dimensions.terms | Specify the term aggregations that you want to roll up. | Object | No | . | dimensions.terms.fields | Specify terms aggregation for compatible fields. | Object | No | . | dimensions.histogram | Specify the histogram aggregations that you want to roll up. | Object | No | . | dimensions.histogram.field | Add a field for histogram aggregations. | String | Yes | . | dimensions.histogram.interval | Specify the histogram aggregation interval for the field. | Long | Yes | . | dimensions.metrics | Specify a list of objects that represent the fields and metrics that you want to calculate. | Nested object | No | . | dimensions.metrics.field | Specify the field that you want to perform metric aggregations on. | String | No | . | dimensions.metrics.field.metrics | Specify the metric aggregations you want to calculate for the field. | Multiple strings | No | . Sample response . { \"_id\": \"rollup_id\", \"_seqNo\": 1, \"_primaryTerm\": 1, \"rollup\": { ... } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-rollups/rollup-api/#create-or-update-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/rollup-api/#create-or-update-an-index-rollup-job"
  },"31": {
    "doc": "Index rollups API",
    "title": "Get an index rollup job",
    "content": "Introduced 1.0 . Returns all information about an index rollup job based on the rollup_id. Request . GET _plugins/_rollup/jobs/&lt;rollup_id&gt; . Sample response . { \"_id\": \"my_rollup\", \"_seqNo\": 1, \"_primaryTerm\": 1, \"rollup\": { ... } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-rollups/rollup-api/#get-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/rollup-api/#get-an-index-rollup-job"
  },"32": {
    "doc": "Index rollups API",
    "title": "Delete an index rollup job",
    "content": "Introduced 1.0 . Deletes an index rollup job based on the rollup_id. Request . DELETE _plugins/_rollup/jobs/&lt;rollup_id&gt; . Sample response . 200 OK . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-rollups/rollup-api/#delete-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/rollup-api/#delete-an-index-rollup-job"
  },"33": {
    "doc": "Index rollups API",
    "title": "Start or stop an index rollup job",
    "content": "Introduced 1.0 . Start or stop an index rollup job. Request . POST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_start POST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_stop . Sample response . 200 OK . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-rollups/rollup-api/#start-or-stop-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/rollup-api/#start-or-stop-an-index-rollup-job"
  },"34": {
    "doc": "Index rollups API",
    "title": "Explain an index rollup job",
    "content": "Introduced 1.0 . Returns detailed metadata information about the index rollup job and its current progress. Request . GET _plugins/_rollup/jobs/&lt;rollup_id&gt;/_explain . Sample response . { \"example_rollup\": { \"rollup_id\": \"example_rollup\", \"last_updated_time\": 1602014281, \"continuous\": { \"next_window_start_time\": 1602055591, \"next_window_end_time\": 1602075591 }, \"status\": \"running\", \"failure_reason\": null, \"stats\": { \"pages_processed\": 342, \"documents_processed\": 489359, \"rollups_indexed\": 3420, \"index_time_in_ms\": 30495, \"search_time_in_ms\": 584922 } } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-rollups/rollup-api/#explain-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/rollup-api/#explain-an-index-rollup-job"
  },"35": {
    "doc": "Settings",
    "title": "Index rollup settings",
    "content": "We don’t recommend changing these settings; the defaults should work well for most use cases. All settings are available using the OpenSearch _cluster/settings operation. None require a restart, and all can be marked persistent or transient. | Setting | Default | Description | . | plugins.rollup.search.backoff_millis | 1000 milliseconds | The backoff time between retries for failed rollup jobs. | . | plugins.rollup.search.backoff_count | 5 | How many retries the plugin should attempt for failed rollup jobs. | . | plugins.rollup.search.search_all_jobs | false | Whether OpenSearch should return all jobs that match all specified search terms. If disabled, OpenSearch returns just one, as opposed to all, of the jobs that matches the search terms. | . | plugins.rollup.dashboards.enabled | true | Whether rollups are enabled in OpenSearch Dashboards. | . | plugins.rollup.enabled | true | Whether the rollup plugin is enabled. | . | plugins.ingest.backoff_millis | 1000 milliseconds | The backoff time between data ingestions for rollup jobs. | . | plugins.ingest.backoff_count | 5 | How many retries the plugin should attempt for failed ingestions. | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-rollups/settings/#index-rollup-settings",
    "relUrl": "/im-plugin/index-rollups/settings/#index-rollup-settings"
  },"36": {
    "doc": "Settings",
    "title": "Settings",
    "content": " ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-rollups/settings/",
    "relUrl": "/im-plugin/index-rollups/settings/"
  },"37": {
    "doc": "Index transforms",
    "title": "Index transforms",
    "content": "Whereas index rollup jobs let you reduce data granularity by rolling up old data into condensed indexes, transform jobs let you create a different, summarized view of your data centered around certain fields, so you can visualize or analyze the data in different ways. For example, suppose that you have airline data that’s scattered across multiple fields and categories, and you want to view a summary of the data that’s organized by airline, quarter, and then price. You can use a transform job to create a new, summarized index that’s organized by those specific categories. You can use transform jobs in two ways: . | Use the OpenSearch Dashboards UI to specify the index you want to transform and any optional data filters you want to use to filter the original index. Then select the fields you want to transform and the aggregations to use in the transformation. Finally, define a schedule for your job to follow. | Use the transforms API to specify all the details about your job: the index you want to transform, target groups you want the transformed index to have, any aggregations you want to use to group columns, and a schedule for your job to follow. | . OpenSearch Dashboards provides a detailed summary of the jobs you created and their relevant information, such as associated indexes and job statuses. You can review and edit your job’s details and selections before creation, and even preview a transformed index’s data as you’re choosing which fields to transform. However, you can also use the REST API to create transform jobs and preview transform job results, but you must know all of the necessary settings and parameters to submit them as part of the HTTP request body. Submitting your transform job configurations as JSON scripts offers you more portability, allowing you to share and replicate your transform jobs, which is harder to do using OpenSearch Dashboards. Your use cases will help you decide which method to use to create transform jobs. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-transforms/index/",
    "relUrl": "/im-plugin/index-transforms/index/"
  },"38": {
    "doc": "Index transforms",
    "title": "Create a transform job",
    "content": "If you don’t have any data in your cluster, you can use the sample flight data within OpenSearch Dashboards to try out transform jobs. Otherwise, after launching OpenSearch Dashboards, choose Index Management. Select Transform Jobs, and choose Create Transform Job. Step 1: Choose indexes . | In the Job name and description section, specify a name and an optional description for your job. | In the Indices section, select the source and target index. You can either select an existing target index or create a new one by entering a name for your new index. If you want to transform just a subset of your source index, choose Edit data filter, and use the OpenSearch query DSL to specify a subset of your source index. For more information about the OpenSearch query DSL, see query DSL. | Choose Next. | . Step 2: Select fields to transform . After specifying the indexes, you can select the fields you want to use in your transform job, as well as whether to use groupings or aggregations. You can use groupings to place your data into separate buckets in your transformed index. For example, if you want to group all of the airport destinations within the sample flight data, you can group the DestAirportID field into a target field of DestAirportID_terms field, and you can find the grouped airport IDs in your transformed index after the transform job finishes. On the other hand, aggregations let you perform simple calculations. For example, you can include an aggregation in your transform job to define a new field of sum_of_total_ticket_price that calculates the sum of all airplane tickets, and then analyze the newly summer data within your transformed index. | In the data table, select the fields you want to transform and expand the drop-down menu within the column header to choose the grouping or aggregation you want to use. Currently, transform jobs support histogram, date_histogram, and terms groupings. For more information about groupings, see Bucket Aggregations. In terms of aggregations, you can select from sum, avg, max, min, value_count, percentiles, and scripted_metric. For more information about aggregations, see Metric Aggregations. | Repeat step 1 for any other fields that you want to transform. | After selecting the fields that you want to transform and verifying the transformation, choose Next. | . Step 3: Specify a schedule . You can configure transform jobs to run once or multiple times on a schedule. Transform jobs are enabled by default. | Choose whether the job should be continuous. Continuous jobs execute at each transform execution interval and incrementally transform newly modified buckets, which can include new data added to the source indexes. Non-continuous jobs execute only once. | For transformation execution interval, specify a transform interval in minutes, hours, or days. This interval dicatates how often continuous jobs should execute, and non-continuous jobs execute once after the interval elapses. | Under Advanced, specify an optional amount for Pages per execution. A larger number means more data is processed in each search request, but also uses more memory and causes higher latency. Exceeding allowed memory limits can cause exceptions and errors to occur. | Choose Next. | . Step 4: Review and confirm details . After confirming your transform job’s details are correct, choose Create Transform Job. If you want to edit any part of the job, choose Edit of the section you want to change, and make the necessary changes. You can’t change aggregations or groupings after creating a job. Step 5: Search through the transformed index. Once the transform job finishes, you can use the _search API operation to search the target index. GET &lt;target_index&gt;/_search . For example, after running a transform job that transforms the flight data based on a DestAirportID field, you can run the following request that returns all of the fields that have a value of SFO. Sample Request . GET finished_flight_job/_search { \"query\": { \"match\": { \"DestAirportID_terms\" : \"SFO\" } } } . Sample Response . { \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 5, \"successful\" : 5, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 4, \"relation\" : \"eq\" }, \"max_score\" : 3.845883, \"hits\" : [ { \"_index\" : \"finished_flight_job\", \"_id\" : \"dSNKGb8U3OJOmC4RqVCi1Q\", \"_score\" : 3.845883, \"_source\" : { \"transform._id\" : \"sample_flight_job\", \"transform._doc_count\" : 14, \"Carrier_terms\" : \"Dashboards Airlines\", \"DestAirportID_terms\" : \"SFO\" } }, { \"_index\" : \"finished_flight_job\", \"_id\" : \"_D7oqOy7drx9E-MG96U5RA\", \"_score\" : 3.845883, \"_source\" : { \"transform._id\" : \"sample_flight_job\", \"transform._doc_count\" : 14, \"Carrier_terms\" : \"Logstash Airways\", \"DestAirportID_terms\" : \"SFO\" } }, { \"_index\" : \"finished_flight_job\", \"_id\" : \"YuZ8tOt1OsBA54e84WuAEw\", \"_score\" : 3.6988301, \"_source\" : { \"transform._id\" : \"sample_flight_job\", \"transform._doc_count\" : 11, \"Carrier_terms\" : \"ES-Air\", \"DestAirportID_terms\" : \"SFO\" } }, { \"_index\" : \"finished_flight_job\", \"_id\" : \"W_-e7bVmH6eu8veJeK8ZxQ\", \"_score\" : 3.6988301, \"_source\" : { \"transform._id\" : \"sample_flight_job\", \"transform._doc_count\" : 10, \"Carrier_terms\" : \"JetBeats\", \"DestAirportID_terms\" : \"SFO\" } } ] } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-transforms/index/#create-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/index/#create-a-transform-job"
  },"39": {
    "doc": "Transforms APIs",
    "title": "Transforms APIs",
    "content": "Aside from using OpenSearch Dashboards, you can also use the REST API to create, start, stop, and complete other operations relative to transform jobs. | Create a transform job . | Request format | Path parameters | Request body fields | . | Update a transform job . | Request format | Query parameters | Request body fields | . | Get a transform job’s details . | Request format | Query parameters | . | Start a transform job . | Request format | . | Stop a transform job . | Request format | . | Get the status of a transform job . | Request format | . | Preview a transform job’s results | Delete a transform job . | Request format | . | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-transforms/transforms-apis/",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/"
  },"40": {
    "doc": "Transforms APIs",
    "title": "Create a transform job",
    "content": "Introduced 1.0 . Creates a transform job. Request format . PUT _plugins/_transform/&lt;transform_id&gt; . Path parameters . | Parameter | Data Type | Description | . | transform_id | String | Transform ID | . Request body fields . You can specify the following options in the HTTP request body: . | Option | Data Type | Description | Required | . | enabled | Boolean | If true, the transform job is enabled at creation. | No | . | continuous | Boolean | Specifies whether the transform job should be continuous. Continuous jobs execute every time they are scheduled according to the schedule field and run based off of newly transformed buckets as well as any new data added to source indexes. Non-continuous jobs execute only once. Default is false. | No | . | schedule | Object | The schedule for the transform job. | Yes | . | start_time | Integer | The Unix epoch time of the transform job’s start time. | Yes | . | description | String | Describes the transform job. | No | . | metadata_id | String | Any metadata to be associated with the transform job. | No | . | source_index | String | The source index containing the data to be transformed. | Yes | . | target_index | String | The target index the newly transformed data is added to. You can create a new index or update an existing one. | Yes | . | data_selection_query | Object | The query DSL to use to filter a subset of the source index for the transform job. See query domain-specific language(DSL) for more information. | Yes | . | page_size | Integer | The number of buckets IM processes and indexes concurrently. A higher number results in better performance, but it requires more memory. If your machine runs out of memory, Index Management (IM) automatically adjusts this field and retries until the operation succeeds. | Yes | . | groups | Array | Specifies the grouping(s) to use in the transform job. Supported groups are terms, histogram, and date_histogram. For more information, see Bucket Aggregations. | Yes if not using aggregations. | . | source_field | String | The field(s) to transform. | Yes | . | aggregations | Object | The aggregations to use in the transform job. Supported aggregations are sum, max, min, value_count, avg, scripted_metric, and percentiles. For more information, see Metric Aggregations. | Yes if not using groups. | . Sample Request . The following request creates a transform job with the id sample: . PUT _plugins/_transform/sample { \"transform\": { \"enabled\": true, \"continuous\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . Sample Response . { \"_id\": \"sample\", \"_version\": 7, \"_seq_no\": 13, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample\", \"schema_version\": 7, \"continuous\": true, \"schedule\": { \"interval\": { \"start_time\": 1621467964243, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": null, \"updated_at\": 1621467964243, \"enabled\": true, \"enabled_at\": 1621467964243, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-transforms/transforms-apis/#create-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#create-a-transform-job"
  },"41": {
    "doc": "Transforms APIs",
    "title": "Update a transform job",
    "content": "Introduced 1.0 . Updates the transform job if transform_id already exists. For this request you must specify the sequence number and primary term of the transform to be updated. To get these, use the Get a transform job’s details API call. Request format . PUT _plugins/_transform/&lt;transform_id&gt;?if_seq_no=&lt;seq_no&gt;&amp;if_primary_term=&lt;primary_term&gt; . Query parameters . The update operation supports the following query parameters: . | Parameter | Description | Required | . | seq_no | Only perform the transform operation if the last operation that changed the transform job has the specified sequence number. | Yes | . | primary_term | Only perform the transform operation if the last operation that changed the transform job has the specified sequence term. | Yes | . Request body fields . You can update the following fields. | Option | Data Type | Description | . | schedule | Object | The schedule for the transform job. Contains the fields interval.start_time, interval.period, and interval.unit. | . | start_time | Integer | The Unix epoch start time of the transform job. | . | period | Integer | How often to execute the transform job. | . | unit | String | The unit of time associated with the execution period. Available options are Minutes, Hours, and Days. | . | description | Integer | Describes the transform job. | . | page_size | Integer | The number of buckets IM processes and indexes concurrently. A higher number results in better performance, but it requires more memory. If your machine runs out of memory, IM automatically adjusts this field and retries until the operation succeeds. | . Sample Request . The following request updates a transform job with the id sample, sequence number 13, and primary term 1: . PUT _plugins/_transform/sample?if_seq_no=13&amp;if_primary_term=1 { \"transform\": { \"enabled\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . Sample Response . PUT _plugins/_transform/sample?if_seq_no=13&amp;if_primary_term=1 { \"transform\": { \"enabled\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-transforms/transforms-apis/#update-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#update-a-transform-job"
  },"42": {
    "doc": "Transforms APIs",
    "title": "Get a transform job’s details",
    "content": "Introduced 1.0 . Returns a transform job’s details. Request format . GET _plugins/_transform/&lt;transform_id&gt; . Sample Request . The following request returns the details of the transform job with the id sample: . GET _plugins/_transform/sample . Sample Response . { \"_id\": \"sample\", \"_version\": 7, \"_seq_no\": 13, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample\", \"schema_version\": 7, \"continuous\": true, \"schedule\": { \"interval\": { \"start_time\": 1621467964243, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": null, \"updated_at\": 1621467964243, \"enabled\": true, \"enabled_at\": 1621467964243, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . You can also get details of all transform jobs by omitting transform_id. Sample Request . The following request returns the details of all transform jobs: . GET _plugins/_transform/ . Sample Response . { \"total_transforms\": 1, \"transforms\": [ { \"_id\": \"sample\", \"_seq_no\": 13, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample\", \"schema_version\": 7, \"continuous\": true, \"schedule\": { \"interval\": { \"start_time\": 1621467964243, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": null, \"updated_at\": 1621467964243, \"enabled\": true, \"enabled_at\": 1621467964243, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } ] } . Query parameters . You can specify the following GET API operation’s query parameters to filter the results. | Parameter | Description | Required | . | from | The starting transform to return. Default is 0. | No | . | size | Specifies the number of transforms to return. Default is 10. | No | . | search | The search term to use to filter results. | No | . | sortField | The field to sort results with. | No | . | sortDirection | Specifies the direction to sort results in. Can be ASC or DESC. Default is ASC. | No | . Sample Request . The following request returns two results starting from transform 8: . GET _plugins/_transform?size=2&amp;from=8 . Sample Response . { \"total_transforms\": 18, \"transforms\": [ { \"_id\": \"sample8\", \"_seq_no\": 93, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample8\", \"schema_version\": 7, \"schedule\": { \"interval\": { \"start_time\": 1622063596812, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": \"y4hFAB2ZURQ2dzY7BAMxWA\", \"updated_at\": 1622063657233, \"enabled\": false, \"enabled_at\": null, \"description\": \"Sample transform job\", \"source_index\": \"sample_index3\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target3\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } }, { \"_id\": \"sample9\", \"_seq_no\": 98, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample9\", \"schema_version\": 7, \"schedule\": { \"interval\": { \"start_time\": 1622063598065, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": \"x8tCIiYMTE3veSbIJkit5A\", \"updated_at\": 1622063658388, \"enabled\": false, \"enabled_at\": null, \"description\": \"Sample transform job\", \"source_index\": \"sample_index4\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target4\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } ] } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-transforms/transforms-apis/#get-a-transform-jobs-details",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#get-a-transform-jobs-details"
  },"43": {
    "doc": "Transforms APIs",
    "title": "Start a transform job",
    "content": "Introduced 1.0 . Transform jobs created using the API are automatically enabled, but if you ever need to enable a job, you can use the start API operation. Request format . POST _plugins/_transform/&lt;transform_id&gt;/_start . Sample Request . The following request starts the transform job with the ID sample: . POST _plugins/_transform/sample/_start . Sample Response . { \"acknowledged\": true } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-transforms/transforms-apis/#start-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#start-a-transform-job"
  },"44": {
    "doc": "Transforms APIs",
    "title": "Stop a transform job",
    "content": "Introduced 1.0 . Stops a transform job. Request format . POST _plugins/_transform/&lt;transform_id&gt;/_stop . Sample Request . The following request stops the transform job with the ID sample: . POST _plugins/_transform/sample/_stop . Sample Response . { \"acknowledged\": true } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-transforms/transforms-apis/#stop-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#stop-a-transform-job"
  },"45": {
    "doc": "Transforms APIs",
    "title": "Get the status of a transform job",
    "content": "Introduced 1.0 . Returns the status and metadata of a transform job. Request format . GET _plugins/_transform/&lt;transform_id&gt;/_explain . Sample Request . The following request returns the details of the transform job with the ID sample: . GET _plugins/_transform/sample/_explain . Sample Response . { \"sample\": { \"metadata_id\": \"PzmjweME5xbgkenl9UpsYw\", \"transform_metadata\": { \"continuous_stats\": { \"last_timestamp\": 1621883525672, \"documents_behind\": { \"sample_index\": 72 } }, \"transform_id\": \"sample\", \"last_updated_at\": 1621883525873, \"status\": \"finished\", \"failure_reason\": \"null\", \"stats\": { \"pages_processed\": 0, \"documents_processed\": 0, \"documents_indexed\": 0, \"index_time_in_millis\": 0, \"search_time_in_millis\": 0 } } } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-transforms/transforms-apis/#get-the-status-of-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#get-the-status-of-a-transform-job"
  },"46": {
    "doc": "Transforms APIs",
    "title": "Preview a transform job’s results",
    "content": "Introduced 1.0 . Returns a preview of what a transformed index would look like. Sample Request . POST _plugins/_transform/_preview { \"transform\": { \"enabled\": false, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"test transform\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 10, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . Sample Response . { \"documents\" : [ { \"quantity\" : 862.0, \"gender\" : \"FEMALE\", \"day\" : \"Friday\" }, { \"quantity\" : 682.0, \"gender\" : \"FEMALE\", \"day\" : \"Monday\" }, { \"quantity\" : 772.0, \"gender\" : \"FEMALE\", \"day\" : \"Saturday\" }, { \"quantity\" : 669.0, \"gender\" : \"FEMALE\", \"day\" : \"Sunday\" }, { \"quantity\" : 887.0, \"gender\" : \"FEMALE\", \"day\" : \"Thursday\" } ] } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-transforms/transforms-apis/#preview-a-transform-jobs-results",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#preview-a-transform-jobs-results"
  },"47": {
    "doc": "Transforms APIs",
    "title": "Delete a transform job",
    "content": "Introduced 1.0 . Deletes a transform job. This operation does not delete the source or target indexes. Request format . DELETE _plugins/_transform/&lt;transform_id&gt; . Sample Request . The following request deletes the transform job with the ID sample: . DELETE _plugins/_transform/sample . Sample Response . { \"took\": 205, \"errors\": false, \"items\": [ { \"delete\": { \"_index\": \".opensearch-ism-config\", \"_id\": \"sample\", \"_version\": 4, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 6, \"_primary_term\": 1, \"status\": 200 } } ] } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index-transforms/transforms-apis/#delete-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#delete-a-transform-job"
  },"48": {
    "doc": "About Index Management",
    "title": "About Index Management",
    "content": "OpenSearch Dashboards . The Index Management (IM) plugin lets you automate recurring index management activities and reduce storage costs. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/index/",
    "relUrl": "/im-plugin/index/"
  },"49": {
    "doc": "ISM API",
    "title": "ISM API",
    "content": "Use the index state management operations to programmatically work with policies and managed indexes. . | Create policy | Add policy | Update policy | Get policy | Remove policy from index | Update managed index policy | Retry failed index | Explain index | Delete policy | Error prevention validation | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/api/",
    "relUrl": "/im-plugin/ism/api/"
  },"50": {
    "doc": "ISM API",
    "title": "Create policy",
    "content": "Introduced 1.0 . Creates a policy. Sample request . PUT _plugins/_ism/policies/policy_1 { \"policy\": { \"description\": \"ingesting logs\", \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } } ], \"transitions\": [ { \"state_name\": \"search\" } ] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} } ], \"transitions\": [] } ] } } . Sample response . { \"_id\": \"policy_1\", \"_version\": 1, \"_primary_term\": 1, \"_seq_no\": 7, \"policy\": { \"policy\": { \"policy_id\": \"policy_1\", \"description\": \"ingesting logs\", \"last_updated_time\": 1577990761311, \"schema_version\": 1, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } } ], \"transitions\": [ { \"state_name\": \"search\" } ] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} } ], \"transitions\": [] } ] } } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/api/#create-policy",
    "relUrl": "/im-plugin/ism/api/#create-policy"
  },"51": {
    "doc": "ISM API",
    "title": "Add policy",
    "content": "Introduced 1.0 . Adds a policy to an index. This operation does not change the policy if the index already has one. Sample request . POST _plugins/_ism/add/index_1 { \"policy_id\": \"policy_1\" } . Sample response . { \"updated_indices\": 1, \"failures\": false, \"failed_indices\": [] } . If you use a wildcard * while adding a policy to an index, the ISM plugin interprets * as all indexes, including system indexes like .opendistro-security, which stores users, roles, and tenants. A delete action in your policy might accidentally delete all user roles and tenants in your cluster. Don’t use the broad * wildcard, and instead add a prefix, such as my-logs*, when specifying indexes with the _ism/add API. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/api/#add-policy",
    "relUrl": "/im-plugin/ism/api/#add-policy"
  },"52": {
    "doc": "ISM API",
    "title": "Update policy",
    "content": "Introduced 1.0 . Updates a policy. Use the seq_no and primary_term parameters to update an existing policy. If these numbers don’t match the existing policy or the policy doesn’t exist, ISM throws an error. It’s possible that the policy currently applied to your index isn’t the most up-to-date policy available. To see what policy is currently applied to your index, see Explain index. To get the most up-to-date version of a policy, see Get policy. Sample request . PUT _plugins/_ism/policies/policy_1?if_seq_no=7&amp;if_primary_term=1 { \"policy\": { \"description\": \"ingesting logs\", \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } } ], \"transitions\": [ { \"state_name\": \"search\" } ] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} } ], \"transitions\": [] } ] } } . Sample response . { \"_id\": \"policy_1\", \"_version\": 2, \"_primary_term\": 1, \"_seq_no\": 10, \"policy\": { \"policy\": { \"policy_id\": \"policy_1\", \"description\": \"ingesting logs\", \"last_updated_time\": 1577990934044, \"schema_version\": 1, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } } ], \"transitions\": [ { \"state_name\": \"search\" } ] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} } ], \"transitions\": [] } ] } } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/api/#update-policy",
    "relUrl": "/im-plugin/ism/api/#update-policy"
  },"53": {
    "doc": "ISM API",
    "title": "Get policy",
    "content": "Introduced 1.0 . Gets the policy by policy_id. Sample request . GET _plugins/_ism/policies/policy_1 . Sample response . { \"_id\": \"policy_1\", \"_version\": 2, \"_seq_no\": 10, \"_primary_term\": 1, \"policy\": { \"policy_id\": \"policy_1\", \"description\": \"ingesting logs\", \"last_updated_time\": 1577990934044, \"schema_version\": 1, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } } ], \"transitions\": [ { \"state_name\": \"search\" } ] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} } ], \"transitions\": [] } ] } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/api/#get-policy",
    "relUrl": "/im-plugin/ism/api/#get-policy"
  },"54": {
    "doc": "ISM API",
    "title": "Remove policy from index",
    "content": "Introduced 1.0 . Removes any ISM policy from the index. Sample request . POST _plugins/_ism/remove/index_1 . Sample response . { \"updated_indices\": 1, \"failures\": false, \"failed_indices\": [] } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/api/#remove-policy-from-index",
    "relUrl": "/im-plugin/ism/api/#remove-policy-from-index"
  },"55": {
    "doc": "ISM API",
    "title": "Update managed index policy",
    "content": "Introduced 1.0 . Updates the managed index policy to a new policy (or to a new version of the policy). You can use an index pattern to update multiple indexes at once. When updating multiple indexes, you might want to include a state filter to only affect certain managed indexes. The change policy filters out all the existing managed indexes and only applies the change to the ones in the state that you specify. You can also explicitly specify the state that the managed index transitions to after the change policy takes effect. A policy change is an asynchronous background process. The changes are queued and are not executed immediately by the background process. This delay in execution protects the currently running managed indexes from being put into a broken state. If the policy you are changing to has only some small configuration changes, then the change takes place immediately. For example, if the policy changes the min_index_age parameter in a rollover condition from 1000d to 100d, this change takes place immediately in its next execution. If the change modifies the state, actions, or the order of actions of the current state the index is in, then the change happens at the end of its current state before transitioning to a new state. In this example, the policy applied on the index_1 index is changed to policy_1, which could either be a completely new policy or an updated version of its existing policy. The process only applies the change if the index is currently in the searches state. After this change in policy takes place, index_1 transitions to the delete state. Sample request . POST _plugins/_ism/change_policy/index_1 { \"policy_id\": \"policy_1\", \"state\": \"delete\", \"include\": [ { \"state\": \"searches\" } ] } . Sample response . { \"updated_indices\": 0, \"failures\": false, \"failed_indices\": [] } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/api/#update-managed-index-policy",
    "relUrl": "/im-plugin/ism/api/#update-managed-index-policy"
  },"56": {
    "doc": "ISM API",
    "title": "Retry failed index",
    "content": "Introduced 1.0 . Retries the failed action for an index. For the retry call to succeed, ISM must manage the index, and the index must be in a failed state. You can use index patterns (*) to retry multiple failed indexes. Sample request . POST _plugins/_ism/retry/index_1 { \"state\": \"delete\" } . Sample response . { \"updated_indices\": 0, \"failures\": false, \"failed_indices\": [] } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/api/#retry-failed-index",
    "relUrl": "/im-plugin/ism/api/#retry-failed-index"
  },"57": {
    "doc": "ISM API",
    "title": "Explain index",
    "content": "Introduced 1.0 . Gets the current state of the index. You can use index patterns to get the status of multiple indexes. Sample request . GET _plugins/_ism/explain/index_1 . Sample response . { \"index_1\": { \"index.plugins.index_state_management.policy_id\": \"policy_1\" } } . Optionally, you can add the show_policy parameter to your request’s path to get the policy that is currently applied to your index, which is useful for seeing whether the policy applied to your index is the latest one. To get the most up-to-date policy, see Get Policy API. Sample request . GET _plugins/_ism/explain/index_1?show_policy=true . Sample response . { \"index_1\": { \"index.plugins.index_state_management.policy_id\": \"sample-policy\", \"index.opendistro.index_state_management.policy_id\": \"sample-policy\", \"index\": \"index_1\", \"index_uuid\": \"gCFlS_zcTdih8xyxf3jQ-A\", \"policy_id\": \"sample-policy\", \"enabled\": true, \"policy\": { \"policy_id\": \"sample-policy\", \"description\": \"ingesting logs\", \"last_updated_time\": 1647284980148, \"schema_version\": 13, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [...], \"ism_template\": null } }, \"total_managed_indices\": 1 } . The plugins.index_state_management.policy_id setting is deprecated starting from ODFE version 1.13.0. We retain this field in the response API for consistency. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/api/#explain-index",
    "relUrl": "/im-plugin/ism/api/#explain-index"
  },"58": {
    "doc": "ISM API",
    "title": "Delete policy",
    "content": "Introduced 1.0 . Deletes the policy by policy_id. Sample request . DELETE _plugins/_ism/policies/policy_1 . Sample response . { \"_index\": \".opendistro-ism-config\", \"_id\": \"policy_1\", \"_version\": 3, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 15, \"_primary_term\": 1 } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/api/#delete-policy",
    "relUrl": "/im-plugin/ism/api/#delete-policy"
  },"59": {
    "doc": "ISM API",
    "title": "Error prevention validation",
    "content": "Introduced 2.4 . ISM allows you to run an action automatically. However, running an action can fail for a variety of reasons. You can use error prevention validation to test an action in order to rule out failures. To enable error prevention validation, set the plugins.index_state_management.validation_service.enabled setting to true: . PUT _cluster/settings { \"persistent\":{ \"plugins.index_state_management.validation_action.enabled\": true } } . Sample response . { \"acknowledged\" : true, \"persistent\" : { \"plugins\" : { \"index_state_management\" : { \"validation_action\" : { \"enabled\" : \"true\" } } } }, \"transient\" : { } } . To check an error prevention validation status and message, pass validate_action=true to the _plugins/_ism/explain endpoint: . GET _plugins/_ism/explain/test-000001?validate_action=true . Sample response . The response contains an additional validate object with a validation message and status: . { \"test-000001\" : { \"index.plugins.index_state_management.policy_id\" : \"test_rollover\", \"index.opendistro.index_state_management.policy_id\" : \"test_rollover\", \"index\" : \"test-000001\", \"index_uuid\" : \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\" : \"test_rollover\", \"policy_seq_no\" : -2, \"policy_primary_term\" : 0, \"rolled_over\" : false, \"index_creation_date\" : 1667410460649, \"state\" : { \"name\" : \"rollover\", \"start_time\" : 1667410766045 }, \"action\" : { \"name\" : \"rollover\", \"start_time\" : 1667411127803, \"index\" : 0, \"failed\" : false, \"consumed_retries\" : 0, \"last_retry_time\" : 0 }, \"step\" : { \"name\" : \"attempt_rollover\", \"start_time\" : 1667411127803, \"step_status\" : \"starting\" }, \"retry_info\" : { \"failed\" : true, \"consumed_retries\" : 0 }, \"info\" : { \"message\" : \"Previous action was not able to update IndexMetaData.\" }, \"enabled\" : false, \"validate\" : { \"validation_message\" : \"Missing rollover_alias index setting [index=test-000001]\", \"validation_status\" : \"re_validating\" } }, \"total_managed_indices\" : 1 } . If you pass validate_action=false or do not pass a validate_action value to the _plugins/_ism/explain endpoint, the response will not contain an error prevention validation status and message: . GET _plugins/_ism/explain/test-000001?validate_action=false . Or: . GET _plugins/_ism/explain/test-000001 . Sample response . { \"test-000001\" : { \"index.plugins.index_state_management.policy_id\" : \"test_rollover\", \"index.opendistro.index_state_management.policy_id\" : \"test_rollover\", \"index\" : \"test-000001\", \"index_uuid\" : \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\" : \"test_rollover\", \"policy_seq_no\" : -2, \"policy_primary_term\" : 0, \"rolled_over\" : false, \"index_creation_date\" : 1667410460649, \"state\" : { \"name\" : \"rollover\", \"start_time\" : 1667410766045 }, \"action\" : { \"name\" : \"rollover\", \"start_time\" : 1667411127803, \"index\" : 0, \"failed\" : false, \"consumed_retries\" : 0, \"last_retry_time\" : 0 }, \"step\" : { \"name\" : \"attempt_rollover\", \"start_time\" : 1667411127803, \"step_status\" : \"starting\" }, \"retry_info\" : { \"failed\" : true, \"consumed_retries\" : 0 }, \"info\" : { \"message\" : \"Previous action was not able to update IndexMetaData.\" }, \"enabled\" : false }, \"total_managed_indices\" : 1 } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/api/#error-prevention-validation",
    "relUrl": "/im-plugin/ism/api/#error-prevention-validation"
  },"60": {
    "doc": "ISM Error Prevention API",
    "title": "ISM Error Prevention API",
    "content": "The ISM Error Prevention API allows you to enable Index State Management (ISM) error prevention and check the validation status and message. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/api/",
    "relUrl": "/im-plugin/ism/error-prevention/api/"
  },"61": {
    "doc": "ISM Error Prevention API",
    "title": "Enable error prevention validation",
    "content": "You can configure error prevention validation by setting the plugins.index_state_management.validation_service.enabled parameter. Sample request . PUT _cluster/settings { \"persistent\":{ \"plugins.index_state_management.validation_action.enabled\": true } } . Sample response . { \"acknowledged\" : true, \"persistent\" : { \"plugins\" : { \"index_state_management\" : { \"validation_action\" : { \"enabled\" : \"true\" } } } }, \"transient\" : { } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/api/#enable-error-prevention-validation",
    "relUrl": "/im-plugin/ism/error-prevention/api/#enable-error-prevention-validation"
  },"62": {
    "doc": "ISM Error Prevention API",
    "title": "Check validation status and message via the Explain API",
    "content": "Pass the validate_action=true path parameter in the Explain API URI to see the validation status and message. Sample request . GET _plugins/_ism/explain/test-000001?validate_action=true . Sample response . { \"test-000001\" : { \"index.plugins.index_state_management.policy_id\" : \"test_rollover\", \"index.opendistro.index_state_management.policy_id\" : \"test_rollover\", \"index\" : \"test-000001\", \"index_uuid\" : \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\" : \"test_rollover\", \"policy_seq_no\" : -2, \"policy_primary_term\" : 0, \"rolled_over\" : false, \"index_creation_date\" : 1667410460649, \"state\" : { \"name\" : \"rollover\", \"start_time\" : 1667410766045 }, \"action\" : { \"name\" : \"rollover\", \"start_time\" : 1667411127803, \"index\" : 0, \"failed\" : false, \"consumed_retries\" : 0, \"last_retry_time\" : 0 }, \"step\" : { \"name\" : \"attempt_rollover\", \"start_time\" : 1667411127803, \"step_status\" : \"starting\" }, \"retry_info\" : { \"failed\" : true, \"consumed_retries\" : 0 }, \"info\" : { \"message\" : \"Previous action was not able to update IndexMetaData.\" }, \"enabled\" : false, \"validate\" : { \"validation_message\" : \"Missing rollover_alias index setting [index=test-000001]\", \"validation_status\" : \"re_validating\" } }, \"total_managed_indices\" : 1 } . If you pass the parameter without a value or false, then it doesn’t return the validation status and message. Only if you pass validate_action=true will the response will return the validation status and message. Sample request . GET _plugins/_ism/explain/test-000001?validate_action=false --- OR --- GET _plugins/_ism/explain/test-000001 . Sample response . { \"test-000001\" : { \"index.plugins.index_state_management.policy_id\" : \"test_rollover\", \"index.opendistro.index_state_management.policy_id\" : \"test_rollover\", \"index\" : \"test-000001\", \"index_uuid\" : \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\" : \"test_rollover\", \"policy_seq_no\" : -2, \"policy_primary_term\" : 0, \"rolled_over\" : false, \"index_creation_date\" : 1667410460649, \"state\" : { \"name\" : \"rollover\", \"start_time\" : 1667410766045 }, \"action\" : { \"name\" : \"rollover\", \"start_time\" : 1667411127803, \"index\" : 0, \"failed\" : false, \"consumed_retries\" : 0, \"last_retry_time\" : 0 }, \"step\" : { \"name\" : \"attempt_rollover\", \"start_time\" : 1667411127803, \"step_status\" : \"starting\" }, \"retry_info\" : { \"failed\" : true, \"consumed_retries\" : 0 }, \"info\" : { \"message\" : \"Previous action was not able to update IndexMetaData.\" }, \"enabled\" : false }, \"total_managed_indices\" : 1 } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/api/#check-validation-status-and-message-via-the-explain-api",
    "relUrl": "/im-plugin/ism/error-prevention/api/#check-validation-status-and-message-via-the-explain-api"
  },"63": {
    "doc": "ISM Error Prevention",
    "title": "ISM error prevention",
    "content": "Error prevention validates Index State Management (ISM) actions before they are performed in order to prevent actions from failing. It also outputs additional information from the action validation results in the response of the Index Explain API. Validation rules and troubleshooting of each action are listed in the following sections. . | rollover | delete | force_merge | replica_count | open | read_only | read_write | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/index/#ism-error-prevention",
    "relUrl": "/im-plugin/ism/error-prevention/index/#ism-error-prevention"
  },"64": {
    "doc": "ISM Error Prevention",
    "title": "rollover",
    "content": "ISM does not perform a rollover action for an index under any of these conditions: . | The index is not the write index. | The index does not have an alias. | The rollover policy does not contain a rollover_alias index setting. | Skipping of a rollover action has occured. | The index has already been rolled over using the alias successfully. | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/index/#rollover",
    "relUrl": "/im-plugin/ism/error-prevention/index/#rollover"
  },"65": {
    "doc": "ISM Error Prevention",
    "title": "delete",
    "content": "ISM does not perform a delete action for an index under any of these conditions: . | The index does not exist. | The index name is invalid. | The index is the write index for a data stream. | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/index/#delete",
    "relUrl": "/im-plugin/ism/error-prevention/index/#delete"
  },"66": {
    "doc": "ISM Error Prevention",
    "title": "force_merge",
    "content": "ISM does not perform a force_merge action for an index if its dataset is too large and exceeds the threshold. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/index/#force_merge",
    "relUrl": "/im-plugin/ism/error-prevention/index/#force_merge"
  },"67": {
    "doc": "ISM Error Prevention",
    "title": "replica_count",
    "content": "ISM does not perform a replica_count action for an index under any of these conditions: . | The amount of data exceeds the threshold. | The number of shards exceeds the maximum. | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/index/#replica_count",
    "relUrl": "/im-plugin/ism/error-prevention/index/#replica_count"
  },"68": {
    "doc": "ISM Error Prevention",
    "title": "open",
    "content": "ISM does not perform an open action for an index under any of these conditions: . | The index is blocked. | The number of shards exceeds the maximum. | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/index/#open",
    "relUrl": "/im-plugin/ism/error-prevention/index/#open"
  },"69": {
    "doc": "ISM Error Prevention",
    "title": "read_only",
    "content": "ISM does not perform a read_only action for an index under any of these conditions: . | The index is blocked. | The amount of data exceeds the threshold. | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/index/#read_only",
    "relUrl": "/im-plugin/ism/error-prevention/index/#read_only"
  },"70": {
    "doc": "ISM Error Prevention",
    "title": "read_write",
    "content": "ISM does not perform a read_write action for an index if the index is blocked. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/index/#read_write",
    "relUrl": "/im-plugin/ism/error-prevention/index/#read_write"
  },"71": {
    "doc": "ISM Error Prevention",
    "title": "ISM Error Prevention",
    "content": " ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/index/",
    "relUrl": "/im-plugin/ism/error-prevention/index/"
  },"72": {
    "doc": "ISM Error Prevention resolutions",
    "title": "ISM error prevention resolutions",
    "content": "Resolutions of errors for each validation rule action are listed in the following sections. . | The index is not the write index | The index does not have an alias | Skipping rollover action is true | This index has already been rolled over successfully | The rollover policy misses rollover_alias index setting | Data too large and exceeding the threshold | Maximum shards exceeded | The index is a write index for some data stream | The index is blocked | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/resolutions/#ism-error-prevention-resolutions",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#ism-error-prevention-resolutions"
  },"73": {
    "doc": "ISM Error Prevention resolutions",
    "title": "The index is not the write index",
    "content": "To confirm that the index is a write index, run the following request: . GET &lt;index&gt;/_alias?pretty . If the response does not contain \"is_write_index\" : true, the index is not a write index. The following example confirms that the index is a write index: . { \"&lt;index&gt;\" : { \"aliases\" : { \"&lt;index_alias&gt;\" : { \"is_write_index\" : true } } } } . To set the index as a write index, run the following request: . PUT &lt;index&gt; { \"aliases\": { \"&lt;index_alias&gt;\" : { \"is_write_index\" : true } } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/resolutions/#the-index-is-not-the-write-index",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#the-index-is-not-the-write-index"
  },"74": {
    "doc": "ISM Error Prevention resolutions",
    "title": "The index does not have an alias",
    "content": "If the index does not have an alias, you can add one by running the following request: . POST _aliases { \"actions\": [ { \"add\": { \"index\": \"&lt;target_index&gt;\", \"alias\": \"&lt;index_alias&gt;\" } } ] } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/resolutions/#the-index-does-not-have-an-alias",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#the-index-does-not-have-an-alias"
  },"75": {
    "doc": "ISM Error Prevention resolutions",
    "title": "Skipping rollover action is true",
    "content": "In the event that skipping a rollover action occurs, run the following request: . GET &lt;target_index&gt;/_settings?pretty . If you receive the response in the first example, you can reset it by running the request in the second example: . { \"index\": { \"opendistro.index_state_management.rollover_skip\": true } } . PUT &lt;target_index&gt;/_settings { \"index\": { \"index_state_management.rollover_skip\": false } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/resolutions/#skipping-rollover-action-is-true",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#skipping-rollover-action-is-true"
  },"76": {
    "doc": "ISM Error Prevention resolutions",
    "title": "This index has already been rolled over successfully",
    "content": "Remove the rollover policy from the index to prevent this error from reoccurring. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/resolutions/#this-index-has-already-been-rolled-over-successfully",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#this-index-has-already-been-rolled-over-successfully"
  },"77": {
    "doc": "ISM Error Prevention resolutions",
    "title": "The rollover policy misses rollover_alias index setting",
    "content": "Add a rollover_alias index setting to the rollover policy to resolve this issue. Run the following request: . PUT _index_template/ism_rollover { \"index_patterns\": [\"&lt;index_patterns_in_rollover_policy&gt;\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"&lt;rollover_alias&gt;\" } } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/resolutions/#the-rollover-policy-misses-rollover_alias-index-setting",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#the-rollover-policy-misses-rollover_alias-index-setting"
  },"78": {
    "doc": "ISM Error Prevention resolutions",
    "title": "Data too large and exceeding the threshold",
    "content": "Check the JVM information and increase the heap memory. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/resolutions/#data-too-large-and-exceeding-the-threshold",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#data-too-large-and-exceeding-the-threshold"
  },"79": {
    "doc": "ISM Error Prevention resolutions",
    "title": "Maximum shards exceeded",
    "content": "The shard limit per node, or per index, causes this issue to occur. Check whether there is a total_shards_per_node limit by running the following request: . GET /_cluster/settings . If the response contains total_shards_per_node, increase its value temporarily by running the following request: . PUT _cluster/settings { \"transient\":{ \"cluster.routing.allocation.total_shards_per_node\":100 } } . To check whether there is a shard limit for an index, run the following request: . GET &lt;index&gt;/_settings/index.routing- . If the response contains the setting in the first example, increase its value or set it to -1 for unlimited shards, as shown in the second example: . \"index\" : { \"routing\" : { \"allocation\" : { \"total_shards_per_node\" : \"10\" } } } . PUT &lt;index&gt;/_settings {\"index.routing.allocation.total_shards_per_node\":-1} . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/resolutions/#maximum-shards-exceeded",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#maximum-shards-exceeded"
  },"80": {
    "doc": "ISM Error Prevention resolutions",
    "title": "The index is a write index for some data stream",
    "content": "If you still want to delete the index, check your data stream settings and change the write index. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/resolutions/#the-index-is-a-write-index-for-some-data-stream",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#the-index-is-a-write-index-for-some-data-stream"
  },"81": {
    "doc": "ISM Error Prevention resolutions",
    "title": "The index is blocked",
    "content": "Generally, the index is blocked because disk usage has exceeded the flood-stage watermark and the index has a read-only-allow-delete block. To resolve this issue, you can: . | Remove the -index.blocks.read_only_allow_delete- parameter. | Temporarily increase the disk watermarks. | Temporarily disable the disk allocation threshold. | . To prevent the issue from reoccurring, it is better to reduce the usage of the disk by increasing disk space, adding new nodes, or removing data or indexes that are no longer needed. Remove -index.blocks.read_only_allow_delete- by running the following request: . PUT &lt;index&gt;/_settings { \"index.blocks.read_only_allow_delete\": null } . Increase the low disk watermarks by running the following request: . PUT _cluster/settings { \"transient\": { \"cluster\": { \"routing\": { \"allocation\": { \"disk\": { \"watermark\": { \"low\": \"25.0gb\" } } } } } } } . Disable the disk allocation threshold by running the following request: . PUT _cluster/settings { \"transient\": { \"cluster\": { \"routing\": { \"allocation\": { \"disk\": { \"threshold_enabled\" : false } } } } } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/resolutions/#the-index-is-blocked",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#the-index-is-blocked"
  },"82": {
    "doc": "ISM Error Prevention resolutions",
    "title": "ISM Error Prevention resolutions",
    "content": " ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/error-prevention/resolutions/",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/"
  },"83": {
    "doc": "Index State Management",
    "title": "Index State Management",
    "content": "OpenSearch Dashboards . If you analyze time-series data, you likely prioritize new data over old data. You might periodically perform certain operations on older indexes, such as reducing replica count or deleting them. Index State Management (ISM) is a plugin that lets you automate these periodic, administrative operations by triggering them based on changes in the index age, index size, or number of documents. Using the ISM plugin, you can define policies that automatically handle index rollovers or deletions to fit your use case. For example, you can define a policy that moves your index into a read_only state after 30 days and then deletes it after a set period of 90 days. You can also set up the policy to send you a notification message when the index is deleted. You might want to perform an index rollover after a certain amount of time or run a force_merge operation on an index during off-peak hours to improve search performance during peak hours. To use the ISM plugin, your user role needs to be mapped to the all_access role that gives you full access to the cluster. To learn more, see Users and roles. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/index/",
    "relUrl": "/im-plugin/ism/index/"
  },"84": {
    "doc": "Index State Management",
    "title": "Get started with ISM",
    "content": "To get started, choose Index Management in OpenSearch Dashboards. Step 1: Set up policies . A policy is a set of rules that describes how an index should be managed. For information about creating a policy, see Policies. You can use the visual editor or JSON editor to create policies. Compared to the JSON editor, the visual editor offers a more structured way of defining policies by separating the process into creating error notifications, defining ISM templates, and adding states. We recommend using the visual editor if you want to see pre-defined fields, such as which actions you can assign to a state or under what conditions a state can transition into a destination state. Visual editor . | Choose the Index Policies tab. | Choose Create policy. | Choose Visual editor. | In the Policy info section, enter a policy ID and an optional description. | In the Error notification section, set up an optional error notification that gets sent whenever a policy execution fails. For more information, see Error notifications. If you’re using auto rollovers in your policy, we recommend setting up error notifications, which notify you of unexpectedly large indexes if rollovers fail. | In ISM templates, enter any ISM template patterns to automatically apply this policy to future indexes. For example, if you specify a template of sample-index*, the ISM plugin automatically applies this policy to any indexes whose names start with sample-index. Your pattern cannot contain any of the following characters: :, \", +, /, \\, |, ?, #, &gt;, and &lt;. | In States, add any states you want to include in the policy. Each state has actions the plugin executes when the index enters a certain state, and transitions, which have conditions that, when met, transition the index into a destination state. The first state you create in a policy is automatically set as the initial state. Each policy must have at least one state, but actions and transitions are optional. | Choose Create. | . JSON editor . | Choose the Index Policies tab. | Choose Create policy. | Choose JSON editor. | In the Name policy section, enter a policy ID. | In the Define policy section, enter your policy. | Choose Create. | . After you create a policy, your next step is to attach it to an index or indexes. You can set up an ism_template in the policy so when an index that matches the ISM template pattern is created, the plugin automatically attaches the policy to the index. The following example demonstrates how to create a policy that automatically gets attached to all indexes whose names start with index_name-. PUT _plugins/_ism/policies/policy_id { \"policy\": { \"description\": \"Example policy.\", \"default_state\": \"...\", \"states\": [...], \"ism_template\": { \"index_patterns\": [\"index_name-*\"], \"priority\": 100 } } } . If you have more than one template that matches an index pattern, ISM uses the priority value to determine which template to apply. For an example ISM template policy, see Sample policy with ISM template for auto rollover. Older versions of the plugin include the policy_id in an index template, so when an index is created that matches the index template pattern, the index will have the policy attached to it: . PUT _index_template/&lt;template_name&gt; { \"index_patterns\": [ \"index_name-*\" ], \"template\": { \"settings\": { \"opendistro.index_state_management.policy_id\": \"policy_id\" } } } . The opendistro.index_state_management.policy_id setting is deprecated. You can continue to automatically manage newly created indexes with the ISM template field. Step 2: Attach policies to indexes . | Choose indexes. | Choose the index or indexes that you want to attach your policy to. | Choose Apply policy. | From the Policy ID menu, choose the policy that you created. You can see a preview of your policy. | If your policy includes a rollover operation, specify a rollover alias. Make sure that the alias that you enter already exists. For more information about the rollover operation, see rollover. | Choose Apply. | . After you attach a policy to an index, ISM creates a job that runs every 5 minutes by default to perform policy actions, check conditions, and transition the index into different states. To change the default time interval for this job, see Settings. ISM does not run jobs if the cluster state is red. Step 3: Manage indexes . | Choose Managed indexes. | To change your policy, see Change Policy. | To attach a rollover alias to your index, select your policy and choose Add rollover alias. Make sure that the alias that you enter already exists. For more information about the rollover operation, see rollover. | To remove a policy, choose your policy, and then choose Remove policy. | To retry a policy, choose your policy, and then choose Retry policy. | . For information about managing your policies, see Managed indexes. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/index/#get-started-with-ism",
    "relUrl": "/im-plugin/ism/index/#get-started-with-ism"
  },"85": {
    "doc": "Managed Indices",
    "title": "Managed indices",
    "content": "You can change or update a policy using the managed index operations. This table lists the fields of managed index operations. | Parameter | Description | Type | Required | Read Only | . | name | The name of the managed index policy. | string | Yes | No | . | index | The name of the managed index that this policy is managing. | string | Yes | No | . | index_uuid | The uuid of the index. | string | Yes | No | . | enabled | When true, the managed index is scheduled and run by the scheduler. | boolean | Yes | No | . | enabled_time | The time the managed index was last enabled. If the managed index process is disabled, then this is null. | timestamp | Yes | Yes | . | last_updated_time | The time the managed index was last updated. | timestamp | Yes | Yes | . | schedule | The schedule of the managed index job. | object | Yes | No | . | policy_id | The name of the policy used by this managed index. | string | Yes | No | . | policy_seq_no | The sequence number of the policy used by this managed index. | number | Yes | No | . | policy_primary_term | The primary term of the policy used by this managed index. | number | Yes | No | . | policy_version | The version of the policy used by this managed index. | number | Yes | Yes | . | policy | The cached JSON of the policy for the policy_version that’s used during runs. If the policy is null, it means that this is the first execution of the job and the latest policy document is read in/saved. | object | No | No | . | change_policy | The information regarding what policy and state to change to. | object | No | No | . | policy_name | The name of the policy to update to. To update to the latest version, set this to be the same as the current policy_name. | string | No | Yes | . | state | The state of the managed index after it finishes updating. If no state is specified, it’s assumed that the policy structure did not change. | string | No | Yes | . The following example shows a managed index policy: . { \"managed_index\": { \"name\": \"my_index\", \"index\": \"my_index\", \"index_uuid\": \"sOKSOfkdsoSKeofjIS\", \"enabled\": true, \"enabled_time\": 1553112384, \"last_updated_time\": 1553112384, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"MINUTES\", \"start_time\": 1553112384 } }, \"policy_id\": \"log_rotation\", \"policy_version\": 1, \"policy\": {...}, \"change_policy\": null } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/managedindexes/#managed-indices",
    "relUrl": "/im-plugin/ism/managedindexes/#managed-indices"
  },"86": {
    "doc": "Managed Indices",
    "title": "Change policy",
    "content": "You can change any managed index policy, but ISM has a few constraints in place to make sure that policy changes don’t break indices. If an index is stuck in its current state, never proceeding, and you want to update its policy immediately, make sure that the new policy includes the same state—same name, same actions, same order—as the old policy. In this case, even if the policy is in the middle of executing an action, ISM applies the new policy. If you update the policy without including an identical state, ISM updates the policy only after all actions in the current state finish executing. Alternately, you can choose a specific state in your old policy after which you want the new policy to take effect. To change a policy using OpenSearch Dashboards, do the following: . | Under Managed indices, choose the indices that you want to attach the new policy to. | To attach the new policy to indices in specific states, choose Choose state filters, and then choose those states. | Under Choose New Policy, choose the new policy. | To start the new policy for indices in the current state, choose Keep indices in their current state after the policy takes effect. | To start the new policy in a specific state, choose Start from a chosen state after changing policies, and then choose the default start state in your new policy. | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/managedindexes/#change-policy",
    "relUrl": "/im-plugin/ism/managedindexes/#change-policy"
  },"87": {
    "doc": "Managed Indices",
    "title": "Managed Indices",
    "content": " ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/managedindexes/",
    "relUrl": "/im-plugin/ism/managedindexes/"
  },"88": {
    "doc": "Policies",
    "title": "Policies",
    "content": "Policies are JSON documents that define the following: . | The states that an index can be in, including the default state for new indexes. For example, you might name your states “hot,” “warm,” “delete,” and so on. For more information, see States. | Any actions that you want the plugin to take when an index enters a state, such as performing a rollover. For more information, see Actions. | The conditions that must be met for an index to move into a new state, known as transitions. For example, if an index is more than eight weeks old, you might want to move it to the “delete” state. For more information, see Transitions. | . In other words, a policy defines the states that an index can be in, the actions to perform when in a state, and the conditions that must be met to transition between states. You have complete flexibility in the way you can design your policies. You can create any state, transition to any other state, and specify any number of actions in each state. This table lists the relevant fields of a policy. | Field | Description | Type | Required | Read Only | . | policy_id | The name of the policy. | string | Yes | Yes | . | description | A human-readable description of the policy. | string | Yes | No | . | ism_template | Specify an ISM template pattern that matches the index to apply the policy. | nested list of objects | No | No | . | last_updated_time | The time the policy was last updated. | timestamp | Yes | Yes | . | error_notification | The destination and message template for error notifications. The destination could be Amazon Chime, Slack, or a webhook URL. | object | No | No | . | default_state | The default starting state for each index that uses this policy. | string | Yes | No | . | states | The states that you define in the policy. | nested list of objects | Yes | No | . | States | Actions | ISM supported operations . | force_merge | read_only | read_write | replica_count | shrink | close | open | delete | rollover | notification | snapshot | index_priority | allocation | rollup | . | Transitions | Error notifications | Sample policy with ISM template for auto rollover | Example policy with ISM templates for the alias action | Example policy | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/policies/",
    "relUrl": "/im-plugin/ism/policies/"
  },"89": {
    "doc": "Policies",
    "title": "States",
    "content": "A state is the description of the status that the managed index is currently in. A managed index can be in only one state at a time. Each state has associated actions that are executed sequentially on entering a state and transitions that are checked after all the actions have been completed. This table lists the parameters that you can define for a state. | Field | Description | Type | Required | . | name | The name of the state. | string | Yes | . | actions | The actions to execute after entering a state. For more information, see Actions. | nested list of objects | Yes | . | transitions | The next states and the conditions required to transition to those states. If no transitions exist, the policy assumes that it’s complete and can now stop managing the index. For more information, see Transitions. | nested list of objects | Yes | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/policies/#states",
    "relUrl": "/im-plugin/ism/policies/#states"
  },"90": {
    "doc": "Policies",
    "title": "Actions",
    "content": "Actions are the steps that the policy sequentially executes on entering a specific state. ISM executes actions in the order in which they are defined. For example, if you define actions [A,B,C,D], ISM executes action A, and then goes into a sleep period based on the cluster setting plugins.index_state_management.job_interval. Once the sleep period ends, ISM continues to execute the remaining actions. However, if ISM cannot successfully execute action A, the operation ends, and actions B, C, and D do not get executed. Optionally, you can define an action’s timeout period, which, if exceeded, forcibly fails the action. For example, if timeout is set to 1d, and ISM has not completed the action within one day, even after retries, the action fails. This table lists the parameters that you can define for an action. | Parameter | Description | Type | Required | Default | . | timeout | The timeout period for the action. Accepts time units for minutes, hours, and days. | time unit | No | - | . | retry | The retry configuration for the action. | object | No | Specific to action | . The retry operation has the following parameters: . | Parameter | Description | Type | Required | Default | . | count | The number of retry counts. | number | Yes | - | . | backoff | The backoff policy type to use when retrying. Valid values are Exponential, Constant, and Linear. | string | No | Exponential | . | delay | The time to wait between retries. Accepts time units for minutes, hours, and days. | time unit | No | 1 minute | . The following example action has a timeout period of one hour. The policy retries this action three times with an exponential backoff policy, with a delay of 10 minutes between each retry: . \"actions\": { \"timeout\": \"1h\", \"retry\": { \"count\": 3, \"backoff\": \"exponential\", \"delay\": \"10m\" } } . For a list of available unit types, see Supported units. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/policies/#actions",
    "relUrl": "/im-plugin/ism/policies/#actions"
  },"91": {
    "doc": "Policies",
    "title": "ISM supported operations",
    "content": "ISM supports the following operations: . | force_merge | read_only | read_write | replica_count | shrink | close | open | delete | rollover | notification | snapshot | index_priority | allocation | rollup | . force_merge . Reduces the number of Lucene segments by merging the segments of individual shards. This operation attempts to set the index to a read-only state before starting the merging process. | Parameter | Description | Type | Required | . | max_num_segments | The number of segments to reduce the shard to. | number | Yes | . { \"force_merge\": { \"max_num_segments\": 1 } } . read_only . Sets a managed index to be read only. { \"read_only\": {} } . read_write . Sets a managed index to be writeable. { \"read_write\": {} } . replica_count . Sets the number of replicas to assign to an index. | Parameter | Description | Type | Required | . | number_of_replicas | Defines the number of replicas to assign to an index. | number | Yes | . { \"replica_count\": { \"number_of_replicas\": 2 } } . For information about setting replicas, see Primary and replica shards. shrink . Allows you to reduce the number of primary shards in your indexes. With this action, you can specify: . | The number of primary shards that the target index should contain. | A max shard size for the primary shards in the target index. | Specify a percentage to shrink the number of primary shards in the target index. | . \"shrink\": { \"num_new_shards\": 1, \"target_index_name_template\": { \"source\": \"_shrunken\" }, \"aliases\": [ { \"my-alias\": {} } ], \"force_unsafe\": false } . | Parameter | Description | Type | Example | Required | . | num_new_shards | The maximum number of primary shards in the shrunken index. | integer | 5 | Yes, however it cannot be used with max_shard_size or percentage_of_source_shards | . | max_shard_size | The maximum size in bytes of a shard for the target index. | keyword | 5gb | Yes, however it cannot be used with num_new_shards or percentage_of_source_shards | . | percentage_of_source_shards | Percentage of the number of original primary shards to shrink. This parameter indicates the minimum percentage to use when shrinking the number of primary shards. Must be between 0.0 and 1.0, exclusive. | Percentage | 0.5 | Yes, however it cannot be used with max_shard_size or num_new_shards | . | target_index_name_template | The name of the shrunken index. Accepts strings and the Mustache variables and. | string or Mustache template | {\"source\": \"_shrunken\"} | No | . | aliases | Aliases to add to the new index. | object | myalias | No, but must be an array of alias objects | . | force_unsafe | If true, executes the shrink action even if there are no replicas. | boolean | false | No | . If you want to add aliases to the action, the parameter must include an array of alias objects. For example, . \"aliases\": [ { \"my-alias\": {} }, { \"my-second-alias\": { \"is_write_index\": false, \"filter\": { \"multi_match\": { \"query\": \"QUEEN\", \"fields\": [\"speaker\", \"text_entry\"] } }, \"index_routing\" : \"1\", \"search_routing\" : \"1\" } }, ] . close . Closes the managed index. { \"close\": {} } . Closed indexes remain on disk, but consume no CPU or memory. You can’t read from, write to, or search closed indexes. Closing an index is a good option if you need to retain data for longer than you need to actively search it and have sufficient disk space on your data nodes. If you need to search the data again, reopening a closed index is simpler than restoring an index from a snapshot. open . Opens a managed index. { \"open\": {} } . delete . Deletes a managed index. { \"delete\": {} } . rollover . Rolls an alias over to a new index when the managed index meets one of the rollover conditions. Important: ISM checks the conditions for operations on every execution of the policy based on the set interval, not continuously. The rollover will be performed if the value has reached or exceeded the configured limit when the check is performed. For example with min_size configured to a value of 100GiB, ISM might check the index at 99 GiB and not perform the rollover. However, if the index has grown past the limit (e.g. 105GiB) by the next check, the operation is performed. The index format must match the pattern: ^.*-\\d+$. For example, (logs-000001). Set index.plugins.index_state_management.rollover_alias as the alias to rollover. | Parameter | Description | Type | Example | Required | . | min_size | The minimum size of the total primary shard storage (not counting replicas) required to roll over the index. For example, if you set min_size to 100 GiB and your index has 5 primary shards and 5 replica shards of 20 GiB each, the total size of all primary shards is 100 GiB, so the rollover occurs. See Important note above. | string | 20gb or 5mb | No | . | min_primary_shard_size | The minimum storage size of a single primary shard required to roll over the index. For example, if you set min_primary_shard_size to 30 GiB and one of the primary shards in the index has a size greater than the condition, the rollover occurs. See Important note above. | string | 20gb or 5mb | No | . | min_doc_count | The minimum number of documents required to roll over the index. See Important note above. | number | 2000000 | No | . | min_index_age | The minimum age required to roll over the index. Index age is the time between its creation and the present. Supported units are d (days), h (hours), m (minutes), s (seconds), ms (milliseconds), and micros (microseconds). See Important note above. | string | 5d or 7h | No | . { \"rollover\": { \"min_size\": \"50gb\" } } . { \"rollover\": { \"min_primary_shard_size\": \"30gb\" } } . { \"rollover\": { \"min_doc_count\": 100000000 } } . { \"rollover\": { \"min_index_age\": \"30d\" } } . notification . Sends you a notification. | Parameter | Description | Type | Required | . | destination | The destination URL. | Slack, Amazon Chime, or webhook URL | Yes | . | message_template | The text of the message. You can add variables to your messages using Mustache templates. | object | Yes | . The destination system must return a response otherwise the notification operation throws an error. Example 1: Chime notification . { \"notification\": { \"destination\": { \"chime\": { \"url\": \"&lt;url&gt;\" } }, \"message_template\": { \"source\": \"the index is {{ctx.index}}\" } } } . Example 2: Custom webhook notification . { \"notification\": { \"destination\": { \"custom_webhook\": { \"url\": \"https://&lt;your_webhook&gt;\" } }, \"message_template\": { \"source\": \"the index is {{ctx.index}}\" } } } . Example 3: Slack notification . { \"notification\": { \"destination\": { \"slack\": { \"url\": \"https://hooks.slack.com/services/xxx/xxxxxx\" } }, \"message_template\": { \"source\": \"the index is {{ctx.index}}\" } } } . You can use ctx variables in your message to represent a number of policy parameters based on the past executions of your policy. For example, if your policy has a rollover action, you can use {{ctx.action.name}} in your message to represent the name of the rollover. The following ctx variable options are available for every policy: . Guaranteed variables . | Parameter | Description | Type | . | index | The name of the index. | string | . | index_uuid | The uuid of the index. | string | . | policy_id | The name of the policy. | string | . snapshot . Back up your cluster’s indexes and state. For more information about snapshots, see Take and restore snapshots. The snapshot operation has the following parameters: . | Parameter | Description | Type | Required | Default | . | repository | The repository name that you register through the native snapshot API operations. | string | Yes | - | . | snapshot | The name of the snapshot. Accepts strings and the Mustache variables and. If the Mustache variables are invalid, then the snapshot name defaults to the index’s name. | string or Mustache template | Yes | - | . { \"snapshot\": { \"repository\": \"my_backup\", \"snapshot\": \"\" } } . index_priority . Set the priority for the index in a specific state. Unallocated shards of indexes are recovered in the order of their priority, whenever possible. The indexes with higher priority values are recovered first followed by the indexes with lower priority values. The index_priority operation has the following parameter: . | Parameter | Description | Type | Required | Default | . | priority | The priority for the index as soon as it enters a state. | number | Yes | 1 | . \"actions\": [ { \"index_priority\": { \"priority\": 50 } } ] . allocation . Allocate the index to a node with a specific attribute set like this. For example, setting require to warm moves your data only to “warm” nodes. The allocation operation has the following parameters: . | Parameter | Description | Type | Required | . | require | Allocate the index to a node with a specified attribute. | string | Yes | . | include | Allocate the index to a node with any of the specified attributes. | string | Yes | . | exclude | Don’t allocate the index to a node with any of the specified attributes. | string | Yes | . | wait_for | Wait for the policy to execute before allocating the index to a node with a specified attribute. | string | Yes | . \"actions\": [ { \"allocation\": { \"require\": { \"temp\": \"warm\" } } } ] . rollup . Index rollup lets you periodically reduce data granularity by rolling up old data into summarized indexes. Rollup jobs can be continuous or non-continuous. A rollup job created using an ISM policy can only be non-continuous. Path and HTTP methods . PUT _plugins/_rollup/jobs/&lt;rollup_id&gt; GET _plugins/_rollup/jobs/&lt;rollup_id&gt; DELETE _plugins/_rollup/jobs/&lt;rollup_id&gt; POST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_start POST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_stop GET _plugins/_rollup/jobs/&lt;rollup_id&gt;/_explain . Sample ISM rollup policy . { \"policy\": { \"description\": \"Sample rollup\" , \"default_state\": \"rollup\", \"states\": [ { \"name\": \"rollup\", \"actions\": [ { \"rollup\": { \"ism_rollup\": { \"description\": \"Creating rollup through ISM\", \"target_index\": \"target\", \"page_size\": 1000, \"dimensions\": [ { \"date_histogram\": { \"fixed_interval\": \"60m\", \"source_field\": \"order_date\", \"target_field\": \"order_date\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"customer_gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day_of_week\" } } ], \"metrics\": [ { \"source_field\": \"taxless_total_price\", \"metrics\": [ { \"sum\": {} } ] }, { \"source_field\": \"total_quantity\", \"metrics\": [ { \"avg\": {} }, { \"max\": {} } ] } ] } } } ], \"transitions\": [] } ] } } . Request fields . Request fields are required when creating an ISM policy. You can reference the Index rollups API page for request field options. Adding a rollup policy in Dashboards . To add a rollup policy in Dashboards, follow the steps below. | Select the menu button on the top-left of the Dashboards user interface. | In the Dashboards menu, select Index Management. | On the next screen select Rollup jobs. | Select the Create rollup button. | Follow the steps in the Create rollup job wizard. | Add a name for the policy in the Name box. | You can reference the Index rollups API page to configure the rollup policy. | Finally, select the Create button on the bottom-right of the Dashboards user interface. | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/policies/#ism-supported-operations",
    "relUrl": "/im-plugin/ism/policies/#ism-supported-operations"
  },"92": {
    "doc": "Policies",
    "title": "Transitions",
    "content": "Transitions define the conditions that need to be met for a state to change. After all actions in the current state are completed, the policy starts checking the conditions for transitions. ISM evaluates transitions in the order in which they are defined. For example, if you define transitions: [A,B,C,D], ISM iterates through this list of transitions until it finds a transition that evaluates to true, it then stops and sets the next state to the one defined in that transition. On its next execution, ISM dismisses the rest of the transitions and starts in that new state. If you don’t specify any conditions in a transition and leave it empty, then it’s assumed to be the equivalent of always true. This means that the policy transitions the index to this state the moment it checks. This table lists the parameters you can define for transitions. | Parameter | Description | Type | Required | . | state_name | The name of the state to transition to if the conditions are met. | string | Yes | . | conditions | List the conditions for the transition. | list | Yes | . The conditions object has the following parameters: . | Parameter | Description | Type | Required | . | min_index_age | The minimum age of the index required to transition. | string | No | . | min_rollover_age | The minimum age required after a rollover has occurred to transition to the next state. | string | No | . | min_doc_count | The minimum document count of the index required to transition. | number | No | . | min_size | The minimum size of the total primary shard storage (not counting replicas) required to transition. For example, if you set min_size to 100 GiB and your index has 5 primary shards and 5 replica shards of 20 GiB each, the total size of all primary shards is 100 GiB, so your index is transitioned to the next state. | string | No | . | cron | The cron job that triggers the transition if no other transition happens first. | object | No | . | cron.cron.expression | The cron expression that triggers the transition. | string | Yes | . | cron.cron.timezone | The timezone that triggers the transition. | string | Yes | . The following example transitions the index to a cold state after a period of 30 days: . \"transitions\": [ { \"state_name\": \"cold\", \"conditions\": { \"min_index_age\": \"30d\" } } ] . ISM checks the conditions on every execution of the policy based on the set interval. This example uses the cron condition to transition indexes every Saturday at 5:00 PT: . \"transitions\": [ { \"state_name\": \"cold\", \"conditions\": { \"cron\": { \"cron\": { \"expression\": \"* 17 * * SAT\", \"timezone\": \"America/Los_Angeles\" } } } } ] . Note that this condition does not execute at exactly 5:00 PM; the job still executes based off the job_interval setting. Due to this variance in start time and the amount of time that it can take for actions to complete prior to checking transition conditions, we recommend against overly narrow cron expressions. For example, don’t use 15 17 * * SAT (5:15 PM on Saturday). A window of an hour, which this example uses, is generally sufficient, but you might increase it to 2–3 hours to avoid missing the window and having to wait a week for the transition to occur. Alternately, you could use a broader expression such as * * * * SAT,SUN to have the transition occur at any time during the weekend. For information on writing cron expressions, see Cron expression reference. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/policies/#transitions",
    "relUrl": "/im-plugin/ism/policies/#transitions"
  },"93": {
    "doc": "Policies",
    "title": "Error notifications",
    "content": "The error_notification operation sends you a notification if your managed index fails. It notifies a single destination or notification channel with a custom message. Set up error notifications at the policy level: . { \"policy\": { \"description\": \"hot warm delete workflow\", \"default_state\": \"hot\", \"schema_version\": 1, \"error_notification\": { }, \"states\": [ ] } } . | Parameter | Description | Type | Required | . | destination | The destination URL. | Slack, Amazon Chime, or webhook URL | Yes if channel isn’t specified | . | channel | A notification channel’s ID | string | Yes if destination isn’t specified | . | message_template | The text of the message. You can add variables to your messages using Mustache templates. | object | Yes | . The destination system must return a response otherwise the error_notification operation throws an error. Example 1: Chime notification . { \"error_notification\": { \"destination\": { \"chime\": { \"url\": \"&lt;url&gt;\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } . Example 2: Custom webhook notification . { \"error_notification\": { \"destination\": { \"custom_webhook\": { \"url\": \"https://&lt;your_webhook&gt;\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } . Example 3: Slack notification . { \"error_notification\": { \"destination\": { \"slack\": { \"url\": \"https://hooks.slack.com/services/xxx/xxxxxx\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } . Example 4: Using a notification channel . { \"error_notification\": { \"channel\": { \"id\": \"some-channel-config-id\" }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } . You can use the same options for ctx variables as the notification operation. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/policies/#error-notifications",
    "relUrl": "/im-plugin/ism/policies/#error-notifications"
  },"94": {
    "doc": "Policies",
    "title": "Sample policy with ISM template for auto rollover",
    "content": "The following sample template policy is for a rollover use case. If you want to skip rollovers for an index, set index.plugins.index_state_management.rollover_skip to true in the settings of that index. | Create a policy with an ism_template field: . PUT _plugins/_ism/policies/rollover_policy { \"policy\": { \"description\": \"Example rollover policy.\", \"default_state\": \"rollover\", \"states\": [ { \"name\": \"rollover\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 1 } } ], \"transitions\": [] } ], \"ism_template\": { \"index_patterns\": [\"log*\"], \"priority\": 100 } } } . You need to specify the index_patterns field. If you don’t specify a value for priority, it defaults to 0. | Set up a template with the rollover_alias as log : . PUT _index_template/ism_rollover { \"index_patterns\": [\"log*\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"log\" } } } . | Create an index with the log alias: . PUT log-000001 { \"aliases\": { \"log\": { \"is_write_index\": true } } } . | Index a document to trigger the rollover condition: . POST log/_doc { \"message\": \"dummy\" } . | Verify if the policy is attached to the log-000001 index: . GET _plugins/_ism/explain/log-000001?pretty . | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/policies/#sample-policy-with-ism-template-for-auto-rollover",
    "relUrl": "/im-plugin/ism/policies/#sample-policy-with-ism-template-for-auto-rollover"
  },"95": {
    "doc": "Policies",
    "title": "Example policy with ISM templates for the alias action",
    "content": "The following example policy is for an alias action use case. In the following example, the first job will trigger the rollover action, and a new index will be created. Next, another document is added to the two indexes. The new job will then cause the second index to point to the log alias, and the older index will be removed due to the alias action. First, create an ISM policy: . PUT /_plugins/_ism/policies/rollover_policy?pretty { \"policy\": { \"description\": \"Example rollover policy.\", \"default_state\": \"rollover\", \"states\": [ { \"name\": \"rollover\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 1 } } ], \"transitions\": [{ \"state_name\": \"alias\", \"conditions\": { \"min_doc_count\": \"2\" } }] }, { \"name\": \"alias\", \"actions\": [ { \"alias\": { \"actions\": [ { \"remove\": { \"alias\": \"log\" } } ] } } ] } ], \"ism_template\": { \"index_patterns\": [\"log*\"], \"priority\": 100 } } } . Next, create an index template on which to enable the policy: . PUT /_index_template/ism_rollover? { \"index_patterns\": [\"log*\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"log\" } } } . copy . Next, change the cluster settings to trigger jobs every minute: . PUT /_cluster/settings?pretty=true { \"persistent\" : { \"plugins.index_state_management.job_interval\" : 1 } } . copy . Next, create a new index: . PUT /log-000001 { \"aliases\": { \"log\": { \"is_write_index\": true } } } . copy . Finally, add a document to the index to trigger the job: . POST /log-000001/_doc { \"message\": \"dummy\" } . copy . You can verify these steps using the Alias and Index API: . GET /_cat/indices?pretty . copy . GET /_cat/aliases?pretty . copy . Note: The index and remove_index parameters are not allowed with alias action policies. Only the add and remove alias action parameters are allowed. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/policies/#example-policy-with-ism-templates-for-the-alias-action",
    "relUrl": "/im-plugin/ism/policies/#example-policy-with-ism-templates-for-the-alias-action"
  },"96": {
    "doc": "Policies",
    "title": "Example policy",
    "content": "The following example policy implements a hot, warm, and delete workflow. You can use this policy as a template to prioritize resources to your indexes based on their levels of activity. In this case, an index is initially in a hot state. After a day, it changes to a warm state, where the number of replicas increases to 5 to improve the read performance. After 30 days, the policy moves this index into a delete state. The service sends a notification to a Chime room that the index is being deleted, and then permanently deletes it. { \"policy\": { \"description\": \"hot warm delete workflow\", \"default_state\": \"hot\", \"schema_version\": 1, \"states\": [ { \"name\": \"hot\", \"actions\": [ { \"rollover\": { \"min_index_age\": \"1d\", \"min_primary_shard_size\": \"30gb\" } } ], \"transitions\": [ { \"state_name\": \"warm\" } ] }, { \"name\": \"warm\", \"actions\": [ { \"replica_count\": { \"number_of_replicas\": 5 } } ], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"30d\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"notification\": { \"destination\": { \"chime\": { \"url\": \"&lt;URL&gt;\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} is being deleted\" } } }, { \"delete\": {} } ] } ], \"ism_template\": { \"index_patterns\": [\"log*\"], \"priority\": 100 } } } . This diagram shows the states, transitions, and actions of the above policy as a finite-state machine. For more information about finite-state machines, see Wikipedia. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/policies/#example-policy",
    "relUrl": "/im-plugin/ism/policies/#example-policy"
  },"97": {
    "doc": "Settings",
    "title": "ISM settings",
    "content": "We don’t recommend changing these settings; the defaults should work well for most use cases. Index State Management (ISM) stores its configuration in the .opendistro-ism-config index. Don’t modify this index without using the ISM API operations. All settings are available using the OpenSearch _cluster/settings operation. None require a restart, and all can be marked persistent or transient. | Setting | Default | Description | . | plugins.index_state_management.enabled | True | Specifies whether ISM is enabled or not. | . | plugins.index_state_management.job_interval | 5 minutes | The interval at which the managed index jobs are run. | . | plugins.index_state_management.jitter | 0.6 | A randomized delay that is added to a job’s base run time to prevent a surge of activity from all indices at the same time. A value of 0.6 means a delay of 0-60% of a job interval is added to the base interval. For example, if you have a base interval time of 30 minutes, a value of 0.6 means an amount anywhere between 0 to 18 minutes gets added to your job interval. Maximum is 1, which means an additional interval time of 100%. This maximum cannot exceed plugins.jobscheduler.jitter_limit, which also has a default of 0.6. For example, if plugins.index_state_management.jitter is set to 0.8, ISM uses plugins.jobscheduler.jitter_limit of 0.6 instead. | . | plugins.index_state_management.coordinator.sweep_period | 10 minutes | How often the routine background sweep is run. | . | plugins.index_state_management.coordinator.backoff_millis | 50 milliseconds | The backoff time between retries for failures in the ManagedIndexCoordinator (such as when we update managed indices). | . | plugins.index_state_management.coordinator.backoff_count | 2 | The count of retries for failures in the ManagedIndexCoordinator. | . | plugins.index_state_management.history.enabled | True | Specifies whether audit history is enabled or not. The logs from ISM are automatically indexed to a logs document. | . | plugins.index_state_management.history.max_docs | 2,500,000 | The maximum number of documents before rolling over the audit history index. | . | plugins.index_state_management.history.max_age | 24 hours | The maximum age before rolling over the audit history index. | . | plugins.index_state_management.history.rollover_check_period | 8 hours | The time between rollover checks for the audit history index. | . | plugins.index_state_management.history.rollover_retention_period | 30 days | How long audit history indices are kept. | . | plugins.index_state_management.allow_list | All actions | List of actions that you can use. | . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/settings/#ism-settings",
    "relUrl": "/im-plugin/ism/settings/#ism-settings"
  },"98": {
    "doc": "Settings",
    "title": "Settings",
    "content": " ",
    "url": "http://localhost:4000/docs/latest/im-plugin/ism/settings/",
    "relUrl": "/im-plugin/ism/settings/"
  },"99": {
    "doc": "Refresh search analyzer",
    "title": "Refresh search analyzer",
    "content": "With ISM installed, you can refresh search analyzers in real time with the following API: . POST /_plugins/_refresh_search_analyzers/&lt;index or alias or wildcard&gt; . For example, if you change the synonym list in your analyzer, the change takes effect without you needing to close and reopen the index. To work, the token filter must have an updateable flag of true: . { \"analyzer\": { \"my_synonyms\": { \"tokenizer\": \"whitespace\", \"filter\": [ \"synonym\" ] } }, \"filter\": { \"synonym\": { \"type\": \"synonym_graph\", \"synonyms_path\": \"synonyms.txt\", \"updateable\": true } } } . ",
    "url": "http://localhost:4000/docs/latest/im-plugin/refresh-analyzer/index/",
    "relUrl": "/im-plugin/refresh-analyzer/index/"
  },"100": {
    "doc": "Index management security",
    "title": "Index management security",
    "content": "Using the security plugin with index management lets you limit non-admin users to certain actions. For example, you might want to set up your security such that a group of users can only read ISM policies, while others can create, delete, or change policies. All index management data are protected as system indices, and only a super admin or an admin with a Transport Layer Security (TLS) certificate can access system indices. For more information, see System indices. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/security/",
    "relUrl": "/im-plugin/security/"
  },"101": {
    "doc": "Index management security",
    "title": "Basic permissions",
    "content": "The security plugin comes with one role that offers full access to index management: index_management_full_access. For a description of the role’s permissions, see Predefined roles. With security enabled, users not only need the correct index management permissions, but they also need permissions to execute actions to involved indices. For example, if a user wants to use the REST API to attach a policy that executes a rollup job to an index named system-logs, they would need the permissions to attach a policy and execute a rollup job, as well as access to system-logs. Finally, with the exceptions of Create Policy, Get Policy, and Delete Policy, users also need the indices:admin/opensearch/ism/managedindex permission to execute ISM APIs. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/security/#basic-permissions",
    "relUrl": "/im-plugin/security/#basic-permissions"
  },"102": {
    "doc": "Index management security",
    "title": "(Advanced) Limit access by backend role",
    "content": "You can use backend roles to configure fine-grained access to index management policies and actions. For example, users of different departments in an organization might view different policies depending on what roles and permissions they are assigned. First, ensure your users have the appropriate backend roles. Backend roles usually come from an LDAP server or SAML provider. However, if you use the internal user database, you can use the REST API to add them manually. Use the REST API to enable the following setting: . PUT _cluster/settings { \"transient\": { \"plugins.index_management.filter_by_backend_roles\": \"true\" } } . With security enabled, only users who share at least one backend role can see and execute the policies and actions relevant to their roles. For example, consider a scenario with three users: John and Jill, who have the backend role helpdesk_staff, and Jane, who has the backend role phone_operator. John wants to create a policy that performs a rollup job on an index named airline_data, so John would need a backend role that has permissions to access that index, create relevant policies, and execute relevant actions, and Jill would be able to access the same index, policy, and job. However, Jane cannot access or edit those resources or actions. ",
    "url": "http://localhost:4000/docs/latest/im-plugin/security/#advanced-limit-access-by-backend-role",
    "relUrl": "/im-plugin/security/#advanced-limit-access-by-backend-role"
  },"103": {
    "doc": "Advanced Analytics",
    "title": "Advanced Analytics",
    "content": "Advanced Analytics . ",
    "url": "http://localhost:4000/docs/latest/documentation/adv-analytics/index/",
    "relUrl": "/documentation/adv-analytics/index/"
  },"104": {
    "doc": "Advanced Analytics",
    "title": "Introduction",
    "content": " ",
    "url": "http://localhost:4000/docs/latest/documentation/adv-analytics/index/#introduction",
    "relUrl": "/documentation/adv-analytics/index/#introduction"
  },"105": {
    "doc": "Alerting",
    "title": "Alerting",
    "content": " ",
    "url": "http://localhost:4000/docs/latest/documentation/alerting/index/",
    "relUrl": "/documentation/alerting/index/"
  },"106": {
    "doc": "Analytics",
    "title": "Analytics",
    "content": " ",
    "url": "http://localhost:4000/docs/latest/documentation/analytics/index/",
    "relUrl": "/documentation/analytics/index/"
  },"107": {
    "doc": "Dashboards",
    "title": "Dashboards",
    "content": " ",
    "url": "http://localhost:4000/docs/latest/documentation/dashboards/index/",
    "relUrl": "/documentation/dashboards/index/"
  },"108": {
    "doc": "Data Collection",
    "title": "Data Collection",
    "content": " ",
    "url": "http://localhost:4000/docs/latest/documentation/data-collection/index/",
    "relUrl": "/documentation/data-collection/index/"
  },"109": {
    "doc": "Data Defination",
    "title": "Data Defination",
    "content": " ",
    "url": "http://localhost:4000/docs/latest/documentation/data-defination/index/",
    "relUrl": "/documentation/data-defination/index/"
  },"110": {
    "doc": "About Documentation",
    "title": "About Documentation",
    "content": "Elysium Documentation . About DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout Documentation . ",
    "url": "http://localhost:4000/docs/latest/documentation/index/",
    "relUrl": "/documentation/index/"
  },"111": {
    "doc": "Elysium Full Text Search",
    "title": "Elysium Full Text Search",
    "content": "search engine . ",
    "url": "http://localhost:4000/docs/latest/documentation/search/index/",
    "relUrl": "/documentation/search/index/"
  },"112": {
    "doc": "Elysium Full Text Search",
    "title": "Introduction",
    "content": "Elysium Search is a search tool designed to help users quickly find and filter data within a database. It allows users to search for specific values or patterns within a dataset, and offers a range of options for customizing the search process. ",
    "url": "http://localhost:4000/docs/latest/documentation/search/index/#introduction",
    "relUrl": "/documentation/search/index/#introduction"
  },"113": {
    "doc": "Elysium Full Text Search",
    "title": "How to Guides",
    "content": "Add Index To add an index to Elysium Search, follow these steps: . | Open the Elysium Search. | Click on the three horizontal bars to open side menu. | Click on the “Stack Management“ button. | Click on the “Index Patterns” button. | Click on the “Create Index Pattern” button. | Search for the table name | Click on the “Next Step“ button and wait until the load completes (at the right top corner). | Once loaded use “Time field“ dropdown to select the timestamp. | Selecting timestamp is mandatory to preform search. Do not create index without selecting the timestamp. | Click on the “Create Index pattern” button. | . Set Result Limit for a search in Elysium Search, follow these steps: . | Open the Elysium Search. | Click on the three horizontal bars to open side menu. | Click on the “Stack Management“ button. | Click on the “Advanced Settings” button. | Find “Number of rows” and change to desired number. | Click on the “Save changes” to save the setting. | . Elysium Search allows users to specify the time zone for their searches. To set the time zone, follow these steps: . | Open the Elysium SaaS. | Click on the “user icon” at the top right corner and then click “My account“. | Click “Timezone” for the dropdown and select to the desired time zone. | Once selected then Click on the button “Save“ which requires password. | Enter the password and click on the button “YES“. | . How to search: . | Open the Elysium Search. | Click on the three horizontal bars to open side menu. | Click on the “Discover“ button. | Click on the “index“ dropdown to select the index on which search needs to performed. | Select the time range from right top corner. | Use search bar for entering different types of searches. | Click on the “Refresh“/”Update” button for results. | . ",
    "url": "http://localhost:4000/docs/latest/documentation/search/index/#how-to-guides",
    "relUrl": "/documentation/search/index/#how-to-guides"
  },"114": {
    "doc": "Elysium Full Text Search",
    "title": "Different types of searches you can use to filter data.",
    "content": "Unquoted Search: . Use unquoted search to filter data that partially matches with the value. For example, to filter data where the username field values are like “john white”, “john whitehead”,”sara white”, etc., use the following syntax: . username: white . The field parameter is optional. If not provided, all fields will be searched for the given value. For example, to search all fields for “white”, use the following: . white . Quoted Search: . Use quoted search to filter data that exactly matches with the value. For example, to filter data where the username field value is “john white”, use the following syntax: . username: \"john white\" . The field parameter is optional. If not provided, Smart Search will be enabled and it will identify fields that might be a possible match and search those fields for the given value. For more information on Smart Search, refer Smart Search. To use Exact Match, use the following: . \"john white\" . Wildcard Search: . Even though unquoted search is used for partial matches, there are cases where wildcard search can be useful. To filter data that starts with “tom”, use the following syntax: . username: tom* . To filter data that ends with “sam”, use the following syntax: . username: *sam . Wildcard search can only be used with unquoted searches. If the wildcard () is used in a quoted search, it will be considered as a literal. For example, “sam” will search for “sam*”. Ulike unquoted search which comes with default wildcards at the start and end of the input value. For example, sam will be queried as sam. Meaning any characters can be before and after “sam“. So, when using wildcards explicitly, it removes the default wildcards and treats the input (*) as a wildcard. Wildcards can also be used to find fields where a value exists. Use the following syntax to get all values without nulls: . username: * . Note: Wildcard Searches doesn’t query on flatten columns like raw.src.ip while column is not mentioned. Matching Multiple Fields: . Wildcards can also be used to query multiple fields. For example, to search for documents where any sub-field of “http.response” equals “error”, use the following: . http.response.*: \"error\" . Negating a Query: . To negate or exclude a set of data, use the “not” keyword. For example, to filter documents where the “http.request.method” is not “GET”, use the following query: . NOT http.request.method: “GET” . Negating also contains the null value while a specific search will always negate null values. Combining Multiple Queries: . To combine multiple queries, use the “and” or “or” keywords. For example, to find documents where the “http.request.method” is “GET” or the “http.response.status_code” is 400, use the following query: . http.request.method: “GET” OR http.response.status_code: “400” . Similarly, to find documents where the “http.request.method” is “GET” and the “http.response.status_code” is 400, use this query: . http.request.method: “GET” AND http.response.status_code: “400” . To specify precedence when combining multiple queries, use parentheses. For example, to find documents where the “http.request.method” is “GET” and the “http.response.status_code” is 200, or the “http.request.method is POST” and “http.response.status_code” is 400, use the following: . (http.request.method: “GET” AND http.response.status_code: “200”) OR (http.request.method: “POST” AND http.response.status_code: “400”) . You can also use parentheses for shorthand syntax when querying multiple values for the same field. For example, to find documents where the “http.request.method” is “GET”, “POST”, or “DELETE”, use the following: . http.request.method: (“GET” OR “POST” OR “DELETE”) . http.request.method:”GET” OR “POST“ This would return results that either contain “GET “in the specified field http.request.method or contain the term “POST” anywhere in the document. Precedence: When using multiple logical operators in a search, it is important to consider the precedence of the operators. The AND operator has the highest precedence, followed by OR. So the search term “A AND B OR C” will be evaluated as “(A AND B) OR C”, while the search term “A OR B AND C” will be evaluated as “A OR (B AND C)”. Escaping Special Characters: . There may be times when you need to search for a value that includes special characters. In these cases, you will need to escape these characters. To search for documents where  http.request.referrer  is  https://example.com, use either of the following queries: . http.request.referrer: \"https://example.com\" . http.request.referrer: https\\://example.com . You must escape following characters (unless surrounded by quotes/quoted search): ( ) : &lt; &gt; “ * { } . Smart Search: . Smart Search is a feature that enables searching without specifying the field. When using Smart Search, the system will identify fields that might be a possible match and search those fields for the given value. For example, if you search for “john white” without specifying a field, the system will search all fields that contain “john white” . To use Smart Search, enclose the search value in quotes. For example: . \"john white\". Numeric Range Search: . Elysium Search supports numeric searches, but there are a few considerations to keep in mind. Rounding: Elysium Search rounds numeric values to the nearest value that can be represented in double-precision floating-point format. This can affect the accuracy of numeric searches. Value will be rounded after 9 digits after the decimal point and values with a leading zero.. In cases where more precision is needed , users should use Double Quotes to treat the number as a string. 110.0 is treated as 110 and 10.00000 is treated as 10. Score : 123 . Score : “1.123456789123“ . Score : “10.0“ . Any number more than 16 digits (for both +ve and –ve numbers). We should use double quotes in that case. Adding a + in front treats the search value as a string. Hex number such as 0xabcd is not supported as a number. It must be searched as string. For example, to search for the value “01”, you would enter “01” in the search bar. You can also use the “&gt;” and “&lt;” operators to specify an open-ended range. For example, to search for documents where the “age” field is greater than 18, use the following: age &gt; 18 . To search for documents where the “age” field is less than 25, use the following: age &lt; 25 . Case Sensitivity: . Elysium Search is case-Sensitive by default, meaning that it does distinguish between uppercase and lowercase characters in searches. However, you can enable case-Insensitivity by using the following steps. | Open the Elysium Search. | Click on the three horizontal bars to open side menu. | Click on the “Discover“ button to open search. | On the search page find “settings icon” . | Click on the “settings icon” button. | Click on the “Case Sensitivity“ toggle to turn it on or off. | . Highlights: . Unquoted search will highlight the exact value entered while matching doing a partial match. User: Sannith → User: Sannith Reddy . Quoted search will highlight the exact value entered. User: “Sannith Reddy”: → User: Sannith Reddy . Wildcard matches will highlight depending on the *. User : *Reddy → User: Sannith Reddy . Limitations . Elysium Search has a number of limitations that users should be aware of: . | Arithmetic expressions and variables are not supported. | Only a limited set of character sets is supported. | Searches on timestamps are not available currently. | . Workarounds . Arithmetic Expressions: You will need to use an alternative approach if you need to perform calculations in your searches. One option is to you use a script to pre-process your data and add calculated fields,, which you can then search using Elysium Search or User can also use Dashboards(Looker). Copy/Paste: users have to be careful while copy pasting the results to search as there might be chances to characters which needs to be escaped or numbers with accurate precision. ",
    "url": "http://localhost:4000/docs/latest/documentation/search/index/#different-types-of-searches-you-can-use-to-filter-data",
    "relUrl": "/documentation/search/index/#different-types-of-searches-you-can-use-to-filter-data"
  }
}
