[
  {
    "url": "/docs/latest/im-plugin/index-rollups/index/",
    "title": "Index rollups",
    "content": "Time series data increases storage costs, strains cluster health, and slows down aggregations over time. Index rollup lets you periodically reduce data granularity by rolling up old data into summarized indices.\nYou pick the fields that interest you and use index rollup to create a new index with only those fields aggregated into coarser time buckets. You can store months or years of historical data at a fraction of the cost with the same query performance.\nFor example, say you collect CPU consumption data every five seconds and store it on a hot node. Instead of moving older data to a read-only warm node, you can roll up or compress this data with only the average CPU consumption per day or with a 10% decrease in its interval every week.\nYou can use index rollup in three ways:\nUse the index rollup API for an on-demand index rollup job that operates on an index that’s not being actively ingested such as a rolled-over index. For example, you can perform an index rollup operation to reduce data collected at a five minute interval to a weekly average for trend analysis.\nUse the OpenSearch Dashboards UI to create an index rollup job that runs on a defined schedule. You can also set it up to roll up your indices as it’s being actively ingested. For example, you can continuously roll up Logstash indices from a five second interval to a one hour interval.\nSpecify the index rollup job as an ISM action for complete index management. This allows you to roll up an index after a certain event such as a rollover, index age reaching a certain point, index becoming read-only, and so on. You can also have rollover and index rollup jobs running in sequence, where the rollover first moves the current index to a warm node and then the index rollup job creates a new index with the minimized data on the hot node.\nCreate an Index Rollup Job\nTo get started, choose Index Management in OpenSearch Dashboards.\nSelect Rollup Jobs and choose Create rollup job.\nStep 1: Set up indices\nIn the Job name and description section, specify a unique name and an optional description for the index rollup job.\nIn the Indices section, select the source and target index. The source index is the one that you want to roll up. The source index remains as is, the index rollup job creates a new index referred to as a target index. The target index is where the index rollup results are saved. For target index, you can either type in a name for a new index or you select an existing index.\nChoose Next After you create an index rollup job, you can’t change your index selections.\nStep 2: Define aggregations and metrics\nSelect the attributes with the aggregations (terms and histograms) and metrics (avg, sum, max, min, and value count) that you want to roll up. Make sure you don’t add a lot of highly granular attributes, because you won’t save much space.\nFor example, consider a dataset of cities and demographics within those cities. You can aggregate based on cities and specify demographics within a city as metrics.\nThe order in which you select attributes is critical. A city followed by a demographic is different from a demographic followed by a city.\nIn the Time aggregation section, select a timestamp field. Choose between a Fixed or Calendar interval type and specify the interval and timezone. The index rollup job uses this information to create a date histogram for the timestamp field.\n(Optional) Add additional aggregations for each field. You can choose terms aggregation for all field types and histogram aggregation only for numeric fields.\n(Optional) Add additional metrics for each field. You can choose between All, Min, Max, Sum, Avg, or Value Count.\nChoose Next.\nStep 3: Specify schedule\nSpecify a schedule to roll up your indices as it’s being ingested. The index rollup job is enabled by default.\nSpecify if the data is continuous or not.\nFor roll up execution frequency, select Define by fixed interval and specify the Rollup interval and the time unit or Define by cron expression and add in a cron expression to select the interval. To learn how to define a cron expression, see Alerting.\nSpecify the number of pages per execution process. A larger number means faster execution and more cost for memory.\n(Optional) Add a delay to the roll up executions. This is the amount of time the job waits for data ingestion to accommodate any processing time. For example, if you set this value to 10 minutes, an index rollup that executes at 2 PM to roll up 1 PM to 2 PM of data starts at 2:10 PM.\nChoose Next.\nStep 4: Review and create\nReview your configuration and select Create.\nStep 5: Search the target index\nYou can use the standard _search API to search the target index. Make sure that the query matches the constraints of the target index. For example, if you don’t set up terms aggregations on a field, you don’t receive results for terms aggregations. If you don’t set up the maximum aggregations, you don’t receive results for maximum aggregations.\nYou can’t access the internal structure of the data in the target index because the plugin automatically rewrites the query in the background to suit the target index. This is to make sure you can use the same query for the source and target index.\nTo query the target index, set size to 0: GET target_index/_search { \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"avg_cpu\": { \"avg\": { \"field\": \"cpu_usage\" } } } } Consider a scenario where you collect rolled up data from 1 PM to 9 PM in hourly intervals and live data from 7 PM to 11 PM in minutely intervals. If you execute an aggregation over these in the same query, for 7 PM to 9 PM, you see an overlap of both rolled up data and live data because they get counted twice in the aggregations.\nSample Walkthrough\nThis walkthrough uses the OpenSearch Dashboards sample e-commerce data. To add that sample data, log in to OpenSearch Dashboards, choose Home and Try our sample data. For Sample eCommerce orders, choose Add data.\nThen run a search: GET opensearch_dashboards_sample_data_ecommerce/_search Sample response { \"took\": 23, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4675, \"relation\": \"eq\" }, \"max_score\": 1, \"hits\": [ { \"_index\": \"opensearch_dashboards_sample_data_ecommerce\", \"_type\": \"_doc\", \"_id\": \"jlMlwXcBQVLeQPrkC_kQ\", \"_score\": 1, \"_source\": { \"category\": [ \"Women's Clothing\", \"Women's Accessories\"], \"currency\": \"EUR\", \"customer_first_name\": \"Selena\", \"customer_full_name\": \"Selena Mullins\", \"customer_gender\": \"FEMALE\", \"customer_id\": 42, \"customer_last_name\": \"Mullins\", \"customer_phone\": \"\", \"day_of_week\": \"Saturday\", \"day_of_week_i\": 5, \"email\": \"selena@mullins-family.zzz\", \"manufacturer\": [ \"Tigress Enterprises\"], \"order_date\": \"2021-02-27T03:56:10+00:00\", \"order_id\": 581553, \"products\": [ { \"base_price\": 24.99, \"discount_percentage\": 0, \"quantity\": 1, \"manufacturer\": \"Tigress Enterprises\", \"tax_amount\": 0, \"product_id\": 19240, \"category\": \"Women's Clothing\", \"sku\": \"ZO0064500645\", \"taxless_price\": 24.99, \"unit_discount_amount\": 0, \"min_price\": 12.99, \"_id\": \"sold_product_581553_19240\", \"discount_amount\": 0, \"created_on\": \"2016-12-24T03:56:10+00:00\", \"product_name\": \"Blouse - port royal\", \"price\": 24.99, \"taxful_price\": 24.99, \"base_unit_price\": 24.99 }, { \"base_price\": 10.99, \"discount_percentage\": 0, \"quantity\": 1, \"manufacturer\": \"Tigress Enterprises\", \"tax_amount\": 0, \"product_id\": 17221, \"category\": \"Women's Accessories\", \"sku\": \"ZO0085200852\", \"taxless_price\": 10.99, \"unit_discount_amount\": 0, \"min_price\": 5.06, \"_id\": \"sold_product_581553_17221\", \"discount_amount\": 0, \"created_on\": \"2016-12-24T03:56:10+00:00\", \"product_name\": \"Snood - rose\", \"price\": 10.99, \"taxful_price\": 10.99, \"base_unit_price\": 10.99 }], \"sku\": [ \"ZO0064500645\", \"ZO0085200852\"], \"taxful_total_price\": 35.98, \"taxless_total_price\": 35.98, \"total_quantity\": 2, \"total_unique_products\": 2, \"type\": \"order\", \"user\": \"selena\", \"geoip\": { \"country_iso_code\": \"MA\", \"location\": { \"lon\": -8, \"lat\": 31.6 }, \"region_name\": \"Marrakech-Tensift-Al Haouz\", \"continent_name\": \"Africa\", \"city_name\": \"Marrakesh\" }, \"event\": { \"dataset\": \"sample_ecommerce\" } } }] } }... Create an index rollup job.\nThis example picks the order_date, customer_gender, geoip.city_name, geoip.region_name, and day_of_week fields and rolls them into an example_rollup target index: PUT _plugins/_rollup/jobs/example { \"rollup\": { \"enabled\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"last_updated_time\": 1602100553, \"description\": \"An example policy that rolls up the sample ecommerce data\", \"source_index\": \"opensearch_dashboards_sample_data_ecommerce\", \"target_index\": \"example_rollup\", \"page_size\": 1000, \"delay\": 0, \"continuous\": false, \"dimensions\": [ { \"date_histogram\": { \"source_field\": \"order_date\", \"fixed_interval\": \"60m\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"customer_gender\" } }, { \"terms\": { \"source_field\": \"geoip.city_name\" } }, { \"terms\": { \"source_field\": \"geoip.region_name\" } }, { \"terms\": { \"source_field\": \"day_of_week\" } }], \"metrics\": [ { \"source_field\": \"taxless_total_price\", \"metrics\": [ { \"avg\": {} }, { \"sum\": {} }, { \"max\": {} }, { \"min\": {} }, { \"value_count\": {} }] }, { \"source_field\": \"total_quantity\", \"metrics\": [ { \"avg\": {} }, { \"max\": {} }] }] } } You can query the example_rollup index for the terms aggregations on the fields set up in the rollup job.\nYou get back the same response that you would on the original opensearch_dashboards_sample_data_ecommerce source index. POST example_rollup/_search { \"size\": 0, \"query\": { \"bool\": { \"must\": { \"term\": { \"geoip.region_name\": \"California\" } } } }, \"aggregations\": { \"daily_numbers\": { \"terms\": { \"field\": \"day_of_week\" }, \"aggs\": { \"per_city\": { \"terms\": { \"field\": \"geoip.city_name\" }, \"aggregations\": { \"average quantity\": { \"avg\": { \"field\": \"total_quantity\" } } } }, \"total_revenue\": { \"sum\": { \"field\": \"taxless_total_price\" } } } } } } Sample Response { \"took\": 14, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 281, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"aggregations\": { \"daily_numbers\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Friday\", \"doc_count\": 59, \"total_revenue\": { \"value\": 4858.84375 }, \"per_city\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Los Angeles\", \"doc_count\": 59, \"average quantity\": { \"value\": 2.305084745762712 } }] } }, { \"key\": \"Saturday\", \"doc_count\": 46, \"total_revenue\": { \"value\": 3547.203125 }, \"per_city\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Los Angeles\", \"doc_count\": 46, \"average quantity\": { \"value\": 2.260869565217391 } }] } }, { \"key\": \"Tuesday\", \"doc_count\": 45, \"total_revenue\": { \"value\": 3983.28125 }, \"per_city\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Los Angeles\", \"doc_count\": 45, \"average quantity\": { \"value\": 2.2888888888888888 } }] } }, { \"key\": \"Sunday\", \"doc_count\": 44, \"total_revenue\": { \"value\": 3308.1640625 }, \"per_city\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Los Angeles\", \"doc_count\": 44, \"average quantity\": { \"value\": 2.090909090909091 } }] } }, { \"key\": \"Thursday\", \"doc_count\": 40, \"total_revenue\": { \"value\": 2876.125 }, \"per_city\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Los Angeles\", \"doc_count\": 40, \"average quantity\": { \"value\": 2.3 } }] } }, { \"key\": \"Monday\", \"doc_count\": 38, \"total_revenue\": { \"value\": 2673.453125 }, \"per_city\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Los Angeles\", \"doc_count\": 38, \"average quantity\": { \"value\": 2.1578947368421053 } }] } }, { \"key\": \"Wednesday\", \"doc_count\": 38, \"total_revenue\": { \"value\": 3202.453125 }, \"per_city\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Los Angeles\", \"doc_count\": 38, \"average quantity\": { \"value\": 2.236842105263158 } }] } }] } } } The doc_count field\nThe doc_count field in bucket aggregations contains the number of documents collected in each bucket. When calculating the bucket’s doc_count, the number of documents is incremented by the number of the pre-aggregated documents in each summary document. The doc_count returned from rollup searches represents the total number of matching documents from the source index. The document count for each bucket is the same whether you search the source index or the rollup target index.\nQuery string queries\nTo take advantage of shorter and easier to write strings in Query DSL, you can use query strings to simplify search queries in rollup indexes. To use query strings, add the following fields to your rollup search request. \"query\": { \"query_string\": { \"query\": \"field_name:field_value\" } } The following example uses a query string with a * wildcard operator to search inside a rollup index called my_server_logs_rollup. GET my_server_logs_rollup/_search { \"size\": 0, \"query\": { \"query_string\": { \"query\": \"email* OR inventory\", \"default_field\": \"service_name\" } }, \"aggs\": { \"service_name\": { \"terms\": { \"field\": \"service_name\" }, \"aggs\": { \"region\": { \"terms\": { \"field\": \"region\" }, \"aggs\": { \"average quantity\": { \"avg\": { \"field\": \"cpu_usage\" } } } } } } } } For more information on which parameters are supported in query strings, see Advanced filter options.\nDynamic target index\nIn ISM rollup, the target_index field may contain a template that is compiled at the time of each rollup indexing. For example, if you specify the target_index field as rollup_ndx-{{ctx.source_index}}, the source index log-000001 will roll up into a target index rollup_ndx-log-000001. This allows you to roll up data into multiple time-based indices, with one rollup job created for each source index.\nThe source_index parameter in {{ctx.source_index}} cannot contain wildcards.\nSearching multiple rollup indices\nWhen data is rolled up into multiple target indices, you can run one search across all of the rollup indices. To search multiple target indices that have the same rollup, specify the index names as a comma-separated list or a wildcard pattern. For example, with target_index as rollup_ndx-{{ctx.source_index}} and source indices that start with log, specify the rollup_ndx-log* pattern. Or, to search for rolled up log-000001 and log-000002 indices, specify the rollup_ndx-log-000001,rollup_ndx-log-000002 list.\nYou cannot search a mix of rollup and non-rollup indices with the same query.\nExample\nThe following example demonstrates the doc_count field, dynamic index names, and searching multiple rollup indices with the same rollup. Step 1: Add an index template for ISM to manage the rolling over of the indices aliased by log. PUT _index_template/ism_rollover { \"index_patterns\": [ \"log*\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"log\" } } } Step 2: Set up an ISM rollover policy to roll over any index whose name starts with log* after one document is uploaded to it, and then roll up the individual backing index. The target index name is dynamically generated from the source index name by prepending the string rollup_ndx- to the source index name. PUT _plugins/_ism/policies/rollover_policy { \"policy\": { \"description\": \"Example rollover policy.\", \"default_state\": \"rollover\", \"states\": [ { \"name\": \"rollover\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 1 } }], \"transitions\": [ { \"state_name\": \"rp\" }] }, { \"name\": \"rp\", \"actions\": [ { \"rollup\": { \"ism_rollup\": { \"target_index\": \"rollup_ndx-{{ctx.source_index}}\", \"description\": \"Example rollup job\", \"page_size\": 200, \"dimensions\": [ { \"date_histogram\": { \"source_field\": \"ts\", \"fixed_interval\": \"60m\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"message.keyword\" } }], \"metrics\": [ { \"source_field\": \"msg_size\", \"metrics\": [ { \"sum\": {} }] }] } } }], \"transitions\": [] }], \"ism_template\": { \"index_patterns\": [ \"log*\"], \"priority\": 100 } } } Step 3: Create an index named log-000001 and set up an alias log for it. PUT log -000001 { \"aliases\": { \"log\": { \"is_write_index\": true } } } Step 4: Index four documents into the index created above. Two of the documents have the message “Success”, and two have the message “Error”. POST log/_doc?refresh= true { \"ts\": \"2022-08-26T09:28:48-04:00\", \"message\": \"Success\", \"msg_size\": 10 } POST log/_doc?refresh= true { \"ts\": \"2022-08-26T10:06:25-04:00\", \"message\": \"Error\", \"msg_size\": 20 } POST log/_doc?refresh= true { \"ts\": \"2022-08-26T10:23:54-04:00\", \"message\": \"Error\", \"msg_size\": 30 } POST log/_doc?refresh= true { \"ts\": \"2022-08-26T10:53:41-04:00\", \"message\": \"Success\", \"msg_size\": 40 } Once you index the first document, the rollover action is executed. This action creates the index log-000002 with rollover_policy attached to it. Then the rollup action is executed, which creates the rollup index rollup_ndx-log-000001.\nTo monitor the status of rollover and rollup index creation, you can use the ISM explain API: GET _plugins/_ism/explain Step 5: Search the rollup index. GET rollup_ndx-log-*/_search { \"size\": 0, \"query\": { \"match_all\": {} }, \"aggregations\": { \"message_numbers\": { \"terms\": { \"field\": \"message.keyword\" }, \"aggs\": { \"per_message\": { \"terms\": { \"field\": \"message.keyword\" }, \"aggregations\": { \"sum_message\": { \"sum\": { \"field\": \"msg_size\" } } } } } } } } The response contains two buckets, “Error” and “Success”, and the document count for each bucket is 2: { \"took\": 30, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"aggregations\": { \"message_numbers\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Success\", \"doc_count\": 2, \"per_message\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Success\", \"doc_count\": 2, \"sum_message\": { \"value\": 50.0 } }] } }, { \"key\": \"Error\", \"doc_count\": 2, \"per_message\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Error\", \"doc_count\": 2, \"sum_message\": { \"value\": 50.0 } }] } }] } } }",
    "ancestors": [
      "Index management plugin"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/index-rollups/rollup-api/",
    "title": "Index rollups API",
    "content": "Use the index rollup operations to programmatically work with index rollup jobs.\nTable of contents Create or update an index rollup job Get an index rollup job Delete an index rollup job Start or stop an index rollup job Explain an index rollup job Create or update an index rollup job\nIntroduced 1.0\nCreates or updates an index rollup job.\nYou must provide the seq_no and primary_term parameters.\nRequest PUT _plugins/_rollup/jobs/&lt;rollup_id&gt; // Create PUT _plugins/_rollup/jobs/&lt;rollup_id&gt;?if_seq_no= 1 &amp;if_primary_term= 1 // Update { \"rollup\": { \"source_index\": \"nyc-taxi-data\", \"target_index\": \"rollup-nyc-taxi-data\", \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Days\" } }, \"description\": \"Example rollup job\", \"enabled\": true, \"page_size\": 200, \"delay\": 0, \"roles\": [ \"rollup_all\", \"nyc_taxi_all\", \"example_rollup_index_all\"], \"continuous\": false, \"dimensions\": { \"date_histogram\": { \"source_field\": \"tpep_pickup_datetime\", \"fixed_interval\": \"1h\", \"timezone\": \"America/Los_Angeles\" }, \"terms\": { \"source_field\": \"PULocationID\" }, \"metrics\": [ { \"source_field\": \"passenger_count\", \"metrics\": [ { \"avg\": {} }, { \"sum\": {} }, { \"max\": {} }, { \"min\": {} }, { \"value_count\": {} }] }] } } } You can specify the following options. Options Description Type Required source_index The name of the detector.\nString\nYes target_index Specify the target index that the rolled up data is ingested into. You can either create a new target index or use an existing index. The target index cannot be a combination of raw and rolled up data. This field supports dynamically generated index names like rollup_{{ctx.source_index}}, where source_index cannot contain wildcards.\nString\nYes schedule Schedule of the index rollup job which can be an interval or a cron expression.\nObject\nYes schedule.interval Specify the frequency of execution of the rollup job.\nObject\nNo schedule.interval.start_time Start time of the interval.\nTimestamp\nYes schedule.interval.period Define the interval period.\nString\nYes schedule.interval.unit Specify the time unit of the interval.\nString\nYes schedule.interval.cron Optionally, specify a cron expression to define therollup frequency.\nList\nNo schedule.interval.cron.expression Specify a Unix cron expression.\nString\nYes schedule.interval.cron.timezone Specify timezones as defined by the IANA Time Zone Database. Defaults to UTC.\nString\nNo description Optionally, describe the rollup job.\nString\nNo enabled When true, the index rollup job is scheduled. Default is true.\nBoolean\nYes continuous Specify whether or not the index rollup job continuously rolls up data forever or just executes over the current data set once and stops. Default is false.\nBoolean\nYes error_notification Set up a Mustache message template sent for error notifications. For example, if an index rollup job fails, the system sends a message to a Slack channel.\nObject\nNo page_size Specify the number of buckets to paginate through at a time while rolling up.\nNumber\nYes delay The number of milliseconds to delay execution of the index rollup job.\nLong\nNo dimensions Specify aggregations to create dimensions for the roll up time window.\nObject\nYes dimensions.date_histogram Specify either fixed_interval or calendar_interval, but not both. Either one limits what you can query in the target index.\nObject\nNo dimensions.date_histogram.fixed_interval Specify the fixed interval for aggregations in milliseconds, seconds, minutes, hours, or days.\nString\nNo dimensions.date_histogram.calendar_interval Specify the calendar interval for aggregations in minutes, hours, days, weeks, months, quarters, or years.\nString\nNo dimensions.date_histogram.field Specify the date field used in date histogram aggregation.\nString\nNo dimensions.date_histogram.timezone Specify the timezones as defined by the IANA Time Zone Database. The default is UTC.\nString\nNo dimensions.terms Specify the term aggregations that you want to roll up.\nObject\nNo dimensions.terms.fields Specify terms aggregation for compatible fields.\nObject\nNo dimensions.histogram Specify the histogram aggregations that you want to roll up.\nObject\nNo dimensions.histogram.field Add a field for histogram aggregations.\nString\nYes dimensions.histogram.interval Specify the histogram aggregation interval for the field.\nLong\nYes dimensions.metrics Specify a list of objects that represent the fields and metrics that you want to calculate.\nNested object\nNo dimensions.metrics.field Specify the field that you want to perform metric aggregations on.\nString\nNo dimensions.metrics.field.metrics Specify the metric aggregations you want to calculate for the field.\nMultiple strings\nNo Sample response { \"_id\": \"rollup_id\", \"_seqNo\": 1, \"_primaryTerm\": 1, \"rollup\": {... } } Get an index rollup job\nIntroduced 1.0\nReturns all information about an index rollup job based on the rollup_id.\nRequest GET _plugins/_rollup/jobs/&lt;rollup_id&gt; Sample response { \"_id\": \"my_rollup\", \"_seqNo\": 1, \"_primaryTerm\": 1, \"rollup\": {... } } Delete an index rollup job\nIntroduced 1.0\nDeletes an index rollup job based on the rollup_id.\nRequest DELETE _plugins/_rollup/jobs/&lt;rollup_id&gt; Sample response 200 OK Start or stop an index rollup job\nIntroduced 1.0\nStart or stop an index rollup job.\nRequest POST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_start POST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_stop Sample response 200 OK Explain an index rollup job\nIntroduced 1.0\nReturns detailed metadata information about the index rollup job and its current progress.\nRequest GET _plugins/_rollup/jobs/&lt;rollup_id&gt;/_explain Sample response { \"example_rollup\": { \"rollup_id\": \"example_rollup\", \"last_updated_time\": 1602014281, \"continuous\": { \"next_window_start_time\": 1602055591, \"next_window_end_time\": 1602075591 }, \"status\": \"running\", \"failure_reason\": null, \"stats\": { \"pages_processed\": 342, \"documents_processed\": 489359, \"rollups_indexed\": 3420, \"index_time_in_ms\": 30495, \"search_time_in_ms\": 584922 } } }",
    "ancestors": [
      "Index management plugin",
      "Index rollups"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/index-rollups/settings/",
    "title": "Settings",
    "content": "We don’t recommend changing these settings; the defaults should work well for most use cases.\nAll settings are available using the OpenSearch _cluster/settings operation. None require a restart, and all can be marked persistent or transient. Setting Default Description plugins.rollup.search.backoff_millis 1000 milliseconds\nThe backoff time between retries for failed rollup jobs. plugins.rollup.search.backoff_count 5\nHow many retries the plugin should attempt for failed rollup jobs. plugins.rollup.search.search_all_jobs false\nWhether OpenSearch should return all jobs that match all specified search terms. If disabled, OpenSearch returns just one, as opposed to all, of the jobs that matches the search terms. plugins.rollup.dashboards.enabled true\nWhether rollups are enabled in OpenSearch Dashboards. plugins.rollup.enabled true\nWhether the rollup plugin is enabled. plugins.ingest.backoff_millis 1000 milliseconds\nThe backoff time between data ingestions for rollup jobs. plugins.ingest.backoff_count 5\nHow many retries the plugin should attempt for failed ingestions.",
    "ancestors": [
      "Index management plugin",
      "Index rollups"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/index-transforms/index/",
    "title": "Index transforms",
    "content": "Whereas index rollup jobs let you reduce data granularity by rolling up old data into condensed indexes, transform jobs let you create a different, summarized view of your data centered around certain fields, so you can visualize or analyze the data in different ways.\nFor example, suppose that you have airline data that’s scattered across multiple fields and categories, and you want to view a summary of the data that’s organized by airline, quarter, and then price. You can use a transform job to create a new, summarized index that’s organized by those specific categories.\nYou can use transform jobs in two ways:\nUse the OpenSearch Dashboards UI to specify the index you want to transform and any optional data filters you want to use to filter the original index. Then select the fields you want to transform and the aggregations to use in the transformation. Finally, define a schedule for your job to follow.\nUse the transforms API to specify all the details about your job: the index you want to transform, target groups you want the transformed index to have, any aggregations you want to use to group columns, and a schedule for your job to follow.\nOpenSearch Dashboards provides a detailed summary of the jobs you created and their relevant information, such as associated indexes and job statuses. You can review and edit your job’s details and selections before creation, and even preview a transformed index’s data as you’re choosing which fields to transform. However, you can also use the REST API to create transform jobs and preview transform job results, but you must know all of the necessary settings and parameters to submit them as part of the HTTP request body. Submitting your transform job configurations as JSON scripts offers you more portability, allowing you to share and replicate your transform jobs, which is harder to do using OpenSearch Dashboards.\nYour use cases will help you decide which method to use to create transform jobs.\nCreate a transform job\nIf you don’t have any data in your cluster, you can use the sample flight data within OpenSearch Dashboards to try out transform jobs. Otherwise, after launching OpenSearch Dashboards, choose Index Management. Select Transform Jobs, and choose Create Transform Job.\nStep 1: Choose indexes\nIn the Job name and description section, specify a name and an optional description for your job.\nIn the Indices section, select the source and target index. You can either select an existing target index or create a new one by entering a name for your new index. If you want to transform just a subset of your source index, choose Edit data filter, and use the OpenSearch query DSL to specify a subset of your source index. For more information about the OpenSearch query DSL, see query DSL.\nChoose Next.\nStep 2: Select fields to transform\nAfter specifying the indexes, you can select the fields you want to use in your transform job, as well as whether to use groupings or aggregations.\nYou can use groupings to place your data into separate buckets in your transformed index. For example, if you want to group all of the airport destinations within the sample flight data, you can group the DestAirportID field into a target field of DestAirportID_terms field, and you can find the grouped airport IDs in your transformed index after the transform job finishes.\nOn the other hand, aggregations let you perform simple calculations. For example, you can include an aggregation in your transform job to define a new field of sum_of_total_ticket_price that calculates the sum of all airplane tickets, and then analyze the newly summer data within your transformed index.\nIn the data table, select the fields you want to transform and expand the drop-down menu within the column header to choose the grouping or aggregation you want to use.\nCurrently, transform jobs support histogram, date_histogram, and terms groupings. For more information about groupings, see Bucket Aggregations. In terms of aggregations, you can select from sum, avg, max, min, value_count, percentiles, and scripted_metric. For more information about aggregations, see Metric Aggregations.\nRepeat step 1 for any other fields that you want to transform.\nAfter selecting the fields that you want to transform and verifying the transformation, choose Next.\nStep 3: Specify a schedule\nYou can configure transform jobs to run once or multiple times on a schedule. Transform jobs are enabled by default.\nChoose whether the job should be continuous. Continuous jobs execute at each transform execution interval and incrementally transform newly modified buckets, which can include new data added to the source indexes. Non-continuous jobs execute only once.\nFor transformation execution interval, specify a transform interval in minutes, hours, or days. This interval dicatates how often continuous jobs should execute, and non-continuous jobs execute once after the interval elapses.\nUnder Advanced, specify an optional amount for Pages per execution. A larger number means more data is processed in each search request, but also uses more memory and causes higher latency. Exceeding allowed memory limits can cause exceptions and errors to occur.\nChoose Next.\nStep 4: Review and confirm details\nAfter confirming your transform job’s details are correct, choose Create Transform Job. If you want to edit any part of the job, choose Edit of the section you want to change, and make the necessary changes. You can’t change aggregations or groupings after creating a job.\nStep 5: Search through the transformed index.\nOnce the transform job finishes, you can use the _search API operation to search the target index. GET &lt;target_index&gt;/_search For example, after running a transform job that transforms the flight data based on a DestAirportID field, you can run the following request that returns all of the fields that have a value of SFO. Sample Request GET finished_flight_job/_search { \"query\": { \"match\": { \"DestAirportID_terms\": \"SFO\" } } } Sample Response { \"took\": 3, \"timed_out\": false, \"_shards\": { \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4, \"relation\": \"eq\" }, \"max_score\": 3.845883, \"hits\": [ { \"_index\": \"finished_flight_job\", \"_id\": \"dSNKGb8U3OJOmC4RqVCi1Q\", \"_score\": 3.845883, \"_source\": { \"transform._id\": \"sample_flight_job\", \"transform._doc_count\": 14, \"Carrier_terms\": \"Dashboards Airlines\", \"DestAirportID_terms\": \"SFO\" } }, { \"_index\": \"finished_flight_job\", \"_id\": \"_D7oqOy7drx9E-MG96U5RA\", \"_score\": 3.845883, \"_source\": { \"transform._id\": \"sample_flight_job\", \"transform._doc_count\": 14, \"Carrier_terms\": \"Logstash Airways\", \"DestAirportID_terms\": \"SFO\" } }, { \"_index\": \"finished_flight_job\", \"_id\": \"YuZ8tOt1OsBA54e84WuAEw\", \"_score\": 3.6988301, \"_source\": { \"transform._id\": \"sample_flight_job\", \"transform._doc_count\": 11, \"Carrier_terms\": \"ES-Air\", \"DestAirportID_terms\": \"SFO\" } }, { \"_index\": \"finished_flight_job\", \"_id\": \"W_-e7bVmH6eu8veJeK8ZxQ\", \"_score\": 3.6988301, \"_source\": { \"transform._id\": \"sample_flight_job\", \"transform._doc_count\": 10, \"Carrier_terms\": \"JetBeats\", \"DestAirportID_terms\": \"SFO\" } }] } }",
    "ancestors": [
      "Index management plugin"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/index-transforms/transforms-apis/",
    "title": "Transforms APIs",
    "content": "Aside from using OpenSearch Dashboards, you can also use the REST API to create, start, stop, and complete other operations relative to transform jobs.\nTable of contents Create a transform job Request format Path parameters Request body fields Update a transform job Request format Query parameters Request body fields Get a transform job’s details Request format Query parameters Start a transform job Request format Stop a transform job Request format Get the status of a transform job Request format Preview a transform job’s results Delete a transform job Request format Create a transform job\nIntroduced 1.0\nCreates a transform job.\nRequest format PUT _plugins/_transform/&lt;transform_id&gt; Path parameters Parameter Data Type Description transform_id\nString\nTransform ID Request body fields\nYou can specify the following options in the HTTP request body: Option Data Type Description Required enabled\nBoolean\nIf true, the transform job is enabled at creation.\nNo\ncontinuous\nBoolean\nSpecifies whether the transform job should be continuous. Continuous jobs execute every time they are scheduled according to the schedule field and run based off of newly transformed buckets as well as any new data added to source indexes. Non-continuous jobs execute only once. Default is false.\nNo\nschedule\nObject\nThe schedule for the transform job.\nYes\nstart_time\nInteger\nThe Unix epoch time of the transform job’s start time.\nYes\ndescription\nString\nDescribes the transform job.\nNo\nmetadata_id\nString\nAny metadata to be associated with the transform job.\nNo\nsource_index\nString\nThe source index containing the data to be transformed.\nYes\ntarget_index\nString\nThe target index the newly transformed data is added to. You can create a new index or update an existing one.\nYes\ndata_selection_query\nObject\nThe query DSL to use to filter a subset of the source index for the transform job. See query domain-specific language(DSL) for more information.\nYes\npage_size\nInteger\nThe number of buckets IM processes and indexes concurrently. A higher number results in better performance, but it requires more memory. If your machine runs out of memory, Index Management (IM) automatically adjusts this field and retries until the operation succeeds.\nYes\ngroups\nArray\nSpecifies the grouping(s) to use in the transform job. Supported groups are terms, histogram, and date_histogram. For more information, see Bucket Aggregations.\nYes if not using aggregations.\nsource_field\nString\nThe field(s) to transform.\nYes\naggregations\nObject\nThe aggregations to use in the transform job. Supported aggregations are sum, max, min, value_count, avg, scripted_metric, and percentiles. For more information, see Metric Aggregations.\nYes if not using groups. Sample Request\nThe following request creates a transform job with the id sample: PUT _plugins/_transform/sample { \"transform\": { \"enabled\": true, \"continuous\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } Sample Response { \"_id\": \"sample\", \"_version\": 7, \"_seq_no\": 13, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample\", \"schema_version\": 7, \"continuous\": true, \"schedule\": { \"interval\": { \"start_time\": 1621467964243, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": null, \"updated_at\": 1621467964243, \"enabled\": true, \"enabled_at\": 1621467964243, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } Update a transform job\nIntroduced 1.0\nUpdates the transform job if transform_id already exists. For this request you must specify the sequence number and primary term of the transform to be updated. To get these, use the Get a transform job’s details API call.\nRequest format PUT _plugins/_transform/&lt;transform_id&gt;?if_seq_no=&lt;seq_no&gt;&amp;if_primary_term=&lt;primary_term&gt; Query parameters\nThe update operation supports the following query parameters: Parameter Description Required seq_no Only perform the transform operation if the last operation that changed the transform job has the specified sequence number.\nYes primary_term Only perform the transform operation if the last operation that changed the transform job has the specified sequence term.\nYes Request body fields\nYou can update the following fields. Option Data Type Description schedule\nObject\nThe schedule for the transform job. Contains the fields interval.start_time, interval.period, and interval.unit.\nstart_time\nInteger\nThe Unix epoch start time of the transform job.\nperiod\nInteger\nHow often to execute the transform job.\nunit\nString\nThe unit of time associated with the execution period. Available options are Minutes, Hours, and Days.\ndescription\nInteger\nDescribes the transform job.\npage_size\nInteger\nThe number of buckets IM processes and indexes concurrently. A higher number results in better performance, but it requires more memory. If your machine runs out of memory, IM automatically adjusts this field and retries until the operation succeeds. Sample Request\nThe following request updates a transform job with the id sample, sequence number 13, and primary term 1: PUT _plugins/_transform/sample?if_seq_no= 13 &amp;if_primary_term= 1 { \"transform\": { \"enabled\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } Sample Response PUT _plugins/_transform/sample?if_seq_no= 13 &amp;if_primary_term= 1 { \"transform\": { \"enabled\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } Get a transform job’s details\nIntroduced 1.0\nReturns a transform job’s details.\nRequest format GET _plugins/_transform/&lt;transform_id&gt; Sample Request\nThe following request returns the details of the transform job with the id sample: GET _plugins/_transform/sample Sample Response { \"_id\": \"sample\", \"_version\": 7, \"_seq_no\": 13, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample\", \"schema_version\": 7, \"continuous\": true, \"schedule\": { \"interval\": { \"start_time\": 1621467964243, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": null, \"updated_at\": 1621467964243, \"enabled\": true, \"enabled_at\": 1621467964243, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } You can also get details of all transform jobs by omitting transform_id.\nSample Request\nThe following request returns the details of all transform jobs: GET _plugins/_transform/ Sample Response { \"total_transforms\": 1, \"transforms\": [ { \"_id\": \"sample\", \"_seq_no\": 13, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample\", \"schema_version\": 7, \"continuous\": true, \"schedule\": { \"interval\": { \"start_time\": 1621467964243, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": null, \"updated_at\": 1621467964243, \"enabled\": true, \"enabled_at\": 1621467964243, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } }] } Query parameters\nYou can specify the following GET API operation’s query parameters to filter the results. Parameter Description Required from\nThe starting transform to return. Default is 0.\nNo\nsize\nSpecifies the number of transforms to return. Default is 10.\nNo\nsearch\nThe search term to use to filter results.\nNo\nsortField\nThe field to sort results with.\nNo\nsortDirection\nSpecifies the direction to sort results in. Can be ASC or DESC. Default is ASC.\nNo Sample Request\nThe following request returns two results starting from transform 8: GET _plugins/_transform?size= 2 &amp;from= 8 Sample Response { \"total_transforms\": 18, \"transforms\": [ { \"_id\": \"sample8\", \"_seq_no\": 93, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample8\", \"schema_version\": 7, \"schedule\": { \"interval\": { \"start_time\": 1622063596812, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": \"y4hFAB2ZURQ2dzY7BAMxWA\", \"updated_at\": 1622063657233, \"enabled\": false, \"enabled_at\": null, \"description\": \"Sample transform job\", \"source_index\": \"sample_index3\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target3\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } }, { \"_id\": \"sample9\", \"_seq_no\": 98, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample9\", \"schema_version\": 7, \"schedule\": { \"interval\": { \"start_time\": 1622063598065, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": \"x8tCIiYMTE3veSbIJkit5A\", \"updated_at\": 1622063658388, \"enabled\": false, \"enabled_at\": null, \"description\": \"Sample transform job\", \"source_index\": \"sample_index4\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target4\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } }] } Start a transform job\nIntroduced 1.0\nTransform jobs created using the API are automatically enabled, but if you ever need to enable a job, you can use the start API operation.\nRequest format POST _plugins/_transform/&lt;transform_id&gt;/_start Sample Request\nThe following request starts the transform job with the ID sample: POST _plugins/_transform/sample/_start Sample Response { \"acknowledged\": true } Stop a transform job\nIntroduced 1.0\nStops a transform job.\nRequest format POST _plugins/_transform/&lt;transform_id&gt;/_stop Sample Request\nThe following request stops the transform job with the ID sample: POST _plugins/_transform/sample/_stop Sample Response { \"acknowledged\": true } Get the status of a transform job\nIntroduced 1.0\nReturns the status and metadata of a transform job.\nRequest format GET _plugins/_transform/&lt;transform_id&gt;/_explain Sample Request\nThe following request returns the details of the transform job with the ID sample: GET _plugins/_transform/sample/_explain Sample Response { \"sample\": { \"metadata_id\": \"PzmjweME5xbgkenl9UpsYw\", \"transform_metadata\": { \"continuous_stats\": { \"last_timestamp\": 1621883525672, \"documents_behind\": { \"sample_index\": 72 } }, \"transform_id\": \"sample\", \"last_updated_at\": 1621883525873, \"status\": \"finished\", \"failure_reason\": \"null\", \"stats\": { \"pages_processed\": 0, \"documents_processed\": 0, \"documents_indexed\": 0, \"index_time_in_millis\": 0, \"search_time_in_millis\": 0 } } } } Preview a transform job’s results\nIntroduced 1.0\nReturns a preview of what a transformed index would look like.\nSample Request POST _plugins/_transform/_preview { \"transform\": { \"enabled\": false, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"test transform\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 10, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } Sample Response { \"documents\": [ { \"quantity\": 862.0, \"gender\": \"FEMALE\", \"day\": \"Friday\" }, { \"quantity\": 682.0, \"gender\": \"FEMALE\", \"day\": \"Monday\" }, { \"quantity\": 772.0, \"gender\": \"FEMALE\", \"day\": \"Saturday\" }, { \"quantity\": 669.0, \"gender\": \"FEMALE\", \"day\": \"Sunday\" }, { \"quantity\": 887.0, \"gender\": \"FEMALE\", \"day\": \"Thursday\" }] } Delete a transform job\nIntroduced 1.0\nDeletes a transform job. This operation does not delete the source or target indexes.\nRequest format DELETE _plugins/_transform/&lt;transform_id&gt; Sample Request\nThe following request deletes the transform job with the ID sample: DELETE _plugins/_transform/sample Sample Response { \"took\": 205, \"errors\": false, \"items\": [ { \"delete\": { \"_index\": \".opensearch-ism-config\", \"_id\": \"sample\", \"_version\": 4, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 6, \"_primary_term\": 1, \"status\": 200 } }] }",
    "ancestors": [
      "Index management plugin",
      "Index transforms"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/index/",
    "title": "About Index Management",
    "content": "OpenSearch Dashboards\nThe Index Management (IM) plugin lets you automate recurring index management activities and reduce storage costs.",
    "ancestors": [
      "Index management plugin"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/ism/api/",
    "title": "ISM API",
    "content": "Use the index state management operations to programmatically work with policies and managed indexes.\nTable of contents Create policy Add policy Update policy Get policy Remove policy from index Update managed index policy Retry failed index Explain index Delete policy Error prevention validation Create policy\nIntroduced 1.0\nCreates a policy.\nSample request PUT _plugins/_ism/policies/policy_ 1 { \"policy\": { \"description\": \"ingesting logs\", \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } }], \"transitions\": [ { \"state_name\": \"search\" }] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } }] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} }], \"transitions\": [] }] } } Sample response { \"_id\": \"policy_1\", \"_version\": 1, \"_primary_term\": 1, \"_seq_no\": 7, \"policy\": { \"policy\": { \"policy_id\": \"policy_1\", \"description\": \"ingesting logs\", \"last_updated_time\": 1577990761311, \"schema_version\": 1, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } }], \"transitions\": [ { \"state_name\": \"search\" }] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } }] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} }], \"transitions\": [] }] } } } Add policy\nIntroduced 1.0\nAdds a policy to an index. This operation does not change the policy if the index already has one.\nSample request POST _plugins/_ism/add/index_ 1 { \"policy_id\": \"policy_1\" } Sample response { \"updated_indices\": 1, \"failures\": false, \"failed_indices\": [] } If you use a wildcard * while adding a policy to an index, the ISM plugin interprets * as all indexes, including system indexes like.opendistro-security, which stores users, roles, and tenants. A delete action in your policy might accidentally delete all user roles and tenants in your cluster.\nDon’t use the broad * wildcard, and instead add a prefix, such as my-logs*, when specifying indexes with the _ism/add API.\nUpdate policy\nIntroduced 1.0\nUpdates a policy. Use the seq_no and primary_term parameters to update an existing policy. If these numbers don’t match the existing policy or the policy doesn’t exist, ISM throws an error.\nIt’s possible that the policy currently applied to your index isn’t the most up-to-date policy available. To see what policy is currently applied to your index, see Explain index. To get the most up-to-date version of a policy, see Get policy.\nSample request PUT _plugins/_ism/policies/policy_ 1?if_seq_no= 7 &amp;if_primary_term= 1 { \"policy\": { \"description\": \"ingesting logs\", \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } }], \"transitions\": [ { \"state_name\": \"search\" }] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } }] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} }], \"transitions\": [] }] } } Sample response { \"_id\": \"policy_1\", \"_version\": 2, \"_primary_term\": 1, \"_seq_no\": 10, \"policy\": { \"policy\": { \"policy_id\": \"policy_1\", \"description\": \"ingesting logs\", \"last_updated_time\": 1577990934044, \"schema_version\": 1, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } }], \"transitions\": [ { \"state_name\": \"search\" }] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } }] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} }], \"transitions\": [] }] } } } Get policy\nIntroduced 1.0\nGets the policy by policy_id.\nSample request GET _plugins/_ism/policies/policy_ 1 Sample response { \"_id\": \"policy_1\", \"_version\": 2, \"_seq_no\": 10, \"_primary_term\": 1, \"policy\": { \"policy_id\": \"policy_1\", \"description\": \"ingesting logs\", \"last_updated_time\": 1577990934044, \"schema_version\": 1, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } }], \"transitions\": [ { \"state_name\": \"search\" }] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } }] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} }], \"transitions\": [] }] } } Remove policy from index\nIntroduced 1.0\nRemoves any ISM policy from the index.\nSample request POST _plugins/_ism/remove/index_ 1 Sample response { \"updated_indices\": 1, \"failures\": false, \"failed_indices\": [] } Update managed index policy\nIntroduced 1.0\nUpdates the managed index policy to a new policy (or to a new version of the policy). You can use an index pattern to update multiple indexes at once. When updating multiple indexes, you might want to include a state filter to only affect certain managed indexes. The change policy filters out all the existing managed indexes and only applies the change to the ones in the state that you specify. You can also explicitly specify the state that the managed index transitions to after the change policy takes effect.\nA policy change is an asynchronous background process. The changes are queued and are not executed immediately by the background process. This delay in execution protects the currently running managed indexes from being put into a broken state. If the policy you are changing to has only some small configuration changes, then the change takes place immediately. For example, if the policy changes the min_index_age parameter in a rollover condition from 1000d to 100d, this change takes place immediately in its next execution. If the change modifies the state, actions, or the order of actions of the current state the index is in, then the change happens at the end of its current state before transitioning to a new state.\nIn this example, the policy applied on the index_1 index is changed to policy_1, which could either be a completely new policy or an updated version of its existing policy. The process only applies the change if the index is currently in the searches state. After this change in policy takes place, index_1 transitions to the delete state.\nSample request POST _plugins/_ism/change_policy/index_ 1 { \"policy_id\": \"policy_1\", \"state\": \"delete\", \"include\": [ { \"state\": \"searches\" }] } Sample response { \"updated_indices\": 0, \"failures\": false, \"failed_indices\": [] } Retry failed index\nIntroduced 1.0\nRetries the failed action for an index. For the retry call to succeed, ISM must manage the index, and the index must be in a failed state. You can use index patterns ( *) to retry multiple failed indexes.\nSample request POST _plugins/_ism/retry/index_ 1 { \"state\": \"delete\" } Sample response { \"updated_indices\": 0, \"failures\": false, \"failed_indices\": [] } Explain index\nIntroduced 1.0\nGets the current state of the index. You can use index patterns to get the status of multiple indexes.\nSample request GET _plugins/_ism/explain/index_ 1 Sample response { \"index_1\": { \"index.plugins.index_state_management.policy_id\": \"policy_1\" } } Optionally, you can add the show_policy parameter to your request’s path to get the policy that is currently applied to your index, which is useful for seeing whether the policy applied to your index is the latest one. To get the most up-to-date policy, see Get Policy API.\nSample request GET _plugins/_ism/explain/index_ 1?show_policy= true Sample response { \"index_1\": { \"index.plugins.index_state_management.policy_id\": \"sample-policy\", \"index.opendistro.index_state_management.policy_id\": \"sample-policy\", \"index\": \"index_1\", \"index_uuid\": \"gCFlS_zcTdih8xyxf3jQ-A\", \"policy_id\": \"sample-policy\", \"enabled\": true, \"policy\": { \"policy_id\": \"sample-policy\", \"description\": \"ingesting logs\", \"last_updated_time\": 1647284980148, \"schema_version\": 13, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [...], \"ism_template\": null } }, \"total_managed_indices\": 1 } The plugins.index_state_management.policy_id setting is deprecated starting from ODFE version 1.13.0. We retain this field in the response API for consistency.\nDelete policy\nIntroduced 1.0\nDeletes the policy by policy_id.\nSample request DELETE _plugins/_ism/policies/policy_ 1 Sample response { \"_index\": \".opendistro-ism-config\", \"_id\": \"policy_1\", \"_version\": 3, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 15, \"_primary_term\": 1 } Error prevention validation\nIntroduced 2.4\nISM allows you to run an action automatically. However, running an action can fail for a variety of reasons. You can use error prevention validation to test an action in order to rule out failures.\nTo enable error prevention validation, set the plugins.index_state_management.validation_service.enabled setting to true: PUT _cluster/settings { \"persistent\": { \"plugins.index_state_management.validation_action.enabled\": true } } Sample response { \"acknowledged\": true, \"persistent\": { \"plugins\": { \"index_state_management\": { \"validation_action\": { \"enabled\": \"true\" } } } }, \"transient\": { } } To check an error prevention validation status and message, pass validate_action=true to the _plugins/_ism/explain endpoint: GET _plugins/_ism/explain/test-000001?validate_action = true Sample response\nThe response contains an additional validate object with a validation message and status: { \"test-000001\": { \"index.plugins.index_state_management.policy_id\": \"test_rollover\", \"index.opendistro.index_state_management.policy_id\": \"test_rollover\", \"index\": \"test-000001\", \"index_uuid\": \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\": \"test_rollover\", \"policy_seq_no\": -2, \"policy_primary_term\": 0, \"rolled_over\": false, \"index_creation_date\": 1667410460649, \"state\": { \"name\": \"rollover\", \"start_time\": 1667410766045 }, \"action\": { \"name\": \"rollover\", \"start_time\": 1667411127803, \"index\": 0, \"failed\": false, \"consumed_retries\": 0, \"last_retry_time\": 0 }, \"step\": { \"name\": \"attempt_rollover\", \"start_time\": 1667411127803, \"step_status\": \"starting\" }, \"retry_info\": { \"failed\": true, \"consumed_retries\": 0 }, \"info\": { \"message\": \"Previous action was not able to update IndexMetaData.\" }, \"enabled\": false, \"validate\": { \"validation_message\": \"Missing rollover_alias index setting [index=test-000001]\", \"validation_status\": \"re_validating\" } }, \"total_managed_indices\": 1 } If you pass validate_action=false or do not pass a validate_action value to the _plugins/_ism/explain endpoint, the response will not contain an error prevention validation status and message: GET _plugins/_ism/explain/test-000001?validate_action = false Or: GET _plugins/_ism/explain/test-000001 Sample response { \"test-000001\": { \"index.plugins.index_state_management.policy_id\": \"test_rollover\", \"index.opendistro.index_state_management.policy_id\": \"test_rollover\", \"index\": \"test-000001\", \"index_uuid\": \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\": \"test_rollover\", \"policy_seq_no\": -2, \"policy_primary_term\": 0, \"rolled_over\": false, \"index_creation_date\": 1667410460649, \"state\": { \"name\": \"rollover\", \"start_time\": 1667410766045 }, \"action\": { \"name\": \"rollover\", \"start_time\": 1667411127803, \"index\": 0, \"failed\": false, \"consumed_retries\": 0, \"last_retry_time\": 0 }, \"step\": { \"name\": \"attempt_rollover\", \"start_time\": 1667411127803, \"step_status\": \"starting\" }, \"retry_info\": { \"failed\": true, \"consumed_retries\": 0 }, \"info\": { \"message\": \"Previous action was not able to update IndexMetaData.\" }, \"enabled\": false }, \"total_managed_indices\": 1 }",
    "ancestors": [
      "Index management plugin",
      "Index State Management"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/ism/error-prevention/api/",
    "title": "ISM Error Prevention API",
    "content": "The ISM Error Prevention API allows you to enable Index State Management (ISM) error prevention and check the validation status and message.\nEnable error prevention validation\nYou can configure error prevention validation by setting the plugins.index_state_management.validation_service.enabled parameter.\nSample request PUT _cluster/settings { \"persistent\": { \"plugins.index_state_management.validation_action.enabled\": true } } Sample response { \"acknowledged\": true, \"persistent\": { \"plugins\": { \"index_state_management\": { \"validation_action\": { \"enabled\": \"true\" } } } }, \"transient\": { } } Check validation status and message via the Explain API\nPass the validate_action=true path parameter in the Explain API URI to see the validation status and message.\nSample request GET _plugins/_ism/explain/test-000001?validate_action = true Sample response { \"test-000001\": { \"index.plugins.index_state_management.policy_id\": \"test_rollover\", \"index.opendistro.index_state_management.policy_id\": \"test_rollover\", \"index\": \"test-000001\", \"index_uuid\": \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\": \"test_rollover\", \"policy_seq_no\": -2, \"policy_primary_term\": 0, \"rolled_over\": false, \"index_creation_date\": 1667410460649, \"state\": { \"name\": \"rollover\", \"start_time\": 1667410766045 }, \"action\": { \"name\": \"rollover\", \"start_time\": 1667411127803, \"index\": 0, \"failed\": false, \"consumed_retries\": 0, \"last_retry_time\": 0 }, \"step\": { \"name\": \"attempt_rollover\", \"start_time\": 1667411127803, \"step_status\": \"starting\" }, \"retry_info\": { \"failed\": true, \"consumed_retries\": 0 }, \"info\": { \"message\": \"Previous action was not able to update IndexMetaData.\" }, \"enabled\": false, \"validate\": { \"validation_message\": \"Missing rollover_alias index setting [index=test-000001]\", \"validation_status\": \"re_validating\" } }, \"total_managed_indices\": 1 } If you pass the parameter without a value or false, then it doesn’t return the validation status and message. Only if you pass validate_action=true will the response will return the validation status and message.\nSample request GET _plugins/_ism/explain/test-000001?validate_action = false --- OR --- GET _plugins/_ism/explain/test-000001 Sample response { \"test-000001\": { \"index.plugins.index_state_management.policy_id\": \"test_rollover\", \"index.opendistro.index_state_management.policy_id\": \"test_rollover\", \"index\": \"test-000001\", \"index_uuid\": \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\": \"test_rollover\", \"policy_seq_no\": -2, \"policy_primary_term\": 0, \"rolled_over\": false, \"index_creation_date\": 1667410460649, \"state\": { \"name\": \"rollover\", \"start_time\": 1667410766045 }, \"action\": { \"name\": \"rollover\", \"start_time\": 1667411127803, \"index\": 0, \"failed\": false, \"consumed_retries\": 0, \"last_retry_time\": 0 }, \"step\": { \"name\": \"attempt_rollover\", \"start_time\": 1667411127803, \"step_status\": \"starting\" }, \"retry_info\": { \"failed\": true, \"consumed_retries\": 0 }, \"info\": { \"message\": \"Previous action was not able to update IndexMetaData.\" }, \"enabled\": false }, \"total_managed_indices\": 1 }",
    "ancestors": [
      "Index management plugin",
      "Index State Management",
      "ISM Error Prevention"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/ism/error-prevention/index/",
    "title": "ISM Error Prevention",
    "content": "Error prevention validates Index State Management (ISM) actions before they are performed in order to prevent actions from failing. It also outputs additional information from the action validation results in the response of the Index Explain API. Validation rules and troubleshooting of each action are listed in the following sections.\nTable of contents rollover delete force_merge replica_count open read_only read_write rollover\nISM does not perform a rollover action for an index under any of these conditions: The index is not the write index. The index does not have an alias. The rollover policy does not contain a rollover_alias index setting. Skipping of a rollover action has occured. The index has already been rolled over using the alias successfully.\ndelete\nISM does not perform a delete action for an index under any of these conditions:\nThe index does not exist.\nThe index name is invalid.\nThe index is the write index for a data stream.\nforce_merge\nISM does not perform a force_merge action for an index if its dataset is too large and exceeds the threshold.\nreplica_count\nISM does not perform a replica_count action for an index under any of these conditions:\nThe amount of data exceeds the threshold.\nThe number of shards exceeds the maximum.\nopen\nISM does not perform an open action for an index under any of these conditions:\nThe index is blocked.\nThe number of shards exceeds the maximum.\nread_only\nISM does not perform a read_only action for an index under any of these conditions:\nThe index is blocked.\nThe amount of data exceeds the threshold.\nread_write\nISM does not perform a read_write action for an index if the index is blocked.",
    "ancestors": [
      "Index management plugin"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/ism/error-prevention/resolutions/",
    "title": "ISM Error Prevention resolutions",
    "content": "Resolutions of errors for each validation rule action are listed in the following sections.\nTable of contents The index is not the write index The index does not have an alias Skipping rollover action is true This index has already been rolled over successfully The rollover policy misses rollover_alias index setting Data too large and exceeding the threshold Maximum shards exceeded The index is a write index for some data stream The index is blocked The index is not the write index\nTo confirm that the index is a write index, run the following request: GET &lt;index&gt;/_alias?pretty If the response does not contain \"is_write_index\": true, the index is not a write index. The following example confirms that the index is a write index: { \"&lt;index&gt;\": { \"aliases\": { \"&lt;index_alias&gt;\": { \"is_write_index\": true } } } } To set the index as a write index, run the following request: PUT &lt;index&gt; { \"aliases\": { \"&lt;index_alias&gt;\": { \"is_write_index\": true } } } The index does not have an alias\nIf the index does not have an alias, you can add one by running the following request: POST _aliases { \"actions\": [ { \"add\": { \"index\": \"&lt;target_index&gt;\", \"alias\": \"&lt;index_alias&gt;\" } }] } Skipping rollover action is true\nIn the event that skipping a rollover action occurs, run the following request: GET &lt;target_index&gt;/_settings?pretty If you receive the response in the first example, you can reset it by running the request in the second example: { \"index\": { \"opendistro.index_state_management.rollover_skip\": true } } PUT &lt;target_index&gt;/_settings { \"index\": { \"index_state_management.rollover_skip\": false } } This index has already been rolled over successfully\nRemove the rollover policy from the index to prevent this error from reoccurring.\nThe rollover policy misses rollover_alias index setting\nAdd a rollover_alias index setting to the rollover policy to resolve this issue. Run the following request: PUT _index_template/ism_rollover { \"index_patterns\": [ \"&lt;index_patterns_in_rollover_policy&gt;\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"&lt;rollover_alias&gt;\" } } } Data too large and exceeding the threshold\nCheck the JVM information and increase the heap memory.\nMaximum shards exceeded\nThe shard limit per node, or per index, causes this issue to occur. Check whether there is a total_shards_per_node limit by running the following request: GET /_cluster/settings If the response contains total_shards_per_node, increase its value temporarily by running the following request: PUT _cluster/settings { \"transient\": { \"cluster.routing.allocation.total_shards_per_node\":100 } } To check whether there is a shard limit for an index, run the following request: GET &lt;index&gt;/_settings/index.routing- If the response contains the setting in the first example, increase its value or set it to -1 for unlimited shards, as shown in the second example: \"index\": { \"routing\": { \"allocation\": { \"total_shards_per_node\": \"10\" } } } PUT &lt;index&gt;/_settings { \"index.routing.allocation.total_shards_per_node\":-1 } The index is a write index for some data stream\nIf you still want to delete the index, check your data stream settings and change the write index.\nThe index is blocked\nGenerally, the index is blocked because disk usage has exceeded the flood-stage watermark and the index has a read-only-allow-delete block. To resolve this issue, you can:\nRemove the -index.blocks.read_only_allow_delete- parameter.\nTemporarily increase the disk watermarks.\nTemporarily disable the disk allocation threshold.\nTo prevent the issue from reoccurring, it is better to reduce the usage of the disk by increasing disk space, adding new nodes, or removing data or indexes that are no longer needed.\nRemove -index.blocks.read_only_allow_delete- by running the following request: PUT &lt;index&gt;/_settings { \"index.blocks.read_only_allow_delete\": null } Increase the low disk watermarks by running the following request: PUT _cluster/settings { \"transient\": { \"cluster\": { \"routing\": { \"allocation\": { \"disk\": { \"watermark\": { \"low\": \"25.0gb\" } } } } } } } Disable the disk allocation threshold by running the following request: PUT _cluster/settings { \"transient\": { \"cluster\": { \"routing\": { \"allocation\": { \"disk\": { \"threshold_enabled\": false } } } } } }",
    "ancestors": [
      "Index management plugin",
      "Index State Management",
      "ISM Error Prevention"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/ism/index/",
    "title": "Index State Management",
    "content": "OpenSearch Dashboards\nIf you analyze time-series data, you likely prioritize new data over old data. You might periodically perform certain operations on older indexes, such as reducing replica count or deleting them.\nIndex State Management (ISM) is a plugin that lets you automate these periodic, administrative operations by triggering them based on changes in the index age, index size, or number of documents. Using the ISM plugin, you can define policies that automatically handle index rollovers or deletions to fit your use case.\nFor example, you can define a policy that moves your index into a read_only state after 30 days and then deletes it after a set period of 90 days. You can also set up the policy to send you a notification message when the index is deleted.\nYou might want to perform an index rollover after a certain amount of time or run a force_merge operation on an index during off-peak hours to improve search performance during peak hours.\nTo use the ISM plugin, your user role needs to be mapped to the all_access role that gives you full access to the cluster. To learn more, see Users and roles.\nGet started with ISM\nTo get started, choose Index Management in OpenSearch Dashboards.\nStep 1: Set up policies\nA policy is a set of rules that describes how an index should be managed. For information about creating a policy, see Policies.\nYou can use the visual editor or JSON editor to create policies. Compared to the JSON editor, the visual editor offers a more structured way of defining policies by separating the process into creating error notifications, defining ISM templates, and adding states. We recommend using the visual editor if you want to see pre-defined fields, such as which actions you can assign to a state or under what conditions a state can transition into a destination state.\nVisual editor\nChoose the Index Policies tab.\nChoose Create policy.\nChoose Visual editor.\nIn the Policy info section, enter a policy ID and an optional description.\nIn the Error notification section, set up an optional error notification that gets sent whenever a policy execution fails. For more information, see Error notifications. If you’re using auto rollovers in your policy, we recommend setting up error notifications, which notify you of unexpectedly large indexes if rollovers fail.\nIn ISM templates, enter any ISM template patterns to automatically apply this policy to future indexes. For example, if you specify a template of sample-index*, the ISM plugin automatically applies this policy to any indexes whose names start with sample-index. Your pattern cannot contain any of the following characters::, \", +, /, \\, |,?, #, &gt;, and &lt;.\nIn States, add any states you want to include in the policy. Each state has actions the plugin executes when the index enters a certain state, and transitions, which have conditions that, when met, transition the index into a destination state. The first state you create in a policy is automatically set as the initial state. Each policy must have at least one state, but actions and transitions are optional.\nChoose Create.\nJSON editor\nChoose the Index Policies tab.\nChoose Create policy.\nChoose JSON editor.\nIn the Name policy section, enter a policy ID.\nIn the Define policy section, enter your policy.\nChoose Create.\nAfter you create a policy, your next step is to attach it to an index or indexes.\nYou can set up an ism_template in the policy so when an index that matches the ISM template pattern is created, the plugin automatically attaches the policy to the index.\nThe following example demonstrates how to create a policy that automatically gets attached to all indexes whose names start with index_name-. PUT _plugins/_ism/policies/policy_id { \"policy\": { \"description\": \"Example policy.\", \"default_state\": \"...\", \"states\": [...], \"ism_template\": { \"index_patterns\": [ \"index_name-*\"], \"priority\": 100 } } } If you have more than one template that matches an index pattern, ISM uses the priority value to determine which template to apply.\nFor an example ISM template policy, see Sample policy with ISM template for auto rollover.\nOlder versions of the plugin include the policy_id in an index template, so when an index is created that matches the index template pattern, the index will have the policy attached to it: PUT _index_template/&lt;template_name&gt; { \"index_patterns\": [ \"index_name-*\"], \"template\": { \"settings\": { \"opendistro.index_state_management.policy_id\": \"policy_id\" } } } The opendistro.index_state_management.policy_id setting is deprecated. You can continue to automatically manage newly created indexes with the ISM template field.\nStep 2: Attach policies to indexes\nChoose indexes.\nChoose the index or indexes that you want to attach your policy to.\nChoose Apply policy.\nFrom the Policy ID menu, choose the policy that you created.\nYou can see a preview of your policy.\nIf your policy includes a rollover operation, specify a rollover alias.\nMake sure that the alias that you enter already exists. For more information about the rollover operation, see rollover.\nChoose Apply.\nAfter you attach a policy to an index, ISM creates a job that runs every 5 minutes by default to perform policy actions, check conditions, and transition the index into different states. To change the default time interval for this job, see Settings.\nISM does not run jobs if the cluster state is red.\nStep 3: Manage indexes\nChoose Managed indexes.\nTo change your policy, see Change Policy.\nTo attach a rollover alias to your index, select your policy and choose Add rollover alias.\nMake sure that the alias that you enter already exists. For more information about the rollover operation, see rollover.\nTo remove a policy, choose your policy, and then choose Remove policy.\nTo retry a policy, choose your policy, and then choose Retry policy.\nFor information about managing your policies, see Managed indexes.",
    "ancestors": [
      "Index management plugin"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/ism/managedindexes/",
    "title": "Managed Indices",
    "content": "You can change or update a policy using the managed index operations.\nThis table lists the fields of managed index operations. Parameter Description Type Required Read Only name The name of the managed index policy. string Yes\nNo index The name of the managed index that this policy is managing. string Yes\nNo index_uuid The uuid of the index. string Yes\nNo enabled When true, the managed index is scheduled and run by the scheduler. boolean Yes\nNo enabled_time The time the managed index was last enabled. If the managed index process is disabled, then this is null. timestamp Yes\nYes last_updated_time The time the managed index was last updated. timestamp Yes\nYes schedule The schedule of the managed index job. object Yes\nNo policy_id The name of the policy used by this managed index. string Yes\nNo policy_seq_no The sequence number of the policy used by this managed index. number Yes\nNo policy_primary_term The primary term of the policy used by this managed index. number Yes\nNo policy_version The version of the policy used by this managed index. number Yes\nYes policy The cached JSON of the policy for the policy_version that’s used during runs. If the policy is null, it means that this is the first execution of the job and the latest policy document is read in/saved. object No\nNo change_policy The information regarding what policy and state to change to. object No\nNo policy_name The name of the policy to update to. To update to the latest version, set this to be the same as the current policy_name. string No\nYes state The state of the managed index after it finishes updating. If no state is specified, it’s assumed that the policy structure did not change. string No\nYes The following example shows a managed index policy: { \"managed_index\": { \"name\": \"my_index\", \"index\": \"my_index\", \"index_uuid\": \"sOKSOfkdsoSKeofjIS\", \"enabled\": true, \"enabled_time\": 1553112384, \"last_updated_time\": 1553112384, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"MINUTES\", \"start_time\": 1553112384 } }, \"policy_id\": \"log_rotation\", \"policy_version\": 1, \"policy\": {... }, \"change_policy\": null } } Change policy\nYou can change any managed index policy, but ISM has a few constraints in place to make sure that policy changes don’t break indices.\nIf an index is stuck in its current state, never proceeding, and you want to update its policy immediately, make sure that the new policy includes the same state—same name, same actions, same order—as the old policy. In this case, even if the policy is in the middle of executing an action, ISM applies the new policy.\nIf you update the policy without including an identical state, ISM updates the policy only after all actions in the current state finish executing. Alternately, you can choose a specific state in your old policy after which you want the new policy to take effect.\nTo change a policy using OpenSearch Dashboards, do the following:\nUnder Managed indices, choose the indices that you want to attach the new policy to.\nTo attach the new policy to indices in specific states, choose Choose state filters, and then choose those states.\nUnder Choose New Policy, choose the new policy.\nTo start the new policy for indices in the current state, choose Keep indices in their current state after the policy takes effect.\nTo start the new policy in a specific state, choose Start from a chosen state after changing policies, and then choose the default start state in your new policy.",
    "ancestors": [
      "Index management plugin",
      "Index State Management"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/ism/policies/",
    "title": "Policies",
    "content": "Policies are JSON documents that define the following:\nThe states that an index can be in, including the default state for new indexes. For example, you might name your states “hot,” “warm,” “delete,” and so on. For more information, see States.\nAny actions that you want the plugin to take when an index enters a state, such as performing a rollover. For more information, see Actions.\nThe conditions that must be met for an index to move into a new state, known as transitions. For example, if an index is more than eight weeks old, you might want to move it to the “delete” state. For more information, see Transitions.\nIn other words, a policy defines the states that an index can be in, the actions to perform when in a state, and the conditions that must be met to transition between states.\nYou have complete flexibility in the way you can design your policies. You can create any state, transition to any other state, and specify any number of actions in each state.\nThis table lists the relevant fields of a policy. Field Description Type Required Read Only policy_id The name of the policy. string Yes\nYes description A human-readable description of the policy. string Yes\nNo ism_template Specify an ISM template pattern that matches the index to apply the policy. nested list of objects No\nNo last_updated_time The time the policy was last updated. timestamp Yes\nYes error_notification The destination and message template for error notifications. The destination could be Amazon Chime, Slack, or a webhook URL. object No\nNo default_state The default starting state for each index that uses this policy. string Yes\nNo states The states that you define in the policy. nested list of objects Yes\nNo Table of contents States Actions ISM supported operations force_merge read_only read_write replica_count shrink close open delete rollover notification snapshot index_priority allocation rollup Transitions Error notifications Sample policy with ISM template for auto rollover Example policy with ISM templates for the alias action Example policy States\nA state is the description of the status that the managed index is currently in. A managed index can be in only one state at a time. Each state has associated actions that are executed sequentially on entering a state and transitions that are checked after all the actions have been completed.\nThis table lists the parameters that you can define for a state. Field Description Type Required name The name of the state. string Yes actions The actions to execute after entering a state. For more information, see Actions. nested list of objects Yes transitions The next states and the conditions required to transition to those states. If no transitions exist, the policy assumes that it’s complete and can now stop managing the index. For more information, see Transitions. nested list of objects Yes Actions\nActions are the steps that the policy sequentially executes on entering a specific state.\nISM executes actions in the order in which they are defined. For example, if you define actions [A,B,C,D], ISM executes action A, and then goes into a sleep period based on the cluster setting plugins.index_state_management.job_interval. Once the sleep period ends, ISM continues to execute the remaining actions. However, if ISM cannot successfully execute action A, the operation ends, and actions B, C, and D do not get executed.\nOptionally, you can define an action’s timeout period, which, if exceeded, forcibly fails the action. For example, if timeout is set to 1d, and ISM has not completed the action within one day, even after retries, the action fails.\nThis table lists the parameters that you can define for an action. Parameter Description Type Required Default timeout The timeout period for the action. Accepts time units for minutes, hours, and days. time unit No\n- retry The retry configuration for the action. object No\nSpecific to action The retry operation has the following parameters: Parameter Description Type Required Default count The number of retry counts. number Yes\n- backoff The backoff policy type to use when retrying. Valid values are Exponential, Constant, and Linear. string No\nExponential delay The time to wait between retries. Accepts time units for minutes, hours, and days. time unit No\n1 minute The following example action has a timeout period of one hour. The policy retries this action three times with an exponential backoff policy, with a delay of 10 minutes between each retry: \"actions\": { \"timeout\": \"1h\", \"retry\": { \"count\": 3, \"backoff\": \"exponential\", \"delay\": \"10m\" } } For a list of available unit types, see Supported units.\nISM supported operations\nISM supports the following operations: force_merge read_only read_write replica_count shrink close open delete rollover notification snapshot index_priority allocation rollup force_merge\nReduces the number of Lucene segments by merging the segments of individual shards. This operation attempts to set the index to a read-only state before starting the merging process. Parameter Description Type Required max_num_segments The number of segments to reduce the shard to. number Yes { \"force_merge\": { \"max_num_segments\": 1 } } read_only\nSets a managed index to be read only. { \"read_only\": {} } read_write\nSets a managed index to be writeable. { \"read_write\": {} } replica_count\nSets the number of replicas to assign to an index. Parameter Description Type Required number_of_replicas Defines the number of replicas to assign to an index. number Yes { \"replica_count\": { \"number_of_replicas\": 2 } } For information about setting replicas, see Primary and replica shards.\nshrink\nAllows you to reduce the number of primary shards in your indexes. With this action, you can specify:\nThe number of primary shards that the target index should contain.\nA max shard size for the primary shards in the target index.\nSpecify a percentage to shrink the number of primary shards in the target index. \"shrink\": { \"num_new_shards\": 1, \"target_index_name_template\": { \"source\": \"_shrunken\" }, \"aliases\": [ { \"my-alias\": {} }], \"force_unsafe\": false } Parameter Description Type Example Required num_new_shards The maximum number of primary shards in the shrunken index.\ninteger 5 Yes, however it cannot be used with max_shard_size or percentage_of_source_shards max_shard_size The maximum size in bytes of a shard for the target index.\nkeyword 5gb Yes, however it cannot be used with num_new_shards or percentage_of_source_shards percentage_of_source_shards Percentage of the number of original primary shards to shrink. This parameter indicates the minimum percentage to use when shrinking the number of primary shards. Must be between 0.0 and 1.0, exclusive.\nPercentage 0.5 Yes, however it cannot be used with max_shard_size or num_new_shards target_index_name_template The name of the shrunken index. Accepts strings and the Mustache variables and. string or Mustache template {\"source\": \"_shrunken\"} No aliases Aliases to add to the new index.\nobject myalias No, but must be an array of alias objects force_unsafe If true, executes the shrink action even if there are no replicas.\nboolean false No If you want to add aliases to the action, the parameter must include an array of alias objects. For example, \"aliases\": [ { \"my-alias\": {} }, { \"my-second-alias\": { \"is_write_index\": false, \"filter\": { \"multi_match\": { \"query\": \"QUEEN\", \"fields\": [ \"speaker\", \"text_entry\"] } }, \"index_routing\": \"1\", \"search_routing\": \"1\" } },] close\nCloses the managed index. { \"close\": {} } Closed indexes remain on disk, but consume no CPU or memory. You can’t read from, write to, or search closed indexes.\nClosing an index is a good option if you need to retain data for longer than you need to actively search it and have sufficient disk space on your data nodes. If you need to search the data again, reopening a closed index is simpler than restoring an index from a snapshot.\nopen\nOpens a managed index. { \"open\": {} } delete\nDeletes a managed index. { \"delete\": {} } rollover\nRolls an alias over to a new index when the managed index meets one of the rollover conditions. Important: ISM checks the conditions for operations on every execution of the policy based on the set interval, not continuously. The rollover will be performed if the value has reached or exceeded the configured limit when the check is performed. For example with min_size configured to a value of 100GiB, ISM might check the index at 99 GiB and not perform the rollover. However, if the index has grown past the limit (e.g. 105GiB) by the next check, the operation is performed.\nThe index format must match the pattern: ^.*-\\d+$. For example, (logs-000001).\nSet index.plugins.index_state_management.rollover_alias as the alias to rollover. Parameter Description Type Example Required min_size The minimum size of the total primary shard storage (not counting replicas) required to roll over the index. For example, if you set min_size to 100 GiB and your index has 5 primary shards and 5 replica shards of 20 GiB each, the total size of all primary shards is 100 GiB, so the rollover occurs. See Important note above. string 20gb or 5mb No min_primary_shard_size The minimum storage size of a single primary shard required to roll over the index. For example, if you set min_primary_shard_size to 30 GiB and one of the primary shards in the index has a size greater than the condition, the rollover occurs. See Important note above. string 20gb or 5mb No min_doc_count The minimum number of documents required to roll over the index. See Important note above. number 2000000 No min_index_age The minimum age required to roll over the index. Index age is the time between its creation and the present. Supported units are d (days), h (hours), m (minutes), s (seconds), ms (milliseconds), and micros (microseconds). See Important note above. string 5d or 7h No { \"rollover\": { \"min_size\": \"50gb\" } } { \"rollover\": { \"min_primary_shard_size\": \"30gb\" } } { \"rollover\": { \"min_doc_count\": 100000000 } } { \"rollover\": { \"min_index_age\": \"30d\" } } notification\nSends you a notification. Parameter Description Type Required destination The destination URL. Slack, Amazon Chime, or webhook URL Yes message_template The text of the message. You can add variables to your messages using Mustache templates. object Yes The destination system must return a response otherwise the notification operation throws an error.\nExample 1: Chime notification { \"notification\": { \"destination\": { \"chime\": { \"url\": \"&lt;url&gt;\" } }, \"message_template\": { \"source\": \"the index is {{ctx.index}}\" } } } Example 2: Custom webhook notification { \"notification\": { \"destination\": { \"custom_webhook\": { \"url\": \"https://&lt;your_webhook&gt;\" } }, \"message_template\": { \"source\": \"the index is {{ctx.index}}\" } } } Example 3: Slack notification { \"notification\": { \"destination\": { \"slack\": { \"url\": \"https://hooks.slack.com/services/xxx/xxxxxx\" } }, \"message_template\": { \"source\": \"the index is {{ctx.index}}\" } } } You can use ctx variables in your message to represent a number of policy parameters based on the past executions of your policy. For example, if your policy has a rollover action, you can use {{ctx.action.name}} in your message to represent the name of the rollover.\nThe following ctx variable options are available for every policy:\nGuaranteed variables Parameter Description Type index The name of the index. string index_uuid The uuid of the index. string policy_id The name of the policy. string snapshot\nBack up your cluster’s indexes and state. For more information about snapshots, see Take and restore snapshots.\nThe snapshot operation has the following parameters: Parameter Description Type Required Default repository The repository name that you register through the native snapshot API operations. string Yes\n- snapshot The name of the snapshot. Accepts strings and the Mustache variables and. If the Mustache variables are invalid, then the snapshot name defaults to the index’s name. string or Mustache template\nYes\n- { \"snapshot\": { \"repository\": \"my_backup\", \"snapshot\": \"\" } } index_priority\nSet the priority for the index in a specific state. Unallocated shards of indexes are recovered in the order of their priority, whenever possible. The indexes with higher priority values are recovered first followed by the indexes with lower priority values.\nThe index_priority operation has the following parameter: Parameter Description Type Required Default priority The priority for the index as soon as it enters a state. number Yes\n1 \"actions\": [ { \"index_priority\": { \"priority\": 50 } }] allocation\nAllocate the index to a node with a specific attribute set like this.\nFor example, setting require to warm moves your data only to “warm” nodes.\nThe allocation operation has the following parameters: Parameter Description Type Required require Allocate the index to a node with a specified attribute. string Yes include Allocate the index to a node with any of the specified attributes. string Yes exclude Don’t allocate the index to a node with any of the specified attributes. string Yes wait_for Wait for the policy to execute before allocating the index to a node with a specified attribute. string Yes \"actions\": [ { \"allocation\": { \"require\": { \"temp\": \"warm\" } } }] rollup Index rollup lets you periodically reduce data granularity by rolling up old data into summarized indexes.\nRollup jobs can be continuous or non-continuous. A rollup job created using an ISM policy can only be non-continuous.\nPath and HTTP methods PUT _plugins/_rollup/jobs/&lt;rollup_id&gt;\nGET _plugins/_rollup/jobs/&lt;rollup_id&gt;\nDELETE _plugins/_rollup/jobs/&lt;rollup_id&gt;\nPOST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_start\nPOST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_stop\nGET _plugins/_rollup/jobs/&lt;rollup_id&gt;/_explain Sample ISM rollup policy { \"policy\": { \"description\": \"Sample rollup\", \"default_state\": \"rollup\", \"states\": [ { \"name\": \"rollup\", \"actions\": [ { \"rollup\": { \"ism_rollup\": { \"description\": \"Creating rollup through ISM\", \"target_index\": \"target\", \"page_size\": 1000, \"dimensions\": [ { \"date_histogram\": { \"fixed_interval\": \"60m\", \"source_field\": \"order_date\", \"target_field\": \"order_date\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"customer_gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day_of_week\" } }], \"metrics\": [ { \"source_field\": \"taxless_total_price\", \"metrics\": [ { \"sum\": {} }] }, { \"source_field\": \"total_quantity\", \"metrics\": [ { \"avg\": {} }, { \"max\": {} }] }] } } }], \"transitions\": [] }] } } Request fields\nRequest fields are required when creating an ISM policy. You can reference the Index rollups API page for request field options.\nAdding a rollup policy in Dashboards\nTo add a rollup policy in Dashboards, follow the steps below.\nSelect the menu button on the top-left of the Dashboards user interface.\nIn the Dashboards menu, select Index Management.\nOn the next screen select Rollup jobs.\nSelect the Create rollup button.\nFollow the steps in the Create rollup job wizard.\nAdd a name for the policy in the Name box.\nYou can reference the Index rollups API page to configure the rollup policy.\nFinally, select the Create button on the bottom-right of the Dashboards user interface.\nTransitions\nTransitions define the conditions that need to be met for a state to change. After all actions in the current state are completed, the policy starts checking the conditions for transitions.\nISM evaluates transitions in the order in which they are defined. For example, if you define transitions: [A,B,C,D], ISM iterates through this list of transitions until it finds a transition that evaluates to true, it then stops and sets the next state to the one defined in that transition. On its next execution, ISM dismisses the rest of the transitions and starts in that new state.\nIf you don’t specify any conditions in a transition and leave it empty, then it’s assumed to be the equivalent of always true. This means that the policy transitions the index to this state the moment it checks.\nThis table lists the parameters you can define for transitions. Parameter Description Type Required state_name The name of the state to transition to if the conditions are met. string Yes conditions List the conditions for the transition. list Yes The conditions object has the following parameters: Parameter Description Type Required min_index_age The minimum age of the index required to transition. string No min_rollover_age The minimum age required after a rollover has occurred to transition to the next state. string No min_doc_count The minimum document count of the index required to transition. number No min_size The minimum size of the total primary shard storage (not counting replicas) required to transition. For example, if you set min_size to 100 GiB and your index has 5 primary shards and 5 replica shards of 20 GiB each, the total size of all primary shards is 100 GiB, so your index is transitioned to the next state. string No cron The cron job that triggers the transition if no other transition happens first. object No cron.cron.expression The cron expression that triggers the transition. string Yes cron.cron.timezone The timezone that triggers the transition. string Yes The following example transitions the index to a cold state after a period of 30 days: \"transitions\": [ { \"state_name\": \"cold\", \"conditions\": { \"min_index_age\": \"30d\" } }] ISM checks the conditions on every execution of the policy based on the set interval.\nThis example uses the cron condition to transition indexes every Saturday at 5:00 PT: \"transitions\": [ { \"state_name\": \"cold\", \"conditions\": { \"cron\": { \"cron\": { \"expression\": \"* 17 * * SAT\", \"timezone\": \"America/Los_Angeles\" } } } }] Note that this condition does not execute at exactly 5:00 PM; the job still executes based off the job_interval setting. Due to this variance in start time and the amount of time that it can take for actions to complete prior to checking transition conditions, we recommend against overly narrow cron expressions. For example, don’t use 15 17 * * SAT (5:15 PM on Saturday).\nA window of an hour, which this example uses, is generally sufficient, but you might increase it to 2–3 hours to avoid missing the window and having to wait a week for the transition to occur. Alternately, you could use a broader expression such as * * * * SAT,SUN to have the transition occur at any time during the weekend.\nFor information on writing cron expressions, see Cron expression reference.\nError notifications\nThe error_notification operation sends you a notification if your managed index fails.\nIt notifies a single destination or notification channel with a custom message.\nSet up error notifications at the policy level: { \"policy\": { \"description\": \"hot warm delete workflow\", \"default_state\": \"hot\", \"schema_version\": 1, \"error_notification\": { }, \"states\": [] } } Parameter Description Type Required destination The destination URL. Slack, Amazon Chime, or webhook URL Yes if channel isn’t specified channel A notification channel’s ID string Yes if destination isn’t specified message_template The text of the message. You can add variables to your messages using Mustache templates. object Yes The destination system must return a response otherwise the error_notification operation throws an error.\nExample 1: Chime notification { \"error_notification\": { \"destination\": { \"chime\": { \"url\": \"&lt;url&gt;\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } Example 2: Custom webhook notification { \"error_notification\": { \"destination\": { \"custom_webhook\": { \"url\": \"https://&lt;your_webhook&gt;\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } Example 3: Slack notification { \"error_notification\": { \"destination\": { \"slack\": { \"url\": \"https://hooks.slack.com/services/xxx/xxxxxx\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } Example 4: Using a notification channel { \"error_notification\": { \"channel\": { \"id\": \"some-channel-config-id\" }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } You can use the same options for ctx variables as the notification operation.\nSample policy with ISM template for auto rollover\nThe following sample template policy is for a rollover use case.\nIf you want to skip rollovers for an index, set index.plugins.index_state_management.rollover_skip to true in the settings of that index.\nCreate a policy with an ism_template field: PUT _plugins/_ism/policies/rollover_policy { \"policy\": { \"description\": \"Example rollover policy.\", \"default_state\": \"rollover\", \"states\": [ { \"name\": \"rollover\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 1 } }], \"transitions\": [] }], \"ism_template\": { \"index_patterns\": [ \"log*\"], \"priority\": 100 } } } You need to specify the index_patterns field. If you don’t specify a value for priority, it defaults to 0.\nSet up a template with the rollover_alias as log: PUT _index_template/ism_rollover { \"index_patterns\": [ \"log*\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"log\" } } } Create an index with the log alias: PUT log -000001 { \"aliases\": { \"log\": { \"is_write_index\": true } } } Index a document to trigger the rollover condition: POST log/_doc { \"message\": \"dummy\" } Verify if the policy is attached to the log-000001 index: GET _plugins/_ism/explain/log -000001?pretty Example policy with ISM templates for the alias action\nThe following example policy is for an alias action use case.\nIn the following example, the first job will trigger the rollover action, and a new index will be created. Next, another document is added to the two indexes. The new job will then cause the second index to point to the log alias, and the older index will be removed due to the alias action.\nFirst, create an ISM policy: PUT /_plugins/_ism/policies/rollover_policy?pretty { \"policy\": { \"description\": \"Example rollover policy.\", \"default_state\": \"rollover\", \"states\": [ { \"name\": \"rollover\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 1 } }], \"transitions\": [{ \"state_name\": \"alias\", \"conditions\": { \"min_doc_count\": \"2\" } }] }, { \"name\": \"alias\", \"actions\": [ { \"alias\": { \"actions\": [ { \"remove\": { \"alias\": \"log\" } }] } }] }], \"ism_template\": { \"index_patterns\": [ \"log*\"], \"priority\": 100 } } } Next, create an index template on which to enable the policy: PUT /_index_template/ism_rollover? { \"index_patterns\": [ \"log*\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"log\" } } } copy Next, change the cluster settings to trigger jobs every minute: PUT /_cluster/settings?pretty= true { \"persistent\": { \"plugins.index_state_management.job_interval\": 1 } } copy Next, create a new index: PUT /log -000001 { \"aliases\": { \"log\": { \"is_write_index\": true } } } copy Finally, add a document to the index to trigger the job: POST /log -000001 /_doc { \"message\": \"dummy\" } copy You can verify these steps using the Alias and Index API: GET /_cat/indices?pretty copy GET /_cat/aliases?pretty copy Note: The index and remove_index parameters are not allowed with alias action policies. Only the add and remove alias action parameters are allowed.\nExample policy\nThe following example policy implements a hot, warm, and delete workflow. You can use this policy as a template to prioritize resources to your indexes based on their levels of activity.\nIn this case, an index is initially in a hot state. After a day, it changes to a warm state, where the number of replicas increases to 5 to improve the read performance.\nAfter 30 days, the policy moves this index into a delete state. The service sends a notification to a Chime room that the index is being deleted, and then permanently deletes it. { \"policy\": { \"description\": \"hot warm delete workflow\", \"default_state\": \"hot\", \"schema_version\": 1, \"states\": [ { \"name\": \"hot\", \"actions\": [ { \"rollover\": { \"min_index_age\": \"1d\", \"min_primary_shard_size\": \"30gb\" } }], \"transitions\": [ { \"state_name\": \"warm\" }] }, { \"name\": \"warm\", \"actions\": [ { \"replica_count\": { \"number_of_replicas\": 5 } }], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"30d\" } }] }, { \"name\": \"delete\", \"actions\": [ { \"notification\": { \"destination\": { \"chime\": { \"url\": \"&lt;URL&gt;\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} is being deleted\" } } }, { \"delete\": {} }] }], \"ism_template\": { \"index_patterns\": [ \"log*\"], \"priority\": 100 } } } This diagram shows the states, transitions, and actions of the above policy as a finite-state machine. For more information about finite-state machines, see Wikipedia.",
    "ancestors": [
      "Index management plugin",
      "Index State Management"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/ism/settings/",
    "title": "Settings",
    "content": "We don’t recommend changing these settings; the defaults should work well for most use cases.\nIndex State Management (ISM) stores its configuration in the.opendistro-ism-config index. Don’t modify this index without using the ISM API operations.\nAll settings are available using the OpenSearch _cluster/settings operation. None require a restart, and all can be marked persistent or transient. Setting Default Description plugins.index_state_management.enabled True\nSpecifies whether ISM is enabled or not. plugins.index_state_management.job_interval 5 minutes\nThe interval at which the managed index jobs are run. plugins.index_state_management.jitter 0.6\nA randomized delay that is added to a job’s base run time to prevent a surge of activity from all indices at the same time. A value of 0.6 means a delay of 0-60% of a job interval is added to the base interval. For example, if you have a base interval time of 30 minutes, a value of 0.6 means an amount anywhere between 0 to 18 minutes gets added to your job interval. Maximum is 1, which means an additional interval time of 100%. This maximum cannot exceed plugins.jobscheduler.jitter_limit, which also has a default of 0.6. For example, if plugins.index_state_management.jitter is set to 0.8, ISM uses plugins.jobscheduler.jitter_limit of 0.6 instead. plugins.index_state_management.coordinator.sweep_period 10 minutes\nHow often the routine background sweep is run. plugins.index_state_management.coordinator.backoff_millis 50 milliseconds\nThe backoff time between retries for failures in the ManagedIndexCoordinator (such as when we update managed indices). plugins.index_state_management.coordinator.backoff_count 2\nThe count of retries for failures in the ManagedIndexCoordinator. plugins.index_state_management.history.enabled True\nSpecifies whether audit history is enabled or not. The logs from ISM are automatically indexed to a logs document. plugins.index_state_management.history.max_docs 2,500,000\nThe maximum number of documents before rolling over the audit history index. plugins.index_state_management.history.max_age 24 hours\nThe maximum age before rolling over the audit history index. plugins.index_state_management.history.rollover_check_period 8 hours\nThe time between rollover checks for the audit history index. plugins.index_state_management.history.rollover_retention_period 30 days\nHow long audit history indices are kept. plugins.index_state_management.allow_list All actions\nList of actions that you can use.",
    "ancestors": [
      "Index management plugin",
      "Index State Management"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/refresh-analyzer/index/",
    "title": "Refresh search analyzer",
    "content": "With ISM installed, you can refresh search analyzers in real time with the following API: POST /_plugins/_refresh_search_analyzers/&lt;index or alias or wildcard&gt; For example, if you change the synonym list in your analyzer, the change takes effect without you needing to close and reopen the index.\nTo work, the token filter must have an updateable flag of true: { \"analyzer\": { \"my_synonyms\": { \"tokenizer\": \"whitespace\", \"filter\": [ \"synonym\"] } }, \"filter\": { \"synonym\": { \"type\": \"synonym_graph\", \"synonyms_path\": \"synonyms.txt\", \"updateable\": true } } }",
    "ancestors": [
      "Index management plugin"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/security/",
    "title": "Index management security",
    "content": "Using the security plugin with index management lets you limit non-admin users to certain actions. For example, you might want to set up your security such that a group of users can only read ISM policies, while others can create, delete, or change policies.\nAll index management data are protected as system indices, and only a super admin or an admin with a Transport Layer Security (TLS) certificate can access system indices. For more information, see System indices.\nBasic permissions\nThe security plugin comes with one role that offers full access to index management: index_management_full_access. For a description of the role’s permissions, see Predefined roles.\nWith security enabled, users not only need the correct index management permissions, but they also need permissions to execute actions to involved indices. For example, if a user wants to use the REST API to attach a policy that executes a rollup job to an index named system-logs, they would need the permissions to attach a policy and execute a rollup job, as well as access to system-logs.\nFinally, with the exceptions of Create Policy, Get Policy, and Delete Policy, users also need the indices:admin/opensearch/ism/managedindex permission to execute ISM APIs.\n(Advanced) Limit access by backend role\nYou can use backend roles to configure fine-grained access to index management policies and actions. For example, users of different departments in an organization might view different policies depending on what roles and permissions they are assigned.\nFirst, ensure your users have the appropriate backend roles. Backend roles usually come from an LDAP server or SAML provider. However, if you use the internal user database, you can use the REST API to add them manually.\nUse the REST API to enable the following setting: PUT _cluster/settings { \"transient\": { \"plugins.index_management.filter_by_backend_roles\": \"true\" } } With security enabled, only users who share at least one backend role can see and execute the policies and actions relevant to their roles.\nFor example, consider a scenario with three users: John and Jill, who have the backend role helpdesk_staff, and Jane, who has the backend role phone_operator. John wants to create a policy that performs a rollup job on an index named airline_data, so John would need a backend role that has permissions to access that index, create relevant policies, and execute relevant actions, and Jill would be able to access the same index, policy, and job. However, Jane cannot access or edit those resources or actions.",
    "ancestors": [
      "Index management plugin"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/documentation/adv-analytics/index/",
    "title": "Advanced Analytics",
    "content": "Advanced Analytics\nIntroduction",
    "ancestors": [
      "Documentation"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/documentation/index/",
    "title": "About Documentation",
    "content": "Elysium Documentation\nAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout Documentation",
    "ancestors": [
      "Documentation"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/documentation/search/index/",
    "title": "Elysium Full Text Search",
    "content": "search engine\nIntroduction\nElysium Search is a search tool designed to help users quickly find and filter data within a database. It allows users to search for specific values or patterns within a dataset, and offers a range of options for customizing the search process.\nHow to Guides\nAdd Index To add an index to Elysium Search, follow these steps:\nOpen the Elysium Search.\nClick on the three horizontal bars to open side menu.\nClick on the “Stack Management“ button. Click on the “Index Patterns” button.\nClick on the “Create Index Pattern” button.\nSearch for the table name\nClick on the “Next Step“ button and wait until the load completes (at the right top corner). Once loaded use “Time field“ dropdown to select the timestamp.\nSelecting timestamp is mandatory to preform search. Do not create index without selecting the timestamp. Click on the “Create Index pattern” button.\nSet Result Limit for a search in Elysium Search, follow these steps:\nOpen the Elysium Search.\nClick on the three horizontal bars to open side menu.\nClick on the “Stack Management“ button. Click on the “Advanced Settings” button.\nFind “Number of rows” and change to desired number. Click on the “Save changes” to save the setting. Elysium Search allows users to specify the time zone for their searches. To set the time zone, follow these steps:\nOpen the Elysium SaaS.\nClick on the “user icon” at the top right corner and then click “My account“. Click “Timezone” for the dropdown and select to the desired time zone.\nOnce selected then Click on the button “Save“ which requires password. Enter the password and click on the button “YES“. How to search:\nOpen the Elysium Search.\nClick on the three horizontal bars to open side menu.\nClick on the “Discover“ button. Click on the “index“ dropdown to select the index on which search needs to performed. Select the time range from right top corner. Use search bar for entering different types of searches. Click on the “Refresh“/”Update” button for results. Different types of searches you can use to filter data.\nUnquoted Search:\nUse unquoted search to filter data that partially matches with the value. For example, to filter data where the username field values are like “john white”, “john whitehead”,”sara white”, etc., use the following syntax: username: white The field parameter is optional. If not provided, all fields will be searched for the given value. For example, to search all fields for “white”, use the following: white Quoted Search:\nUse quoted search to filter data that exactly matches with the value. For example, to filter data where the username field value is “john white”, use the following syntax: username: \"john white\" The field parameter is optional. If not provided, Smart Search will be enabled and it will identify fields that might be a possible match and search those fields for the given value. For more information on Smart Search, refer Smart Search. To use Exact Match, use the following: \"john white\" Wildcard Search:\nEven though unquoted search is used for partial matches, there are cases where wildcard search can be useful. To filter data that starts with “tom”, use the following syntax: username: tom* To filter data that ends with “sam”, use the following syntax: username: *sam Wildcard search can only be used with unquoted searches. If the wildcard () is used in a quoted search, it will be considered as a literal. For example, “sam ” will search for “sam*”.\nUlike unquoted search which comes with default wildcards at the start and end of the input value. For example, sam will be queried as sam. Meaning any characters can be before and after “sam“. So, when using wildcards explicitly, it removes the default wildcards and treats the input (*) as a wildcard.\nWildcards can also be used to find fields where a value exists. Use the following syntax to get all values without nulls: username: * Note: Wildcard Searches doesn’t query on flatten columns like raw.src.ip\nwhile column is not mentioned.\nMatching Multiple Fields:\nWildcards can also be used to query multiple fields. For example, to search for documents where any sub-field of “http.response” equals “error”, use the following: http.response.*: \"error\" Negating a Query:\nTo negate or exclude a set of data, use the “not” keyword. For example, to filter documents where the “http.request.method” is not “GET”, use the following query: NOT http.request.method: “GET” Negating also contains the null value while a specific search will always negate null values.\nCombining Multiple Queries:\nTo combine multiple queries, use the “and” or “or” keywords. For example, to find documents where the “http.request.method” is “GET” or the “http.response.status_code” is 400, use the following query: http.request.method: “GET” OR http.response.status_code: “400” Similarly, to find documents where the “http.request.method” is “GET” and the “http.response.status_code” is 400, use this query: http.request.method: “GET” AND http.response.status_code: “400” To specify precedence when combining multiple queries, use parentheses. For example, to find documents where the “http.request.method” is “GET” and the “http.response.status_code” is 200, or the “http.request.method is POST” and “http.response.status_code” is 400, use the following: (http.request.method: “GET” AND http.response.status_code: “200”) OR (http.request.method: “POST” AND http.response.status_code: “400”) You can also use parentheses for shorthand syntax when querying multiple values for the same field. For example, to find documents where the “http.request.method” is “GET”, “POST”, or “DELETE”, use the following: http.request.method: (“GET” OR “POST” OR “DELETE”) http.request.method:”GET” OR “POST“ This would return results that either contain “GET “in the specified field http.request.method or contain the term “POST” anywhere in the document.\nPrecedence: When using multiple logical operators in a search, it is important to consider the precedence of the operators. The AND operator has the highest precedence, followed by OR.\nSo the search term “A AND B OR C” will be evaluated as “(A AND B) OR C”, while the search term “A OR B AND C” will be evaluated as “A OR (B AND C)”.\nEscaping Special Characters:\nThere may be times when you need to search for a value that includes special characters. In these cases, you will need to escape these characters.\nTo search for documents where  http.request.referrer  is  https://example.com, use either of the following queries: http.request.referrer: \"https://example.com\" http.request.referrer: https\\://example.com You must escape following characters (unless surrounded by quotes/quoted search):\n(): &lt; &gt; “ * { }\nSmart Search:\nSmart Search is a feature that enables searching without specifying the field. When using Smart Search, the system will identify fields that might be a possible match and search those fields for the given value.\nFor example, if you search for “john white” without specifying a field, the system will search all fields that contain “john white”.\nTo use Smart Search, enclose the search value in quotes. For example: \"john white\". Numeric Range Search:\nElysium Search supports numeric searches, but there are a few considerations to keep in mind.\nRounding: Elysium Search rounds numeric values to the nearest value that can be represented in double-precision floating-point format. This can affect the accuracy of numeric searches.\nValue will be rounded after 9 digits after the decimal point and values with a leading zero.. In cases where more precision is needed, users should use Double Quotes to treat the number as a string. 110.0 is treated as 110 and 10.00000 is treated as 10. Score: 123 Score: “1.123456789123“ Score: “10.0“ Any number more than 16 digits (for both +ve and –ve numbers). We should use double quotes in that case. Adding a + in front treats the search value as a string.\nHex number such as 0xabcd is not supported as a number. It must be searched as string.\nFor example, to search for the value “01”, you would enter “01” in the search bar.\nYou can also use the “&gt;” and “&lt;” operators to specify an open-ended range. For example, to search for documents where the “age” field is greater than 18, use the following: age &gt; 18 To search for documents where the “age” field is less than 25, use the following: age &lt; 25 Case Sensitivity:\nElysium Search is case-Sensitive by default, meaning that it does distinguish between uppercase and lowercase characters in searches. However, you can enable case-Insensitivity by using the following steps.\nOpen the Elysium Search.\nClick on the three horizontal bars to open side menu.\nClick on the “Discover“ button to open search.\nOn the search page find “settings icon”\nClick on the “settings icon” button.\nClick on the “Case Sensitivity“ toggle to turn it on or off.\nHighlights:\nUnquoted search will highlight the exact value entered while matching doing a partial match.\nUser: Sannith → User: Sannith Reddy\nQuoted search will highlight the exact value entered.\nUser: “Sannith Reddy”: → User: Sannith Reddy\nWildcard matches will highlight depending on the *.\nUser: *Reddy → User: Sannith Reddy\nLimitations\nElysium Search has a number of limitations that users should be aware of:\nArithmetic expressions and variables are not supported.\nOnly a limited set of character sets is supported.\nSearches on timestamps are not available currently.\nWorkarounds\nArithmetic Expressions: You will need to use an alternative approach if you need to perform calculations in your searches. One option is to you use a script to pre-process your data and add calculated fields,, which you can then search using Elysium Search or User can also use Dashboards(Looker).\nCopy/Paste: users have to be careful while copy pasting the results to search as there might be chances to characters which needs to be escaped or numbers with accurate precision.",
    "ancestors": [
      "Documentation"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/about/",
    "title": "About OpenSearch",
    "content": "OpenSearch is a distributed search and analytics engine based on Apache Lucene. After adding your data to OpenSearch, you can perform full-text searches on it with all of the features you might expect: search by field, search multiple indices, boost fields, rank results by score, sort results by field, and aggregate results.\nUnsurprisingly, people often use search engines like OpenSearch as the backend for a search application—think Wikipedia or an online store. It offers excellent performance and can scale up and down as the needs of the application grow or shrink.\nAn equally popular, but less obvious use case is log analytics, in which you take the logs from an application, feed them into OpenSearch, and use the rich search and visualization functionality to identify issues. For example, a malfunctioning web server might throw a 500 error 0.5% of the time, which can be hard to notice unless you have a real-time graph of all HTTP status codes that the server has thrown in the past four hours. You can use OpenSearch Dashboards to build these sorts of visualizations from data in OpenSearch.\nClusters and nodes\nIts distributed design means that you interact with OpenSearch clusters. Each cluster is a collection of one or more nodes, servers that store your data and process search requests.\nYou can run OpenSearch locally on a laptop—its system requirements are minimal—but you can also scale a single cluster to hundreds of powerful machines in a data center.\nIn a single node cluster, such as a laptop, one machine has to do everything: manage the state of the cluster, index and search data, and perform any preprocessing of data prior to indexing it. As a cluster grows, however, you can subdivide responsibilities. Nodes with fast disks and plenty of RAM might be great at indexing and searching data, whereas a node with plenty of CPU power and a tiny disk could manage cluster state. For more information on setting node types, see Cluster formation.\nIndices and documents\nOpenSearch organizes data into indices. Each index is a collection of JSON documents. If you have a set of raw encyclopedia articles or log lines that you want to add to OpenSearch, you must first convert them to JSON. A simple JSON document for a movie might look like this: { \"title\": \"The Wind Rises\", \"release_date\": \"2013-07-20\" } When you add the document to an index, OpenSearch adds some metadata, such as the unique document ID: { \"_index\": \"&lt;index-name&gt;\", \"_type\": \"_doc\", \"_id\": \"&lt;document-id&gt;\", \"_version\": 1, \"_source\": { \"title\": \"The Wind Rises\", \"release_date\": \"2013-07-20\" } } Indices also contain mappings and settings:\nA mapping is the collection of fields that documents in the index have. In this case, those fields are title and release_date.\nSettings include data like the index name, creation date, and number of shards.\nPrimary and replica shards\nOpenSearch splits indices into shards for even distribution across nodes in a cluster. For example, a 400 GB index might be too large for any single node in your cluster to handle, but split into ten shards, each one 40 GB, OpenSearch can distribute the shards across ten nodes and work with each shard individually.\nBy default, OpenSearch creates a replica shard for each primary shard. If you split your index into ten shards, for example, OpenSearch also creates ten replica shards. These replica shards act as backups in the event of a node failure—OpenSearch distributes replica shards to different nodes than their corresponding primary shards—but they also improve the speed and rate at which the cluster can process search requests. You might specify more than one replica per index for a search-heavy workload.\nDespite being a piece of an OpenSearch index, each shard is actually a full Lucene index—confusing, we know. This detail is important, though, because each instance of Lucene is a running process that consumes CPU and memory. More shards is not necessarily better. Splitting a 400 GB index into 1,000 shards, for example, would place needless strain on your cluster. A good rule of thumb is to keep shard size between 10–50 GB.\nREST API\nYou interact with OpenSearch clusters using the REST API, which offers a lot of flexibility. You can use clients like curl or any programming language that can send HTTP requests. To add a JSON document to an OpenSearch index (i.e. index a document), you send an HTTP request: PUT https://&lt;host&gt;:&lt;port&gt;/&lt;index-name&gt;/_doc/&lt;document-id&gt; { \"title\": \"The Wind Rises\", \"release_date\": \"2013-07-20\" } To run a search for the document: GET https://&lt;host&gt;:&lt;port&gt;/&lt;index-name&gt;/_search?q=wind To delete the document: DELETE https://&lt;host&gt;:&lt;port&gt;/&lt;index-name&gt;/_doc/&lt;document-id&gt; You can change most OpenSearch settings using the REST API, modify indices, check the health of the cluster, get statistics—almost everything.",
    "ancestors": [
      "OpenSearch documentation"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/breaking-changes/",
    "title": "Breaking changes",
    "content": "2.0.0\nRemove mapping types parameter\nThe type parameter has been removed from all OpenSearch API endpoints. Instead, indexes can be categorized by document type. For more details, see issue #1940.\nDeprecate outdated nomenclature\nIn order for OpenSearch to include more inclusive naming conventions, we’ve replaced the following terms in our code with more inclusive terms:\n“Whitelist” is now “Allow list”\n“Blacklist” is now “Deny list”\n“Master” is now “Cluster Manager”\nIf you are still using the outdated terms in the context of the security APIs or for node management, your calls and automation will continue to work until the terms are removed later in 2022.\nAdd OpenSearch Notifications plugins\nIn OpenSearch 2.0, the Alerting plugin is now integrated with new plugins for Notifications. If you want to continue to use the notification action in the Alerting plugin, install the new backend plugins notifications-core and notifications. If you want to manage notifications in OpenSearch Dashboards, use the new notificationsDashboards plugin. For more information, see Questions about destinations on the Monitors page.\nDrop support for JDK 8\nA Lucene upgrade forced OpenSearch to drop support for JDK 8. As a consequence, the Java high-level REST client no longer supports JDK 8. Restoring JDK 8 support is currently an opensearch-java proposal #156 and will require removing OpenSearch core as a dependency from the Java client (issue #262).",
    "ancestors": [
      "OpenSearch documentation"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/",
    "title": "OpenSearch documentation",
    "content": "Welcome to the OpenSearch documentation! With this documentation, you’ll learn how to use OpenSearch — the only 100% open-source search, analytics, and visualization suite.\nWe have a dedicated and growing number of technical writers who are building our documentation library. We also welcome and encourage community input. To contribute, see the Contributing file. A good place to start is by browsing issues labeled “ good first issue.”\ntest for right nav\nhfbvhs vseh visev eisv iwe ve viwe vihw evih sjbsz fskhb vfg weuv wes fcug esvugesuigvf ugsd fjweuov uwe vu suov ouwev suw eov wueosv sjbe vuose v a\nGetting started About OpenSearch Quickstart Install OpenSearch Install OpenSearch Dashboards See the FAQ Why use OpenSearch?\nWith OpenSearch, you can perform the following use cases: Fast, Scalable Full-text Search\nApplication and Infrastructure Monitoring\nSecurity and Event Information Management\nOperational Health Tracking\nHelp users find the right information within your application, website, or data lake catalog.\nEasily store and analyze log data, and set automated alerts for underperformance.\nCentralize logs to enable real-time security monitoring and forensic analysis.\nUse observability logs, metrics, and traces to monitor your applications and business in real time. Additional features and plugins: OpenSearch has several features and plugins to help index, secure, monitor, and analyze your data. Most OpenSearch plugins have corresponding OpenSearch Dashboards plugins that provide a convenient, unified user interface. Anomaly detection - Identify atypical data and receive automatic notifications KNN - Find “nearest neighbors” in your vector data Performance Analyzer - Monitor and optimize your cluster SQL - Use SQL or a piped processing language to query your data Index State Management - Automate index operations ML Commons plugin - Train and execute machine-learning models Asynchronous search - Run search requests in the background Cross-cluster replication - Replicate your data across multiple OpenSearch clusters\nThe secure path forward\nOpenSearch includes a demo configuration so that you can get up and running quickly, but before using OpenSearch in a production environment, you must configure the security plugin manually with your own certificates, authentication method, users, and passwords.\nLooking for the Javadoc?\nSee opensearch.org/javadocs/.\nGet involved OpenSearch is supported by Amazon Web Services. All components are available under the Apache License, Version 2.0 on GitHub.\nThe project welcomes GitHub issues, bug fixes, features, plugins, documentation—anything at all. To get involved, see Contributing on the OpenSearch website. OpenSearch includes certain Apache-licensed Elasticsearch code from Elasticsearch B.V. and other source code. Elasticsearch B.V. is not the source of that other source code. ELASTICSEARCH is a registered trademark of Elasticsearch B.V.",
    "ancestors": [

    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/quickstart/",
    "title": "Quickstart",
    "content": "Get started using OpenSearch and OpenSearch Dashboards by deploying your containers with Docker. Before proceeding, you need to get Docker and Docker Compose installed on your local machine.\nThe Docker Compose commands used in this guide are written with a hyphen (for example, docker-compose). If you installed Docker Desktop on your machine, which automatically installs a bundled version of Docker Compose, then you should remove the hyphen. For example, change docker-compose to docker compose.\nStarting your cluster\nYou’ll need a special file, called a Compose file, that Docker Compose uses to define and create the containers in your cluster. The OpenSearch Project provides a sample Compose file that you can use to get started. Learn more about working with Compose files by reviewing the official Compose specification.\nBefore running OpenSearch on your machine, you should disable memory paging and swapping performance on the host to improve performance and increase the number of memory maps available to OpenSearch. See important system settings for more information. # Disable memory paging and swapping. sudo swapoff -a # Edit the sysctl config file that defines the host's max map count. sudo vi /etc/sysctl.conf # Set max map count to the recommended value of 262144. vm.max_map_count = 262144 # Reload the kernel parameters. sudo sysctl -p Download the sample Compose file to your host. You can download the file with command line utilities like curl and wget, or you can manually copy docker-compose.yml from the OpenSearch Project documentation-website repository using a web browser. # Using cURL: curl -O https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/docker-compose.yml # Using wget: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/docker-compose.yml In your terminal application, navigate to the directory containing the docker-compose.yml file you just downloaded, and run the following command to create and start the cluster as a background process. docker-compose up -d Confirm that the containers are running with the command docker-compose ps. You should see an output like the following: $ docker-compose ps\nNAME COMMAND SERVICE STATUS PORTS\nopensearch-dashboards \"./opensearch-dashbo…\" opensearch-dashboards running 0.0.0.0:5601-&gt;5601/tcp\nopensearch-node1 \"./opensearch-docker…\" opensearch-node1 running 0.0.0.0:9200-&gt;9200/tcp, 9300/tcp, 0.0.0.0:9600-&gt;9600/tcp, 9650/tcp\nopensearch-node2 \"./opensearch-docker…\" opensearch-node2 running 9200/tcp, 9300/tcp, 9600/tcp, 9650/tcp Query the OpenSearch REST API to verify that the service is running. You should use -k (also written as --insecure) to disable host name checking because the default security configuration uses demo certificates. Use -u to pass the default username and password ( admin:admin). curl https://localhost:9200 -ku admin:admin Sample response: { \"name\": \"opensearch-node1\", \"cluster_name\": \"opensearch-cluster\", \"cluster_uuid\": \"Cd7SL5ysRSyuau325M3h9w\", \"version\": { \"distribution\": \"opensearch\", \"number\": \"2.3.0\", \"build_type\": \"tar\", \"build_hash\": \"6f6e84ebc54af31a976f53af36a5c69d474a5140\", \"build_date\": \"2022-09-09T00:07:12.137133581Z\", \"build_snapshot\": false, \"lucene_version\": \"9.3.0\", \"minimum_wire_compatibility_version\": \"7.10.0\", \"minimum_index_compatibility_version\": \"7.0.0\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } Explore OpenSearch Dashboards by opening http://localhost:5601/ in a web browser on the same host that is running your OpenSearch cluster. The default username is admin and the default password is admin.\nCreate an index and field mappings using sample data\nCreate an index and define field mappings using a dataset provided by the OpenSearch Project. The same fictitious e-commerce data is also used for sample visualizations in OpenSearch Dashboards. To learn more, see Getting started with OpenSearch Dashboards.\nDownload ecommerce-field_mappings.json. This file defines a mapping for the sample data you will use. # Using cURL: curl -O https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/ecommerce-field_mappings.json # Using wget: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/ecommerce-field_mappings.json Download ecommerce.json. This file contains the index data formatted so that it can be ingested by the bulk API. To learn more, see index data and Bulk. # Using cURL: curl -O https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/ecommerce.json # Using wget: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/ecommerce.json Define the field mappings with the mapping file. curl -H \"Content-Type: application/x-ndjson\" -X PUT \"https://localhost:9200/ecommerce\" -ku admin:admin --data-binary \"@ecommerce-field_mappings.json\" Upload the index to the bulk API. curl -H \"Content-Type: application/x-ndjson\" -X PUT \"https://localhost:9200/ecommerce/_bulk\" -ku admin:admin --data-binary \"@ecommerce.json\" Query the data using the search API. The following command submits a query that will return documents where customer_first_name is Sonya. curl -H 'Content-Type: application/json' -X GET \"https://localhost:9200/ecommerce/_search?pretty=true\" -ku admin:admin -d ' {\"query\":{\"match\":{\"customer_first_name\":\"Sonya\"}}}' Queries submitted to the OpenSearch REST API will generally return a flat JSON by default. For a human readable response body, use the query parameter pretty=true. For more information about pretty and other useful query parameters, see Common REST parameters.\nAccess OpenSearch Dashboards by opening http://localhost:5601/ in a web browser on the same host that is running your OpenSearch cluster. The default username is admin and the default password is admin.\nOn the top menu bar, go to Management &gt; Dev Tools.\nIn the left pane of the console, enter the following: GET ecommerce/_search { \"query\": { \"match\": { \"customer_first_name\": \"Sonya\" } } } Choose the triangle icon at the top right of the request to submit the query. You can also submit the request by pressing Ctrl+Enter (or Cmd+Enter for Mac users). To learn more about using the OpenSearch Dashboards console for submitting queries, see Running queries in the console.\nNext steps\nYou successfully deployed your own OpenSearch cluster with OpenSearch Dashboards and added some sample data. Now you’re ready to learn about configuration and functionality in more detail. Here are a few recommendations on where to begin: About the security plugin OpenSearch configuration OpenSearch plugin installation Getting started with OpenSearch Dashboards OpenSearch tools Index APIs Common issues\nReview these common issues and suggested solutions if your containers fail to start or exit unexpectedly.\nDocker commands require elevated permissions\nEliminate the need for running your Docker commands with sudo by adding your user to the docker user group. See Docker’s Post-installation steps for Linux for more information. sudo usermod -aG docker $USER Error message: “-bash: docker-compose: command not found”\nIf you installed Docker Desktop, then Docker Compose is already installed on your machine. Try docker compose (without the hyphen) instead of docker-compose. See Use Docker Compose.\nError message: “docker: ‘compose’ is not a docker command.”\nIf you installed Docker Engine, then you must install Docker Compose separately, and you will use the command docker-compose (with a hyphen). See Docker Compose.\nError message: “max virtual memory areas vm.max_map_count [65530] is too low”\nOpenSearch will fail to start if your host’s vm.max_map_count is too low. Review the important system settings if you see the following errors in the service log, and set vm.max_map_count appropriately. opensearch-node1 | ERROR: [ 1] bootstrap checks failed\nopensearch-node1 | [ 1]: max virtual memory areas vm.max_map_count [ 65530] is too low, increase to at least [ 262144]\nopensearch-node1 | ERROR: OpenSearch did not exit normally - check the logs at /usr/share/opensearch/logs/opensearch-cluster.log",
    "ancestors": [
      "OpenSearch documentation"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/version-history/",
    "title": "Version history",
    "content": "OpenSearch version Release highlights Release date 2.5.0 Includes index management UI enhancements, multi-layer maps, Jaeger support for observability, Debian distributions, returning cluster health by awareness attribute, cluster manager task throttling, weighted zonal search request routing policy, and query string support in index rollups. Experimental features include request-level durability in remote-backed storage and GPU acceleration for ML nodes. For a full list of release highlights, see the Release Notes.\n24 January 2023 2.4.1 Includes maintenance changes and bug fixes for gradle check and indexing pressure tests. Adds support for skipping changelog.\n13 December 2022 2.4.0 Includes Windows support, Point-in-time search, custom k-NN filtering, xy_point and xy_shape field types for Cartesian coordinates, GeoHex grid aggregation, and resilience enhancements, including search backpressure. In OpenSearch Dashboards, this release adds snapshot restore functionality, multiple authentication, and aggregate view of saved objects. This release includes the following experimental features: searchable snapshots, Compare Search Results, multiple data sources in OpenSearch Dashboards, a new Model Serving Framework in ML Commons, a new Neural Search plugin that supports semantic search, and a new Security Analytics plugin to analyze security logs. For a full list of release highlights, see the Release Notes.\n15 November 2022 2.3.0 This release includes the following experimental features: segment replication, remote-backed storage, and drag and drop for OpenSearch Dashboards. Experimental features allow you to test new functionality in OpenSearch. Because these features are still being developed, your testing and feedback can help shape the development of the feature before it’s official released. We do not recommend use of experimental features in production. Additionally, this release adds maketime and makedate datetime functions for the SQL plugin. Creates a new OpenSearch Playground demo site for OpenSearch Dashboards. For a full list of release highlights, see the Release Notes.\n14 September 2022 2.2.1 Includes gradle updates and bug fixes for gradle check.\n01 September 2022 2.2.0 Includes support for Logistic Regression and RCFSummarize machine learning algorithms in ML Commons, Lucene or C-based Nmslib and Faiss libraries for approximate k-NN search, search by relevance using SQL and PPL queries, custom region maps for visualizations, and rollup enhancements. For a full list of release highlights, see the Release Notes.\n11 August 2022 2.1.0 Includes support for dedicated ML node in the ML Commons plugin, relevance search and other features in SQL, multi-terms aggregation, and Snapshot Management. For a full list of release highlights, see the Release Notes.\n07 July 2022 2.0.1 Includes bug fixes and maintenance updates for Alerting and Anomaly Detection.\n16 June 2022 2.0.0 Includes document-level monitors for alerting, OpenSearch Notifications plugins, and Geo Map Tiles in OpenSearch Dashboards. Also adds support for Lucene 9 and bug fixes for all OpenSearch plugins. For a full list of release highlights, see the Release Notes.\n26 May 2022 2.0.0-rc1 The Release Candidate for 2.0.0. This version allows you to preview the upcoming 2.0.0 release before the GA release. The preview release adds document-level alerting, support for Lucene 9, and the ability to use term lookup queries in document level security.\n03 May 2022 1.3.7 Adds Windows support. Includes maintenance updates and bug fixes for error handling.\n13 December 2022 1.3.6 Includes maintenance updates and bug fixes for tenancy in the OpenSearch Security Dashboards plugin.\n06 October 2022 1.3.5 Includes maintenance updates and bug fixes for gradle check and OpenSearch security.\n01 September 2022 1.3.4 Includes maintenance updates and bug fixes for OpenSearch and OpenSearch Dashboards.\n14 July 2022 1.3.3 Adds enhancements to Anomaly Detection and ML Commons. Bug fixes for Anomaly Detection, Observability, and k-NN.\n09 June 2022 1.3.2 Bug fixes for Anomaly Detection and the Security Dashboards Plugin, adds the option to install OpenSearch using RPM, as well as enhancements to the ML Commons execute task, and the removal of the job-scheduler zip in Anomaly Detection.\n05 May 2022 1.3.1 Bug fixes when using document-level security, and adjusted ML Commons to use the latest RCF jar and protostuff to RCF model serialization.\n30 March 2022 1.3.0 Adds Model Type Validation to Validate Detector API, continuous transforms, custom actions, applied policy parameter to Explain API, default action retries, and new rollover and transition conditions to Index Management, new ML Commons plugin, parse command to SQL, Application Analytics, Live Tail, Correlation, and Events Flyout to Observability, and auto backport and support for OPENSEARCH_JAVA_HOME to Performance Analyzer. Bug fixes.\n17 March 2022 1.2.4 Updates Performance Analyzer, SQL, and Security plugins to Log4j 2.17.1, Alerting and Job Scheduler to cron-utils 9.1.6, and gson in Anomaly Detection and SQL.\n18 January 2022 1.2.3 Updates the version of Log4j used in OpenSearch to Log4j 2.17.0 as recommended by the advisory in CVE-2021-45105.\n22 December 2021 1.2.0 Adds observability, new validation API for Anomaly Detection, shard-level indexing back-pressure, new “match” query type for SQL and PPL, support for Faiss libraries in k-NN, and custom Dashboards branding.\n23 November 2021 1.1.0 Adds cross-cluster replication, security for Index Management, bucket-level alerting, a CLI to help with upgrading from Elasticsearch OSS to OpenSearch, and enhancements to high cardinality data in the anomaly detection plugin.\n05 October 2021 1.0.1 Bug fixes.\n01 September 2021 1.0.0 General availability release. Adds compatibility setting for clients that require a version check before connecting.\n12 July 2021 1.0.0-rc1 First release candidate.\n07 June 2021 1.0.0-beta1 Initial beta release. Refactors plugins to work with OpenSearch.\n13 May 2021",
    "ancestors": [
      "OpenSearch documentation"
    ],
    "type": "DOCS"
  }
]
